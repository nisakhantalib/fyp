{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a4efc29b-c886-426e-9a46-2a7b67ec4b6d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-12T11:22:57.593241Z",
          "iopub.status.busy": "2024-03-12T11:22:57.592836Z",
          "iopub.status.idle": "2024-03-12T11:23:15.686045Z",
          "shell.execute_reply": "2024-03-12T11:23:15.684948Z",
          "shell.execute_reply.started": "2024-03-12T11:22:57.593213Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4efc29b-c886-426e-9a46-2a7b67ec4b6d",
        "outputId": "1daf1d1f-2f17-44ce-c733-c76e293b1bae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: textstat in /usr/local/lib/python3.10/dist-packages (0.7.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (1.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.4.99)\n",
            "Requirement already satisfied: pyphen in /usr/local/lib/python3.10/dist-packages (from textstat) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install xgboost scikit-learn pandas scikit-learn joblib openpyxl transformers torch textstat nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Traditional Machine Learning\n"
      ],
      "metadata": {
        "id": "XD1WvKfYjpUe"
      },
      "id": "XD1WvKfYjpUe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f7841c4-f567-4888-b78e-b3e66a6209b1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-12T03:48:28.417197Z",
          "iopub.status.busy": "2024-03-12T03:48:28.416791Z",
          "iopub.status.idle": "2024-03-12T03:49:00.325193Z",
          "shell.execute_reply": "2024-03-12T03:49:00.324349Z",
          "shell.execute_reply.started": "2024-03-12T03:48:28.417167Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7f7841c4-f567-4888-b78e-b3e66a6209b1",
        "outputId": "0f7c8351-7a4b-4e01-cd1a-a1a80d545149"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8169934640522876\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.90      0.86        21\n",
            "           1       1.00      0.86      0.92        21\n",
            "           2       0.68      0.62      0.65        21\n",
            "           3       0.67      0.48      0.56        21\n",
            "           4       0.89      0.76      0.82        21\n",
            "           5       0.68      0.81      0.74        21\n",
            "           6       1.00      0.90      0.95        21\n",
            "           7       0.83      0.90      0.86        21\n",
            "           8       0.80      0.95      0.87        21\n",
            "           9       0.60      0.57      0.59        21\n",
            "          10       0.95      1.00      0.98        21\n",
            "          11       0.78      0.86      0.82        21\n",
            "          12       0.95      1.00      0.98        21\n",
            "          13       0.41      0.57      0.48        21\n",
            "          14       0.76      0.76      0.76        21\n",
            "          15       0.95      0.90      0.93        21\n",
            "          16       0.79      0.71      0.75        21\n",
            "          17       0.79      0.90      0.84        21\n",
            "          18       0.95      0.90      0.93        21\n",
            "          19       0.94      0.76      0.84        21\n",
            "          20       1.00      1.00      1.00        21\n",
            "          21       0.88      1.00      0.93        21\n",
            "          22       0.83      0.95      0.89        21\n",
            "          23       0.74      0.81      0.77        21\n",
            "          24       0.85      0.81      0.83        21\n",
            "          25       0.95      0.95      0.95        21\n",
            "          26       0.94      0.76      0.84        21\n",
            "          27       0.84      0.76      0.80        21\n",
            "          28       0.95      1.00      0.98        21\n",
            "          29       0.72      0.86      0.78        21\n",
            "          30       0.83      0.90      0.86        21\n",
            "          31       0.83      0.71      0.77        21\n",
            "          32       1.00      1.00      1.00        21\n",
            "          33       0.95      0.90      0.93        21\n",
            "          34       0.58      0.67      0.62        21\n",
            "          35       0.88      1.00      0.93        21\n",
            "          36       0.86      0.86      0.86        21\n",
            "          37       0.64      0.76      0.70        21\n",
            "          38       0.93      0.67      0.78        21\n",
            "          39       1.00      0.81      0.89        21\n",
            "          40       1.00      0.95      0.98        21\n",
            "          41       0.67      0.95      0.78        21\n",
            "          42       0.88      0.71      0.79        21\n",
            "          43       0.62      0.38      0.47        21\n",
            "          44       0.71      0.81      0.76        21\n",
            "          45       0.60      0.57      0.59        21\n",
            "          46       0.89      0.76      0.82        21\n",
            "          47       0.86      0.90      0.88        21\n",
            "          48       0.89      0.81      0.85        21\n",
            "          49       0.50      0.52      0.51        21\n",
            "          50       1.00      0.95      0.98        21\n",
            "\n",
            "    accuracy                           0.82      1071\n",
            "   macro avg       0.83      0.82      0.82      1071\n",
            "weighted avg       0.83      0.82      0.82      1071\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer_svm.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "#1.svm, tfidf vectorizer\n",
        "#max_features=1000 accuracy=0.76, max_features=500 acc=0.72,max_features=2000 acc=0.77 ,10000 0.81\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "from nltk import word_tokenize, pos_tag, sent_tokenize,FreqDist\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from collections import Counter\n",
        "from textstat.textstat import textstat\n",
        "\n",
        "import nltk\n",
        "import joblib\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "\n",
        "\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(data[\"text\"], data[\"label\"], test_size=0.3, random_state=42,stratify=data['label'])\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42, stratify=y_temp)\n",
        "\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Initialize an SVM classifier\n",
        "clf = SVC(probability=True,kernel=\"linear\")\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Transform the testing data\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "joblib.dump(clf, 'svm_tfidf.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer_svm.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68259494-4907-4e8c-a0cb-3c50bda00b26",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-12T03:45:25.511633Z",
          "iopub.status.busy": "2024-03-12T03:45:25.511250Z",
          "iopub.status.idle": "2024-03-12T03:45:53.857635Z",
          "shell.execute_reply": "2024-03-12T03:45:53.856882Z",
          "shell.execute_reply.started": "2024-03-12T03:45:25.511604Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68259494-4907-4e8c-a0cb-3c50bda00b26",
        "outputId": "45b341aa-daea-4d5f-e1bc-a913e57e9e53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7849673202614379\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86        30\n",
            "           1       1.00      0.93      0.97        30\n",
            "           2       0.57      0.70      0.63        30\n",
            "           3       0.45      0.47      0.46        30\n",
            "           4       0.78      0.83      0.81        30\n",
            "           5       0.62      0.83      0.71        30\n",
            "           6       0.90      0.90      0.90        30\n",
            "           7       0.83      0.80      0.81        30\n",
            "           8       0.71      0.80      0.75        30\n",
            "           9       0.86      0.60      0.71        30\n",
            "          10       0.97      1.00      0.98        30\n",
            "          11       0.81      0.87      0.84        30\n",
            "          12       0.92      0.73      0.81        30\n",
            "          13       0.46      0.40      0.43        30\n",
            "          14       0.85      0.77      0.81        30\n",
            "          15       1.00      0.93      0.97        30\n",
            "          16       0.63      0.63      0.63        30\n",
            "          17       0.77      0.77      0.77        30\n",
            "          18       0.73      0.80      0.76        30\n",
            "          19       0.96      0.73      0.83        30\n",
            "          20       0.91      1.00      0.95        30\n",
            "          21       0.89      0.80      0.84        30\n",
            "          22       0.62      0.83      0.71        30\n",
            "          23       0.68      0.77      0.72        30\n",
            "          24       0.65      0.73      0.69        30\n",
            "          25       0.90      0.90      0.90        30\n",
            "          26       1.00      0.80      0.89        30\n",
            "          27       0.88      0.73      0.80        30\n",
            "          28       0.97      1.00      0.98        30\n",
            "          29       0.81      0.87      0.84        30\n",
            "          30       0.93      0.87      0.90        30\n",
            "          31       0.70      0.77      0.73        30\n",
            "          32       1.00      1.00      1.00        30\n",
            "          33       0.81      0.70      0.75        30\n",
            "          34       0.67      0.60      0.63        30\n",
            "          35       0.79      0.90      0.84        30\n",
            "          36       0.76      0.73      0.75        30\n",
            "          37       0.71      0.73      0.72        30\n",
            "          38       0.90      0.63      0.75        30\n",
            "          39       0.88      0.73      0.80        30\n",
            "          40       0.97      0.93      0.95        30\n",
            "          41       0.77      0.90      0.83        30\n",
            "          42       0.81      0.87      0.84        30\n",
            "          43       0.33      0.37      0.35        30\n",
            "          44       0.79      0.77      0.78        30\n",
            "          45       0.70      0.70      0.70        30\n",
            "          46       0.86      0.83      0.85        30\n",
            "          47       0.86      0.83      0.85        30\n",
            "          48       0.84      0.87      0.85        30\n",
            "          49       0.47      0.47      0.47        30\n",
            "          50       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           0.78      1530\n",
            "   macro avg       0.79      0.78      0.79      1530\n",
            "weighted avg       0.79      0.78      0.79      1530\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['svm_countvec.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#2.svm, count vectorizer\n",
        "# tfidf 0.81, countvectorizer 0.78\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "from nltk import word_tokenize, pos_tag, sent_tokenize,FreqDist\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from collections import Counter\n",
        "from textstat.textstat import textstat\n",
        "\n",
        "import nltk\n",
        "import joblib\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "\n",
        "\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"label\"], test_size=0.3, random_state=42,stratify=data['label'])\n",
        "\n",
        "# Create a CountVectorizer vectorizer\n",
        "vectorizer = CountVectorizer(max_features=10000)\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Initialize an SVM classifier\n",
        "clf = SVC(kernel=\"linear\")\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Transform the testing data\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "joblib.dump(clf, 'svm_countvec.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c3f65fd-78fb-4ceb-82ed-7f2c63681d1d",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-12T04:36:42.862046Z",
          "iopub.status.busy": "2024-03-12T04:36:42.861655Z",
          "iopub.status.idle": "2024-03-12T04:38:31.425035Z",
          "shell.execute_reply": "2024-03-12T04:38:31.424226Z",
          "shell.execute_reply.started": "2024-03-12T04:36:42.862018Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c3f65fd-78fb-4ceb-82ed-7f2c63681d1d",
        "outputId": "6bb8a877-076d-4e23-915c-da263c4f8b12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:160: UserWarning: [13:36:37] WARNING: /workspace/src/learner.cc:742: \n",
            "Parameters: { \"probability\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6751633986928105\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.80      0.80        30\n",
            "           1       0.67      0.67      0.67        30\n",
            "           2       0.50      0.63      0.56        30\n",
            "           3       0.41      0.37      0.39        30\n",
            "           4       0.64      0.60      0.62        30\n",
            "           5       0.70      0.63      0.67        30\n",
            "           6       0.66      0.83      0.74        30\n",
            "           7       0.62      0.77      0.69        30\n",
            "           8       0.59      0.57      0.58        30\n",
            "           9       0.57      0.43      0.49        30\n",
            "          10       0.94      0.97      0.95        30\n",
            "          11       0.76      0.97      0.85        30\n",
            "          12       0.78      0.83      0.81        30\n",
            "          13       0.40      0.33      0.36        30\n",
            "          14       0.59      0.63      0.61        30\n",
            "          15       0.96      0.90      0.93        30\n",
            "          16       0.53      0.63      0.58        30\n",
            "          17       0.48      0.43      0.46        30\n",
            "          18       0.66      0.70      0.68        30\n",
            "          19       0.75      0.70      0.72        30\n",
            "          20       0.97      0.97      0.97        30\n",
            "          21       0.90      0.63      0.75        30\n",
            "          22       0.47      0.60      0.53        30\n",
            "          23       0.57      0.43      0.49        30\n",
            "          24       0.62      0.50      0.56        30\n",
            "          25       0.69      0.83      0.76        30\n",
            "          26       0.87      0.67      0.75        30\n",
            "          27       0.81      0.73      0.77        30\n",
            "          28       0.94      1.00      0.97        30\n",
            "          29       0.66      0.77      0.71        30\n",
            "          30       0.63      0.73      0.68        30\n",
            "          31       0.68      0.70      0.69        30\n",
            "          32       0.97      1.00      0.98        30\n",
            "          33       0.88      0.70      0.78        30\n",
            "          34       0.33      0.37      0.35        30\n",
            "          35       0.68      0.90      0.77        30\n",
            "          36       0.79      0.63      0.70        30\n",
            "          37       0.46      0.60      0.52        30\n",
            "          38       0.77      0.67      0.71        30\n",
            "          39       0.60      0.70      0.65        30\n",
            "          40       0.90      0.93      0.92        30\n",
            "          41       0.71      0.67      0.69        30\n",
            "          42       0.67      0.67      0.67        30\n",
            "          43       0.48      0.33      0.39        30\n",
            "          44       0.68      0.63      0.66        30\n",
            "          45       0.46      0.43      0.45        30\n",
            "          46       0.69      0.73      0.71        30\n",
            "          47       0.78      0.70      0.74        30\n",
            "          48       0.70      0.47      0.56        30\n",
            "          49       0.38      0.37      0.37        30\n",
            "          50       0.88      0.97      0.92        30\n",
            "\n",
            "    accuracy                           0.68      1530\n",
            "   macro avg       0.68      0.68      0.67      1530\n",
            "weighted avg       0.68      0.68      0.67      1530\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer_xgb.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#3.xgboost tfidf\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import xgboost as xgb\n",
        "import joblib\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "\n",
        "#Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"label\"], test_size=0.3, random_state=42, stratify=data['label'])\n",
        "\n",
        "#Create a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "#Fit and transform the training data\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "\n",
        "#Transform the testing data\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "#Initialize an XGBoost classifier\n",
        "clf = xgb.XGBClassifier(probability=True,use_label_encoder=False, eval_metric='mlogloss')\n",
        "\n",
        "#Train the classifier\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "#Make predictions\n",
        "y_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "#Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "#Save the model and vectorizer\n",
        "joblib.dump(clf, 'xgb_tfidf.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer_xgb.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76fa3ab9-e827-4f2c-82e7-472234a25bf0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-12T04:50:45.783183Z",
          "iopub.status.busy": "2024-03-12T04:50:45.782767Z",
          "iopub.status.idle": "2024-03-12T04:50:48.854415Z",
          "shell.execute_reply": "2024-03-12T04:50:48.853695Z",
          "shell.execute_reply.started": "2024-03-12T04:50:45.783147Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76fa3ab9-e827-4f2c-82e7-472234a25bf0",
        "outputId": "b15cabe6-2c8f-407b-d120-8229626b8de0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.788235294117647\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87        30\n",
            "           1       1.00      0.93      0.97        30\n",
            "           2       0.68      0.57      0.62        30\n",
            "           3       0.62      0.53      0.57        30\n",
            "           4       0.82      0.77      0.79        30\n",
            "           5       0.66      0.77      0.71        30\n",
            "           6       0.90      0.87      0.88        30\n",
            "           7       0.78      0.83      0.81        30\n",
            "           8       0.79      0.90      0.84        30\n",
            "           9       0.77      0.57      0.65        30\n",
            "          10       0.91      0.97      0.94        30\n",
            "          11       0.80      0.93      0.86        30\n",
            "          12       0.90      0.87      0.88        30\n",
            "          13       0.40      0.40      0.40        30\n",
            "          14       0.83      0.80      0.81        30\n",
            "          15       0.97      0.97      0.97        30\n",
            "          16       0.81      0.70      0.75        30\n",
            "          17       0.82      0.90      0.86        30\n",
            "          18       0.89      0.83      0.86        30\n",
            "          19       0.89      0.80      0.84        30\n",
            "          20       0.94      0.97      0.95        30\n",
            "          21       0.87      0.90      0.89        30\n",
            "          22       0.69      0.80      0.74        30\n",
            "          23       0.80      0.67      0.73        30\n",
            "          24       0.78      0.60      0.68        30\n",
            "          25       0.73      0.90      0.81        30\n",
            "          26       0.92      0.73      0.81        30\n",
            "          27       0.85      0.73      0.79        30\n",
            "          28       0.88      1.00      0.94        30\n",
            "          29       0.74      0.87      0.80        30\n",
            "          30       0.82      0.93      0.87        30\n",
            "          31       0.81      0.73      0.77        30\n",
            "          32       0.97      1.00      0.98        30\n",
            "          33       0.87      0.90      0.89        30\n",
            "          34       0.50      0.63      0.56        30\n",
            "          35       0.75      1.00      0.86        30\n",
            "          36       0.84      0.87      0.85        30\n",
            "          37       0.56      0.80      0.66        30\n",
            "          38       0.91      0.70      0.79        30\n",
            "          39       1.00      0.70      0.82        30\n",
            "          40       1.00      0.97      0.98        30\n",
            "          41       0.71      0.90      0.79        30\n",
            "          42       0.82      0.77      0.79        30\n",
            "          43       0.50      0.33      0.40        30\n",
            "          44       0.66      0.83      0.74        30\n",
            "          45       0.62      0.43      0.51        30\n",
            "          46       0.85      0.73      0.79        30\n",
            "          47       0.79      0.90      0.84        30\n",
            "          48       0.77      0.77      0.77        30\n",
            "          49       0.40      0.33      0.36        30\n",
            "          50       1.00      0.97      0.98        30\n",
            "\n",
            "    accuracy                           0.79      1530\n",
            "   macro avg       0.79      0.79      0.78      1530\n",
            "weighted avg       0.79      0.79      0.78      1530\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer_logreg.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "#4.logreg\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import joblib\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"label\"], test_size=0.3, random_state=42, stratify=data['label'])\n",
        "\n",
        "# Create a TF-IDF vectorizer with the best performing max_features based on previous experiments\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "# Fit and transform the training data\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Transform the testing data\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize a Logistic Regression classifier\n",
        "clf = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "# Save the model and vectorizer\n",
        "joblib.dump(clf, 'logreg_tfidf.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer_logreg.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Traditional ML + stylo"
      ],
      "metadata": {
        "id": "YZHS4YqLolNA"
      },
      "id": "YZHS4YqLolNA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02401322-5a22-4449-b390-db20dcf26977",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-12T04:15:16.147054Z",
          "iopub.status.busy": "2024-03-12T04:15:16.146640Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02401322-5a22-4449-b390-db20dcf26977",
        "outputId": "b65de527-f629-470e-bfb4-fb47c839bf62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#5.svm tfidf+stylo,set the max features to 1000, otherwise doesnt stop\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import FreqDist\n",
        "from nltk import word_tokenize, pos_tag, sent_tokenize,FreqDist\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from collections import Counter\n",
        "from textstat.textstat import textstat\n",
        "import joblib\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"label\"], test_size=0.3, random_state=42,stratify=data['label'])\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "\n",
        "\n",
        "def extract_stylometric_features(text, max_length=250):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    sentences = sent_tokenize(text)\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(tokens)\n",
        "\n",
        "    # Basic counts and averages\n",
        "    num_characters = len(text)\n",
        "    num_unique_words = len(set(tokens))\n",
        "    lexical_diversity = num_unique_words / num_words if num_words else 0\n",
        "    avg_word_length = np.mean([len(word) for word in tokens]) if tokens else 0\n",
        "    avg_sentence_length = np.mean([len(sentence.split()) for sentence in sentences]) if sentences else 0\n",
        "\n",
        "    # Frequency distributions\n",
        "    fdist = FreqDist(tokens)\n",
        "    letter_frequencies = {letter: text.lower().count(letter) / num_characters for letter in 'abcdefghijklmnopqrstuvwxyz'}\n",
        "    function_words_count = sum(fdist[word] for word in nltk.corpus.stopwords.words('english') if word in fdist)\n",
        "\n",
        "    # Hapax legomena and dislegomena\n",
        "    hapax_legomena = len([word for word in fdist if fdist[word] == 1])\n",
        "    hapax_dislegomena = len([word for word in fdist if fdist[word] == 2])\n",
        "\n",
        "    # Word length distribution\n",
        "    word_lengths = [len(word) for word in tokens]\n",
        "\n",
        "\n",
        "    def syllable_count(word):\n",
        "        return textstat.syllable_count(word)\n",
        "\n",
        "    avg_syllables_per_word = np.mean([syllable_count(word) for word in tokens]) if tokens else 0\n",
        "\n",
        "    # Readability scores (using Textstat)\n",
        "    fog_index = textstat.gunning_fog(text)\n",
        "    smog_index = textstat.smog_index(text)\n",
        "    readability_index = textstat.flesch_reading_ease(text)\n",
        "\n",
        "    # POS tagging and frequencies\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    pos_counts = Counter(tag for word, tag in tagged_tokens)\n",
        "\n",
        "    # Building feature vector\n",
        "    features = [\n",
        "        num_characters, num_words, num_sentences, num_unique_words, lexical_diversity,\n",
        "        avg_word_length, avg_sentence_length, function_words_count,\n",
        "        hapax_legomena, hapax_dislegomena, np.median(word_lengths),\n",
        "        avg_syllables_per_word, fog_index, smog_index, readability_index\n",
        "    ]\n",
        "    features += list(letter_frequencies.values())\n",
        "    features += [pos_counts[pos] / num_words for pos in sorted(pos_counts)]  # Normalized POS counts\n",
        "    if len(features) < max_length:\n",
        "        # If the feature list is shorter than max_length, pad it with zeros\n",
        "        features += [0] * (max_length - len(features))\n",
        "    elif len(features) > max_length:\n",
        "        # If the feature list is longer, truncate it to max_length\n",
        "        features = features[:max_length]\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "# Extract stylometric features for training data\n",
        "X_train_stylometric = np.array([extract_stylometric_features(text) for text in X_train])\n",
        "\n",
        "# Combine TF-IDF features and stylometric features\n",
        "X_train_combined = np.hstack((X_train_tfidf.toarray(), X_train_stylometric))\n",
        "\n",
        "# Initialize an SVM classifier\n",
        "clf = SVC(kernel=\"linear\")\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train_combined, y_train)\n",
        "\n",
        "# Transform the testing data\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "X_test_stylometric = np.array([extract_stylometric_features(text) for text in X_test])\n",
        "X_test_combined = np.hstack((X_test_tfidf.toarray(), X_test_stylometric))\n",
        "\n",
        "# Make predictions\n",
        "y_pred = clf.predict(X_test_combined)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", report)\n",
        "joblib.dump(clf, 'svm_tfidf_stylo.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer_svm_stylo.pkl')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff39d5e7-7868-4ae4-bdf8-7f3e2dbfb352",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-12T04:38:43.202388Z",
          "iopub.status.busy": "2024-03-12T04:38:43.201883Z",
          "iopub.status.idle": "2024-03-12T04:50:17.312381Z",
          "shell.execute_reply": "2024-03-12T04:50:17.311408Z",
          "shell.execute_reply.started": "2024-03-12T04:38:43.202360Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ff39d5e7-7868-4ae4-bdf8-7f3e2dbfb352",
        "outputId": "20551acf-b7fc-4f6c-cd55-b8c9d7e026cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7013071895424836\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.75      0.80      0.77        30\n",
            "           1       0.82      0.77      0.79        30\n",
            "           2       0.49      0.63      0.55        30\n",
            "           3       0.54      0.47      0.50        30\n",
            "           4       0.64      0.60      0.62        30\n",
            "           5       0.53      0.63      0.58        30\n",
            "           6       0.76      0.73      0.75        30\n",
            "           7       0.67      0.87      0.75        30\n",
            "           8       0.65      0.50      0.57        30\n",
            "           9       0.73      0.53      0.62        30\n",
            "          10       1.00      0.97      0.98        30\n",
            "          11       0.78      0.97      0.87        30\n",
            "          12       0.68      0.87      0.76        30\n",
            "          13       0.43      0.33      0.38        30\n",
            "          14       0.68      0.77      0.72        30\n",
            "          15       1.00      0.97      0.98        30\n",
            "          16       0.53      0.57      0.55        30\n",
            "          17       0.63      0.57      0.60        30\n",
            "          18       0.68      0.77      0.72        30\n",
            "          19       0.80      0.80      0.80        30\n",
            "          20       1.00      0.93      0.97        30\n",
            "          21       0.74      0.77      0.75        30\n",
            "          22       0.60      0.70      0.65        30\n",
            "          23       0.42      0.43      0.43        30\n",
            "          24       0.65      0.57      0.61        30\n",
            "          25       0.73      0.80      0.76        30\n",
            "          26       0.88      0.77      0.82        30\n",
            "          27       0.92      0.73      0.81        30\n",
            "          28       0.94      1.00      0.97        30\n",
            "          29       0.62      0.70      0.66        30\n",
            "          30       0.70      0.63      0.67        30\n",
            "          31       0.67      0.80      0.73        30\n",
            "          32       0.97      1.00      0.98        30\n",
            "          33       0.88      0.73      0.80        30\n",
            "          34       0.49      0.63      0.55        30\n",
            "          35       0.73      0.73      0.73        30\n",
            "          36       0.83      0.63      0.72        30\n",
            "          37       0.47      0.60      0.53        30\n",
            "          38       0.63      0.63      0.63        30\n",
            "          39       0.72      0.70      0.71        30\n",
            "          40       0.85      0.93      0.89        30\n",
            "          41       0.67      0.67      0.67        30\n",
            "          42       0.66      0.63      0.64        30\n",
            "          43       0.38      0.30      0.33        30\n",
            "          44       0.68      0.63      0.66        30\n",
            "          45       0.48      0.37      0.42        30\n",
            "          46       0.72      0.77      0.74        30\n",
            "          47       0.85      0.77      0.81        30\n",
            "          48       0.76      0.63      0.69        30\n",
            "          49       0.54      0.47      0.50        30\n",
            "          50       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           0.70      1530\n",
            "   macro avg       0.70      0.70      0.70      1530\n",
            "weighted avg       0.70      0.70      0.70      1530\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer_xgbstylo.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "#6.xgb tfidf+stylo\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from textstat.textstat import textstat\n",
        "import joblib\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "\n",
        "\n",
        "\n",
        "# Necessary NLTK downloads\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"label\"], test_size=0.3, random_state=42, stratify=data['label'])\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "# Transform the testing data\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Function to extract stylometric features\n",
        "def extract_stylometric_features(text, max_length=250):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    sentences = sent_tokenize(text)\n",
        "    num_characters = len(text)\n",
        "    num_words = len(tokens)\n",
        "    num_sentences = len(sentences)\n",
        "    num_unique_words = len(set(tokens))\n",
        "    lexical_diversity = num_unique_words / num_words if num_words else 0\n",
        "    avg_word_length = np.mean([len(word) for word in tokens]) if tokens else 0\n",
        "    avg_sentence_length = np.mean([len(sentence.split()) for sentence in sentences]) if sentences else 0\n",
        "    function_words_count = sum(token in stopwords.words('english') for token in tokens)\n",
        "    hapax_legomena = len([word for word in set(tokens) if tokens.count(word) == 1])\n",
        "    hapax_dislegomena = len([word for word in set(tokens) if tokens.count(word) == 2])\n",
        "    avg_syllables_per_word = np.mean([textstat.syllable_count(word) for word in tokens]) if tokens else 0\n",
        "    fog_index = textstat.gunning_fog(text)\n",
        "    smog_index = textstat.smog_index(text)\n",
        "    readability_index = textstat.flesch_reading_ease(text)\n",
        "    # Combine all features into a single list\n",
        "    features = [num_characters, num_words, num_sentences, num_unique_words, lexical_diversity, avg_word_length, avg_sentence_length, function_words_count, hapax_legomena, hapax_dislegomena, avg_syllables_per_word, fog_index, smog_index, readability_index]\n",
        "    return features[:max_length]  # Truncate or pad the feature vector to max_length\n",
        "\n",
        "# Extract stylometric features for training and testing data\n",
        "X_train_stylometric = np.array([extract_stylometric_features(text) for text in X_train])\n",
        "X_test_stylometric = np.array([extract_stylometric_features(text) for text in X_test])\n",
        "\n",
        "# Combine TF-IDF features and stylometric features\n",
        "X_train_combined = np.hstack((X_train_tfidf.toarray(), X_train_stylometric))\n",
        "X_test_combined = np.hstack((X_test_tfidf.toarray(), X_test_stylometric))\n",
        "\n",
        "# Initialize an XGBoost classifier\n",
        "clf = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train_combined, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = clf.predict(X_test_combined)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "# Save the model and vectorizer for later use\n",
        "joblib.dump(clf, 'xgb_tfidf_stylo.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer_xgbstylo.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48e3f02c-07cd-4d39-96de-02497695625a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-12T05:50:15.947333Z",
          "iopub.status.busy": "2024-03-12T05:50:15.946933Z",
          "iopub.status.idle": "2024-03-12T06:07:56.554199Z",
          "shell.execute_reply": "2024-03-12T06:07:56.553493Z",
          "shell.execute_reply.started": "2024-03-12T05:50:15.947303Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48e3f02c-07cd-4d39-96de-02497695625a",
        "outputId": "1af57f81-1798-48ba-e0f3-d4f5f4d42827"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.45751633986928103\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.33      0.67      0.44        30\n",
            "           1       0.40      0.77      0.53        30\n",
            "           2       0.44      0.23      0.30        30\n",
            "           3       0.36      0.43      0.39        30\n",
            "           4       0.30      0.43      0.35        30\n",
            "           5       0.39      0.50      0.44        30\n",
            "           6       0.83      0.50      0.62        30\n",
            "           7       0.60      0.70      0.65        30\n",
            "           8       0.55      0.20      0.29        30\n",
            "           9       0.36      0.30      0.33        30\n",
            "          10       0.61      0.73      0.67        30\n",
            "          11       0.49      0.63      0.55        30\n",
            "          12       0.53      0.53      0.53        30\n",
            "          13       0.19      0.20      0.19        30\n",
            "          14       0.71      0.40      0.51        30\n",
            "          15       0.60      0.60      0.60        30\n",
            "          16       0.15      0.13      0.14        30\n",
            "          17       0.38      0.47      0.42        30\n",
            "          18       0.32      0.40      0.35        30\n",
            "          19       0.62      0.60      0.61        30\n",
            "          20       0.50      0.63      0.56        30\n",
            "          21       0.46      0.43      0.45        30\n",
            "          22       0.20      0.23      0.22        30\n",
            "          23       0.53      0.27      0.36        30\n",
            "          24       0.30      0.43      0.35        30\n",
            "          25       0.74      0.57      0.64        30\n",
            "          26       0.58      0.70      0.64        30\n",
            "          27       0.56      0.67      0.61        30\n",
            "          28       0.67      0.87      0.75        30\n",
            "          29       0.72      0.43      0.54        30\n",
            "          30       0.46      0.40      0.43        30\n",
            "          31       0.68      0.43      0.53        30\n",
            "          32       0.48      0.77      0.59        30\n",
            "          33       0.65      0.50      0.57        30\n",
            "          34       0.33      0.27      0.30        30\n",
            "          35       0.48      0.47      0.47        30\n",
            "          36       0.23      0.30      0.26        30\n",
            "          37       0.50      0.77      0.61        30\n",
            "          38       0.50      0.23      0.32        30\n",
            "          39       0.67      0.33      0.44        30\n",
            "          40       0.65      0.80      0.72        30\n",
            "          41       0.22      0.30      0.25        30\n",
            "          42       0.48      0.47      0.47        30\n",
            "          43       0.45      0.17      0.24        30\n",
            "          44       0.36      0.40      0.38        30\n",
            "          45       0.37      0.23      0.29        30\n",
            "          46       0.35      0.23      0.28        30\n",
            "          47       0.27      0.23      0.25        30\n",
            "          48       0.39      0.23      0.29        30\n",
            "          49       0.25      0.13      0.17        30\n",
            "          50       0.97      1.00      0.98        30\n",
            "\n",
            "    accuracy                           0.46      1530\n",
            "   macro avg       0.47      0.46      0.45      1530\n",
            "weighted avg       0.47      0.46      0.45      1530\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer_logregstylo.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "#7.logistic regression tfidf+stylo\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from textstat.textstat import textstat\n",
        "import joblib\n",
        "\n",
        "# Necessary NLTK downloads\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(data[\"text\"], data[\"label\"], test_size=0.3, random_state=42, stratify=data['label'])\n",
        "\n",
        "# Create a TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=10000)\n",
        "\n",
        "# Fit and transform the training data\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Function to extract stylometric features\n",
        "def extract_stylometric_features(text, max_length=250):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    sentences = sent_tokenize(text)\n",
        "    num_characters = len(text)\n",
        "    num_words = len(tokens)\n",
        "    num_sentences = len(sentences)\n",
        "    num_unique_words = len(set(tokens))\n",
        "    lexical_diversity = num_unique_words / num_words if num_words else 0\n",
        "    avg_word_length = np.mean([len(word) for word in tokens]) if tokens else 0\n",
        "    avg_sentence_length = np.mean([len(sentence.split()) for sentence in sentences]) if sentences else 0\n",
        "    function_words_count = sum(token in stopwords.words('english') for token in tokens)\n",
        "    hapax_legomena = len([word for word in set(tokens) if tokens.count(word) == 1])\n",
        "    hapax_dislegomena = len([word for word in set(tokens) if tokens.count(word) == 2])\n",
        "    avg_syllables_per_word = np.mean([textstat.syllable_count(word) for word in tokens]) if tokens else 0\n",
        "    fog_index = textstat.gunning_fog(text)\n",
        "    smog_index = textstat.smog_index(text)\n",
        "    readability_index = textstat.flesch_reading_ease(text)\n",
        "    # Combine all features into a single list\n",
        "    features = [num_characters, num_words, num_sentences, num_unique_words, lexical_diversity, avg_word_length, avg_sentence_length, function_words_count, hapax_legomena, hapax_dislegomena, avg_syllables_per_word, fog_index, smog_index, readability_index]\n",
        "    return features[:max_length]  # Truncate or pad the feature vector to max_length\n",
        "\n",
        "# Extract stylometric features for training and testing data\n",
        "X_train_stylometric = np.array([extract_stylometric_features(text) for text in X_train])\n",
        "X_test_stylometric = np.array([extract_stylometric_features(text) for text in X_test])\n",
        "\n",
        "# Combine TF-IDF features and stylometric features\n",
        "X_train_combined = np.hstack((X_train_tfidf.toarray(), X_train_stylometric))\n",
        "X_test_combined = np.hstack((X_test_tfidf.toarray(), X_test_stylometric))\n",
        "\n",
        "# Initialize a Logistic Regression classifier\n",
        "clf = LogisticRegression(max_iter=6000)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train_combined, y_train)\n",
        "\n",
        "# Make predictions on the testing data\n",
        "y_pred = clf.predict(X_test_combined)\n",
        "\n",
        "# Evaluate the classifier\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(\"Classification Report:\\n\", report)\n",
        "\n",
        "# Save the model and vectorizer for later use\n",
        "joblib.dump(clf, 'logreg_tfidf_stylo.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer_logregstylo.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22256b66-6fd1-46fa-bc42-6b7ff9ae0e3b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-12T11:23:19.704140Z",
          "iopub.status.busy": "2024-03-12T11:23:19.703712Z",
          "iopub.status.idle": "2024-03-12T11:30:49.068771Z",
          "shell.execute_reply": "2024-03-12T11:30:49.066696Z",
          "shell.execute_reply.started": "2024-03-12T11:23:19.704111Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22256b66-6fd1-46fa-bc42-6b7ff9ae0e3b",
        "outputId": "4f982a53-1763-4171-b95b-dd598f34c6fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.7333333333333333\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.83      0.79        30\n",
            "           1       0.83      0.83      0.83        30\n",
            "           2       0.57      0.53      0.55        30\n",
            "           3       0.53      0.63      0.58        30\n",
            "           4       0.68      0.70      0.69        30\n",
            "           5       0.72      0.70      0.71        30\n",
            "           6       0.93      0.90      0.92        30\n",
            "           7       0.74      0.77      0.75        30\n",
            "           8       0.86      0.63      0.73        30\n",
            "           9       0.57      0.40      0.47        30\n",
            "          10       0.88      1.00      0.94        30\n",
            "          11       0.77      0.90      0.83        30\n",
            "          12       0.96      0.73      0.83        30\n",
            "          13       0.39      0.23      0.29        30\n",
            "          14       0.74      0.57      0.64        30\n",
            "          15       1.00      0.87      0.93        30\n",
            "          16       0.77      0.57      0.65        30\n",
            "          17       0.55      0.77      0.64        30\n",
            "          18       0.76      0.83      0.79        30\n",
            "          19       0.92      0.80      0.86        30\n",
            "          20       0.97      0.97      0.97        30\n",
            "          21       0.86      0.83      0.85        30\n",
            "          22       0.47      0.70      0.56        30\n",
            "          23       0.78      0.70      0.74        30\n",
            "          24       0.56      0.60      0.58        30\n",
            "          25       0.76      0.83      0.79        30\n",
            "          26       0.73      0.73      0.73        30\n",
            "          27       0.92      0.73      0.81        30\n",
            "          28       0.86      1.00      0.92        30\n",
            "          29       0.78      0.83      0.81        30\n",
            "          30       0.86      0.80      0.83        30\n",
            "          31       0.73      0.80      0.76        30\n",
            "          32       0.97      1.00      0.98        30\n",
            "          33       0.89      0.80      0.84        30\n",
            "          34       0.47      0.70      0.56        30\n",
            "          35       0.77      0.77      0.77        30\n",
            "          36       0.76      0.63      0.69        30\n",
            "          37       0.57      0.77      0.66        30\n",
            "          38       0.91      0.67      0.77        30\n",
            "          39       0.91      0.70      0.79        30\n",
            "          40       0.91      0.97      0.94        30\n",
            "          41       0.58      0.73      0.65        30\n",
            "          42       0.81      0.73      0.77        30\n",
            "          43       0.48      0.33      0.39        30\n",
            "          44       0.66      0.70      0.68        30\n",
            "          45       0.56      0.47      0.51        30\n",
            "          46       0.76      0.73      0.75        30\n",
            "          47       0.63      0.80      0.71        30\n",
            "          48       0.77      0.80      0.79        30\n",
            "          49       0.41      0.37      0.39        30\n",
            "          50       0.81      1.00      0.90        30\n",
            "\n",
            "    accuracy                           0.73      1530\n",
            "   macro avg       0.74      0.73      0.73      1530\n",
            "weighted avg       0.74      0.73      0.73      1530\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer_logregstylo2.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "#8.logreg using tfidf +stylo but with different stylo features\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk import pos_tag\n",
        "from collections import Counter\n",
        "import textstat\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Function to extract stylometric features\n",
        "def extract_stylometric_features(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    sentences = sent_tokenize(text)\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(tokens)\n",
        "    num_unique_words = len(set(tokens))\n",
        "    num_characters = len(text)\n",
        "    lexical_diversity = num_unique_words / num_words if num_words else 0\n",
        "    avg_word_length = np.mean([len(word) for word in tokens]) if tokens else 0\n",
        "    avg_sentence_length = num_words / num_sentences if num_sentences else 0\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    pos_counts = Counter(tag for word, tag in tagged_tokens)\n",
        "    total_pos_counts = sum(pos_counts.values())\n",
        "    noun_ratio = pos_counts['NN'] / total_pos_counts if 'NN' in pos_counts else 0\n",
        "    adjective_ratio = pos_counts['JJ'] / total_pos_counts if 'JJ' in pos_counts else 0\n",
        "    verb_ratio = pos_counts['VB'] / total_pos_counts if 'VB' in pos_counts else 0\n",
        "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
        "    return np.array([lexical_diversity, avg_word_length, avg_sentence_length, noun_ratio, adjective_ratio, verb_ratio, flesch_reading_ease])\n",
        "\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(data['text']).toarray()\n",
        "\n",
        "# Prepare combined features and labels\n",
        "X_stylometry = np.array([extract_stylometric_features(text) for text in data['text']])\n",
        "X_combined = np.hstack((tfidf_features, X_stylometry))  # Combine TF-IDF features with stylometric features\n",
        "y = data['label'].values\n",
        "\n",
        "# Split the dataset into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_combined, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Train a logistic regression model on the combined features\n",
        "clf = LogisticRegression(random_state=42, max_iter=5000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_pred = clf.predict(X_val)\n",
        "\n",
        "# Calculate and print the accuracy on the validation set\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "\n",
        "# Print the classification report\n",
        "print(classification_report(y_val, y_pred))\n",
        "# Save the model and vectorizer for later use\n",
        "joblib.dump(clf, 'logreg_tfidf_stylo2.pkl')\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer_logregstylo2.pkl')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The result shows that the second set of stylometric features yield better result, so afterward, that features will be use"
      ],
      "metadata": {
        "id": "39qg1RIYj2rm"
      },
      "id": "39qg1RIYj2rm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using other method for stylometric feature extraction: Staked Denoising Auto-Encoder\n",
        "\n",
        "Artificial neural network used for feature extraction and dimensionality reduction\n",
        "\n",
        "\n",
        "\n",
        "> 1. Autoencoder:learns to encode input data into a lower-dimensional representation and then decode it back to its original form. It consists of an encoder network that compresses the input data into a latent representation and a decoder network that reconstructs the input from this representation.\n",
        "\n",
        "\n",
        "> 2. Denoising Autoencoder: during training, noise is added to the input data, and the autoencoder is trained to reconstruct the original, clean input. This helps the autoencoder to learn robust features that capture the underlying structure of the data.\n",
        "\n",
        "\n",
        "> 3. Stacked Autoencoder:multiple layers of autoencoders can be stacked on top of each other to form a deep neural network.\n",
        "\n",
        "\n",
        "\n",
        "> 4. Training:The network learns to reconstruct the input features while removing noise added to the data. Through backpropagation and optimization algorithms such as stochastic gradient descent, the network adjusts its weights to minimize the reconstruction error\n",
        "\n",
        "\n",
        "\n",
        "> 5. Feature Extraction:after the autoencoder is trained, the activations of the hidden layers can be used as stylometric features. By feeding input texts through the trained network and extracting the activations of the hidden layers, a lower-dimensional representation of the texts can be obtained, which can then be used for various stylometric tasks such as authorship attribution or text classification.\n",
        "\n"
      ],
      "metadata": {
        "id": "YcehsEkBsReE"
      },
      "id": "YcehsEkBsReE"
    },
    {
      "cell_type": "code",
      "source": [
        "#9. sdae, tfidf+stylo, log reg\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk import pos_tag\n",
        "from collections import Counter\n",
        "import textstat\n",
        "import tensorflow as tf\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Function to extract stylometric features\n",
        "def extract_stylometric_features(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    sentences = sent_tokenize(text)\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(tokens)\n",
        "    num_unique_words = len(set(tokens))\n",
        "    num_characters = len(text)\n",
        "    lexical_diversity = num_unique_words / num_words if num_words else 0\n",
        "    avg_word_length = np.mean([len(word) for word in tokens]) if tokens else 0\n",
        "    avg_sentence_length = num_words / num_sentences if num_sentences else 0\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    pos_counts = Counter(tag for word, tag in tagged_tokens)\n",
        "    total_pos_counts = sum(pos_counts.values())\n",
        "    noun_ratio = pos_counts['NN'] / total_pos_counts if 'NN' in pos_counts else 0\n",
        "    adjective_ratio = pos_counts['JJ'] / total_pos_counts if 'JJ' in pos_counts else 0\n",
        "    verb_ratio = pos_counts['VB'] / total_pos_counts if 'VB' in pos_counts else 0\n",
        "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
        "    return np.array([lexical_diversity, avg_word_length, avg_sentence_length, noun_ratio, adjective_ratio, verb_ratio, flesch_reading_ease])\n",
        "\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(data['text']).toarray()\n",
        "\n",
        "# Extract and normalize stylometric features\n",
        "X_stylometry = np.array([extract_stylometric_features(text) for text in data['text']])\n",
        "scaler = MinMaxScaler()\n",
        "X_stylometry_normalized = scaler.fit_transform(X_stylometry)\n",
        "\n",
        "# Define Denoising Autoencoder model\n",
        "class DenoisingAutoencoder(tf.keras.Model):\n",
        "    def __init__(self, input_dim, encoding_dim):\n",
        "        super(DenoisingAutoencoder, self).__init__()\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(encoding_dim, activation='relu')\n",
        "        ])\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(input_dim, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "# Setup the autoencoder parameters\n",
        "input_dim = X_stylometry_normalized.shape[1]\n",
        "encoding_dim = 100\n",
        "\n",
        "# Instantiate and compile the SDAE\n",
        "autoencoder = DenoisingAutoencoder(input_dim, encoding_dim)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Introduce noise and train the SDAE\n",
        "noise_factor = 0.05\n",
        "X_stylometry_noisy = X_stylometry_normalized + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_stylometry_normalized.shape)\n",
        "\n",
        "autoencoder.fit(X_stylometry_noisy, X_stylometry_normalized, epochs=50,batch_size=256,shuffle=True)\n",
        "\n",
        "# Extract stylometric features from the encoder\n",
        "X_stylometry_encoded = autoencoder.encoder(X_stylometry_normalized).numpy()\n",
        "\n",
        "# Combine TF-IDF features with stylometric features\n",
        "X_combined = np.hstack((tfidf_features, X_stylometry_encoded))\n",
        "\n",
        "# Prepare labels\n",
        "y = data['label'].values\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_combined, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Train a logistic regression model\n",
        "clf = LogisticRegression(random_state=42, max_iter=5000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(classification_report(y_val, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ge8iJXginy9Z",
        "outputId": "ff5f9d6f-89b6-42be-887e-07857dacc2f6"
      },
      "id": "Ge8iJXginy9Z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "20/20 [==============================] - 2s 2ms/step - loss: 0.0243\n",
            "Epoch 2/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0169\n",
            "Epoch 3/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0148\n",
            "Epoch 4/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0133\n",
            "Epoch 5/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0118\n",
            "Epoch 6/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0104\n",
            "Epoch 7/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0091\n",
            "Epoch 8/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0079\n",
            "Epoch 9/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0068\n",
            "Epoch 10/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0059\n",
            "Epoch 11/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0052\n",
            "Epoch 12/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0046\n",
            "Epoch 13/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0041\n",
            "Epoch 14/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0038\n",
            "Epoch 15/50\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0035\n",
            "Epoch 16/50\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0033\n",
            "Epoch 17/50\n",
            "20/20 [==============================] - 0s 3ms/step - loss: 0.0032\n",
            "Epoch 18/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0030\n",
            "Epoch 19/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0029\n",
            "Epoch 20/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0029\n",
            "Epoch 21/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0028\n",
            "Epoch 22/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0027\n",
            "Epoch 23/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0027\n",
            "Epoch 24/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0026\n",
            "Epoch 25/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0026\n",
            "Epoch 26/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0026\n",
            "Epoch 27/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 28/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 29/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 30/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0025\n",
            "Epoch 31/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0024\n",
            "Epoch 32/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0024\n",
            "Epoch 33/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0024\n",
            "Epoch 34/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0024\n",
            "Epoch 35/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0024\n",
            "Epoch 36/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 37/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 38/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 39/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 40/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 41/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 42/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0023\n",
            "Epoch 43/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 44/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 45/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 46/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 47/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 48/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 49/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Epoch 50/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0022\n",
            "Validation Accuracy: 0.7418300653594772\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.93      0.87        30\n",
            "           1       0.72      0.77      0.74        30\n",
            "           2       0.69      0.60      0.64        30\n",
            "           3       0.47      0.63      0.54        30\n",
            "           4       0.75      0.70      0.72        30\n",
            "           5       0.75      0.70      0.72        30\n",
            "           6       0.84      0.87      0.85        30\n",
            "           7       0.75      0.80      0.77        30\n",
            "           8       0.82      0.60      0.69        30\n",
            "           9       0.62      0.50      0.56        30\n",
            "          10       0.91      1.00      0.95        30\n",
            "          11       0.76      0.87      0.81        30\n",
            "          12       0.88      0.77      0.82        30\n",
            "          13       0.37      0.23      0.29        30\n",
            "          14       0.74      0.57      0.64        30\n",
            "          15       1.00      0.90      0.95        30\n",
            "          16       0.58      0.60      0.59        30\n",
            "          17       0.49      0.67      0.56        30\n",
            "          18       0.72      0.70      0.71        30\n",
            "          19       0.96      0.87      0.91        30\n",
            "          20       0.94      0.97      0.95        30\n",
            "          21       0.96      0.87      0.91        30\n",
            "          22       0.53      0.77      0.63        30\n",
            "          23       0.75      0.60      0.67        30\n",
            "          24       0.62      0.60      0.61        30\n",
            "          25       0.84      0.90      0.87        30\n",
            "          26       0.92      0.73      0.81        30\n",
            "          27       0.88      0.77      0.82        30\n",
            "          28       0.91      1.00      0.95        30\n",
            "          29       0.78      0.83      0.81        30\n",
            "          30       0.72      0.87      0.79        30\n",
            "          31       0.74      0.77      0.75        30\n",
            "          32       1.00      0.93      0.97        30\n",
            "          33       0.93      0.83      0.88        30\n",
            "          34       0.50      0.67      0.57        30\n",
            "          35       0.75      0.80      0.77        30\n",
            "          36       0.70      0.63      0.67        30\n",
            "          37       0.61      0.77      0.68        30\n",
            "          38       0.91      0.70      0.79        30\n",
            "          39       0.95      0.70      0.81        30\n",
            "          40       0.94      0.97      0.95        30\n",
            "          41       0.60      0.80      0.69        30\n",
            "          42       0.83      0.83      0.83        30\n",
            "          43       0.43      0.33      0.38        30\n",
            "          44       0.63      0.73      0.68        30\n",
            "          45       0.57      0.53      0.55        30\n",
            "          46       0.81      0.73      0.77        30\n",
            "          47       0.74      0.83      0.78        30\n",
            "          48       0.77      0.77      0.77        30\n",
            "          49       0.48      0.37      0.42        30\n",
            "          50       0.78      0.97      0.87        30\n",
            "\n",
            "    accuracy                           0.74      1530\n",
            "   macro avg       0.75      0.74      0.74      1530\n",
            "weighted avg       0.75      0.74      0.74      1530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10. sdae tfidf+stylo , but change parameter values, logreg as classifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from collections import Counter\n",
        "import textstat\n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Text preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()  # Lowercase text\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with single space\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
        "    tokens = word_tokenize(text)  # Tokenize\n",
        "    tokens = [token for token in tokens if token not in stopwords.words('english')]  # Remove stopwords\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Function to extract stylometric features\n",
        "def extract_stylometric_features(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    sentences = sent_tokenize(text)\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(tokens)\n",
        "    num_unique_words = len(set(tokens))\n",
        "    num_characters = len(text)\n",
        "    lexical_diversity = num_unique_words / num_words if num_words else 0\n",
        "    avg_word_length = np.mean([len(word) for word in tokens]) if tokens else 0\n",
        "    avg_sentence_length = num_words / num_sentences if num_sentences else 0\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    pos_counts = Counter(tag for word, tag in tagged_tokens)\n",
        "    total_pos_counts = sum(pos_counts.values())\n",
        "    noun_ratio = pos_counts['NN'] / total_pos_counts if 'NN' in pos_counts else 0\n",
        "    adjective_ratio = pos_counts['JJ'] / total_pos_counts if 'JJ' in pos_counts else 0\n",
        "    verb_ratio = pos_counts['VB'] / total_pos_counts if 'VB' in pos_counts else 0\n",
        "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
        "    return np.array([lexical_diversity, avg_word_length, avg_sentence_length, noun_ratio, adjective_ratio, verb_ratio, flesch_reading_ease])\n",
        "\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "\n",
        "# Preprocess text data\n",
        "data['text'] = data['text'].apply(preprocess_text)\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(data['text']).toarray()\n",
        "\n",
        "# Extract and normalize stylometric features\n",
        "X_stylometry = np.array([extract_stylometric_features(text) for text in data['text']])\n",
        "scaler = MinMaxScaler()\n",
        "X_stylometry_normalized = scaler.fit_transform(X_stylometry)\n",
        "\n",
        "# Define Denoising Autoencoder model\n",
        "class DenoisingAutoencoder(tf.keras.Model):\n",
        "    def __init__(self, input_dim, encoding_dim):\n",
        "        super(DenoisingAutoencoder, self).__init__()\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(encoding_dim, activation='relu')\n",
        "        ])\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(input_dim, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "# Setup the autoencoder parameters\n",
        "input_dim = X_stylometry_normalized.shape[1]\n",
        "encoding_dim = 100\n",
        "\n",
        "# Instantiate and compile the SDAE\n",
        "autoencoder = DenoisingAutoencoder(input_dim, encoding_dim)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Introduce noise and train the SDAE\n",
        "noise_factor = 0.5\n",
        "X_stylometry_noisy = X_stylometry_normalized + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_stylometry_normalized.shape)\n",
        "\n",
        "autoencoder.fit(X_stylometry_noisy, X_stylometry_normalized,\n",
        "                epochs=50,\n",
        "                batch_size=256,\n",
        "                shuffle=True)\n",
        "\n",
        "# Extract stylometric features from the encoder\n",
        "X_stylometry_encoded = autoencoder.encoder(X_stylometry_normalized).numpy()\n",
        "\n",
        "# Combine TF-IDF features with  stylometric features\n",
        "X_combined = np.hstack((tfidf_features, X_stylometry_encoded))\n",
        "\n",
        "# Prepare labels\n",
        "y = data['label'].values\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_combined, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Train a logistic regression model\n",
        "clf = LogisticRegression(random_state=42, max_iter=5000)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set and print the accuracy and classification report\n",
        "y_pred = clf.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(classification_report(y_val, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FNnKe7VUNHA",
        "outputId": "53f2dda0-6b9a-44cf-fcd6-2b7cabf9d3cf"
      },
      "id": "7FNnKe7VUNHA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "20/20 [==============================] - 1s 2ms/step - loss: 0.0245\n",
            "Epoch 2/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0161\n",
            "Epoch 3/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0146\n",
            "Epoch 4/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0141\n",
            "Epoch 5/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0138\n",
            "Epoch 6/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0136\n",
            "Epoch 7/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0135\n",
            "Epoch 8/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0134\n",
            "Epoch 9/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0133\n",
            "Epoch 10/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0133\n",
            "Epoch 11/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0132\n",
            "Epoch 12/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0132\n",
            "Epoch 13/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0131\n",
            "Epoch 14/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0131\n",
            "Epoch 15/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0130\n",
            "Epoch 16/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0130\n",
            "Epoch 17/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0130\n",
            "Epoch 18/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0130\n",
            "Epoch 19/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0129\n",
            "Epoch 20/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0129\n",
            "Epoch 21/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0129\n",
            "Epoch 22/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0129\n",
            "Epoch 23/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0129\n",
            "Epoch 24/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0129\n",
            "Epoch 25/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0129\n",
            "Epoch 26/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0129\n",
            "Epoch 27/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0129\n",
            "Epoch 28/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0129\n",
            "Epoch 29/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 30/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 31/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 32/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 33/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 34/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 35/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 36/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 37/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 38/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 39/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 40/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 41/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 42/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 43/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 44/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 45/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 46/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 47/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 48/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 49/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 50/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Validation Accuracy: 0.7660130718954249\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.93      0.89        30\n",
            "           1       0.93      0.90      0.92        30\n",
            "           2       0.62      0.60      0.61        30\n",
            "           3       0.48      0.50      0.49        30\n",
            "           4       0.91      0.67      0.77        30\n",
            "           5       0.74      0.83      0.78        30\n",
            "           6       0.94      0.97      0.95        30\n",
            "           7       0.75      0.80      0.77        30\n",
            "           8       0.75      0.80      0.77        30\n",
            "           9       0.71      0.50      0.59        30\n",
            "          10       0.94      1.00      0.97        30\n",
            "          11       0.74      0.83      0.78        30\n",
            "          12       0.90      0.90      0.90        30\n",
            "          13       0.42      0.33      0.37        30\n",
            "          14       0.77      0.77      0.77        30\n",
            "          15       0.97      0.93      0.95        30\n",
            "          16       0.75      0.70      0.72        30\n",
            "          17       0.70      0.70      0.70        30\n",
            "          18       0.88      0.77      0.82        30\n",
            "          19       0.89      0.83      0.86        30\n",
            "          20       0.91      0.97      0.94        30\n",
            "          21       0.82      0.90      0.86        30\n",
            "          22       0.83      0.80      0.81        30\n",
            "          23       0.68      0.77      0.72        30\n",
            "          24       0.70      0.63      0.67        30\n",
            "          25       0.75      0.90      0.82        30\n",
            "          26       0.96      0.80      0.87        30\n",
            "          27       0.85      0.73      0.79        30\n",
            "          28       0.88      1.00      0.94        30\n",
            "          29       0.69      0.83      0.76        30\n",
            "          30       0.81      0.83      0.82        30\n",
            "          31       0.76      0.73      0.75        30\n",
            "          32       0.97      0.93      0.95        30\n",
            "          33       0.82      0.93      0.87        30\n",
            "          34       0.47      0.60      0.53        30\n",
            "          35       0.76      0.93      0.84        30\n",
            "          36       0.81      0.83      0.82        30\n",
            "          37       0.56      0.67      0.61        30\n",
            "          38       0.90      0.63      0.75        30\n",
            "          39       0.96      0.73      0.83        30\n",
            "          40       1.00      0.97      0.98        30\n",
            "          41       0.64      0.77      0.70        30\n",
            "          42       0.69      0.67      0.68        30\n",
            "          43       0.41      0.37      0.39        30\n",
            "          44       0.68      0.77      0.72        30\n",
            "          45       0.45      0.43      0.44        30\n",
            "          46       0.81      0.70      0.75        30\n",
            "          47       0.76      0.87      0.81        30\n",
            "          48       0.79      0.77      0.78        30\n",
            "          49       0.50      0.33      0.40        30\n",
            "          50       0.97      1.00      0.98        30\n",
            "\n",
            "    accuracy                           0.77      1530\n",
            "   macro avg       0.77      0.77      0.76      1530\n",
            "weighted avg       0.77      0.77      0.76      1530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12. svm_sdae tfidf+ stylo,\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from collections import Counter\n",
        "import textstat\n",
        "import re\n",
        "import tensorflow as tf\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Function to extract stylometric features\n",
        "def extract_stylometric_features(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    sentences = sent_tokenize(text)\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(tokens)\n",
        "    num_unique_words = len(set(tokens))\n",
        "    num_characters = len(text)\n",
        "    lexical_diversity = num_unique_words / num_words if num_words else 0\n",
        "    avg_word_length = np.mean([len(word) for word in tokens]) if tokens else 0\n",
        "    avg_sentence_length = num_words / num_sentences if num_sentences else 0\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    pos_counts = Counter(tag for word, tag in tagged_tokens)\n",
        "    total_pos_counts = sum(pos_counts.values())\n",
        "    noun_ratio = pos_counts['NN'] / total_pos_counts if 'NN' in pos_counts else 0\n",
        "    adjective_ratio = pos_counts['JJ'] / total_pos_counts if 'JJ' in pos_counts else 0\n",
        "    verb_ratio = pos_counts['VB'] / total_pos_counts if 'VB' in pos_counts else 0\n",
        "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
        "    return np.array([lexical_diversity, avg_word_length, avg_sentence_length, noun_ratio, adjective_ratio, verb_ratio, flesch_reading_ease])\n",
        "\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "\n",
        "# Preprocess text data\n",
        "data['text'] = data['text'].apply(preprocess_text)\n",
        "\n",
        "# Initialize TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(data['text']).toarray()\n",
        "\n",
        "# Extract and normalize stylometric features\n",
        "X_stylometry = np.array([extract_stylometric_features(text) for text in data['text']])\n",
        "scaler = MinMaxScaler()\n",
        "X_stylometry_normalized = scaler.fit_transform(X_stylometry)\n",
        "\n",
        "# Define Denoising Autoencoder model\n",
        "class DenoisingAutoencoder(tf.keras.Model):\n",
        "    def __init__(self, input_dim, encoding_dim):\n",
        "        super(DenoisingAutoencoder, self).__init__()\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(encoding_dim, activation='relu')\n",
        "        ])\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(input_dim, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "    def call(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "# Setup the autoencoder parameters\n",
        "input_dim = X_stylometry_normalized.shape[1]\n",
        "encoding_dim = 100\n",
        "\n",
        "# Instantiate and compile the SDAE\n",
        "autoencoder = DenoisingAutoencoder(input_dim, encoding_dim)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "# Introduce noise and train the SDAE\n",
        "noise_factor = 0.5\n",
        "X_stylometry_noisy = X_stylometry_normalized + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_stylometry_normalized.shape)\n",
        "\n",
        "autoencoder.fit(X_stylometry_noisy, X_stylometry_normalized,\n",
        "                epochs=50,\n",
        "                batch_size=256,\n",
        "                shuffle=True)\n",
        "\n",
        "# Extract new stylometric features from the encoder\n",
        "X_stylometry_encoded = autoencoder.encoder(X_stylometry_normalized).numpy()\n",
        "\n",
        "# Combine TF-IDF features with new stylometric features\n",
        "X_combined = np.hstack((tfidf_features, X_stylometry_encoded))\n",
        "\n",
        "# Prepare labels\n",
        "y = data['label'].values\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_combined, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Train an SVM model\n",
        "clf = SVC(kernel=\"linear\", probability=True)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set and print the accuracy and classification report\n",
        "y_pred = clf.predict(X_val)\n",
        "accuracy = accuracy_score(y_val, y_pred)\n",
        "print(f'Validation Accuracy: {accuracy}')\n",
        "print(classification_report(y_val, y_pred))\n",
        "\n",
        "from joblib import dump\n",
        "import os\n",
        "\n",
        "\n",
        "save_dir = 'svm_sdae'\n",
        "if not os.path.exists(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "\n",
        "# Save the models and components\n",
        "dump(tfidf_vectorizer, os.path.join(save_dir, 'tfidf_vectorizer.joblib'))\n",
        "dump(scaler, os.path.join(save_dir, 'scaler.joblib'))\n",
        "dump(clf, os.path.join(save_dir, 'svm_model.joblib'))\n",
        "\n",
        "# Save the Denoising Autoencoder\n",
        "autoencoder.save(os.path.join(save_dir, 'denoising_autoencoder'))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JMIFK2uafFR",
        "outputId": "7c863f5a-d6fa-4aa2-ab53-03e3074d9c45"
      },
      "id": "8JMIFK2uafFR",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "20/20 [==============================] - 1s 2ms/step - loss: 0.0332\n",
            "Epoch 2/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0198\n",
            "Epoch 3/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0155\n",
            "Epoch 4/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0143\n",
            "Epoch 5/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0139\n",
            "Epoch 6/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0136\n",
            "Epoch 7/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0134\n",
            "Epoch 8/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0133\n",
            "Epoch 9/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0132\n",
            "Epoch 10/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0131\n",
            "Epoch 11/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0131\n",
            "Epoch 12/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0130\n",
            "Epoch 13/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0130\n",
            "Epoch 14/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0129\n",
            "Epoch 15/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0129\n",
            "Epoch 16/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0129\n",
            "Epoch 17/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 18/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 19/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 20/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 21/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 22/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 23/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0128\n",
            "Epoch 24/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 25/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 26/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 27/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 28/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 29/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 30/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 31/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 32/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 33/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 34/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 35/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 36/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 37/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 38/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 39/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0126\n",
            "Epoch 40/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0127\n",
            "Epoch 41/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0126\n",
            "Epoch 42/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0126\n",
            "Epoch 43/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0126\n",
            "Epoch 44/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0126\n",
            "Epoch 45/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0126\n",
            "Epoch 46/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0126\n",
            "Epoch 47/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0126\n",
            "Epoch 48/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0126\n",
            "Epoch 49/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0126\n",
            "Epoch 50/50\n",
            "20/20 [==============================] - 0s 2ms/step - loss: 0.0126\n",
            "Validation Accuracy: 0.7986928104575164\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.93      0.89        30\n",
            "           1       1.00      0.83      0.91        30\n",
            "           2       0.70      0.63      0.67        30\n",
            "           3       0.48      0.53      0.51        30\n",
            "           4       0.84      0.70      0.76        30\n",
            "           5       0.72      0.93      0.81        30\n",
            "           6       1.00      0.93      0.97        30\n",
            "           7       0.79      0.87      0.83        30\n",
            "           8       0.81      1.00      0.90        30\n",
            "           9       0.66      0.63      0.64        30\n",
            "          10       0.97      1.00      0.98        30\n",
            "          11       0.76      0.73      0.75        30\n",
            "          12       0.93      0.90      0.92        30\n",
            "          13       0.41      0.37      0.39        30\n",
            "          14       0.77      0.77      0.77        30\n",
            "          15       1.00      0.93      0.97        30\n",
            "          16       0.88      0.73      0.80        30\n",
            "          17       0.68      0.77      0.72        30\n",
            "          18       0.93      0.87      0.90        30\n",
            "          19       0.90      0.87      0.88        30\n",
            "          20       1.00      1.00      1.00        30\n",
            "          21       0.88      0.93      0.90        30\n",
            "          22       0.83      0.80      0.81        30\n",
            "          23       0.71      0.80      0.75        30\n",
            "          24       0.84      0.70      0.76        30\n",
            "          25       0.87      0.90      0.89        30\n",
            "          26       0.96      0.80      0.87        30\n",
            "          27       0.85      0.73      0.79        30\n",
            "          28       0.94      1.00      0.97        30\n",
            "          29       0.78      0.93      0.85        30\n",
            "          30       0.79      0.87      0.83        30\n",
            "          31       0.81      0.73      0.77        30\n",
            "          32       1.00      1.00      1.00        30\n",
            "          33       0.90      0.90      0.90        30\n",
            "          34       0.60      0.70      0.65        30\n",
            "          35       0.80      0.93      0.86        30\n",
            "          36       0.84      0.87      0.85        30\n",
            "          37       0.55      0.70      0.62        30\n",
            "          38       0.95      0.70      0.81        30\n",
            "          39       0.92      0.77      0.84        30\n",
            "          40       1.00      0.97      0.98        30\n",
            "          41       0.59      0.80      0.68        30\n",
            "          42       0.79      0.77      0.78        30\n",
            "          43       0.52      0.43      0.47        30\n",
            "          44       0.77      0.77      0.77        30\n",
            "          45       0.50      0.47      0.48        30\n",
            "          46       0.96      0.77      0.85        30\n",
            "          47       0.79      0.90      0.84        30\n",
            "          48       0.85      0.77      0.81        30\n",
            "          49       0.43      0.40      0.41        30\n",
            "          50       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           0.80      1530\n",
            "   macro avg       0.81      0.80      0.80      1530\n",
            "weighted avg       0.81      0.80      0.80      1530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre Trained Large Language Model"
      ],
      "metadata": {
        "id": "bCbeSAgu3GGw"
      },
      "id": "bCbeSAgu3GGw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9122b34-81e5-45a6-bc4c-42a44849b8e0",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-03-12T11:42:17.859648Z",
          "iopub.status.busy": "2024-03-12T11:42:17.859138Z"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "3e4077c564254d659d9676a9e537a661",
            "8de8c5d982de48769db21381a3588103",
            "27b253e716424dd8bbabd4c392cafed5",
            "f257f150ac904743a135d4a6befd4a3e",
            "0c74e6c437b64316831a059553c19ec1",
            "d1344389791c47f89e21c1a910e7ce9a",
            "def864be73874f3485545d8d6844575d",
            "d0583237535e4e539c05a876d33d800b",
            "6855c5abd7d94a7aa1b198fe7d3fb71c",
            "b22ca82fdde447c1a43a955ca3761f2f",
            "c4a9c36f77bf4acfaa4954d6f2cb1fe6",
            "8ccc7097fc404c6e9bc3418d36fc2abf",
            "f47e5571535944209d7aef28d9d8e2ed",
            "abe1138bb6cf402cba789355fc307408",
            "c66d2c7de4254f6e914ffae9e322f69b",
            "e108b093dc5d4730b1c846ccb30b7f98",
            "3cd56b5fe652418585c86482c906e00e",
            "e9d955186a634b38a581a5b28ba180fc",
            "da0e55b305854ef1996b5731c4f36b59",
            "12e4cb959fc8455a9debb0cbedf85f92",
            "894692a8417a411abd8d788ff45f1b22",
            "9e8f828dc382497dbd2daf70b077cc55",
            "48333a60a19040c89a950b20690d54ae",
            "96d49fac36974544a58cea85dd7e58bb",
            "66dcf0f94dcb41d3976e7663ba6b76c4",
            "1ab93c7bab6a41ee94a1e331dcfbdf7a",
            "49c0a1487c784609b516d4a9a922d70a",
            "b1d3cdea040942668de86452630128d1",
            "300cdcc0713b4790b8874a76dce10488",
            "86b69020fbb44283b428689f1e2db474",
            "b67607997ae14a33805cb27d24590157",
            "e8216787125440dca2a86b03ba980ad5",
            "2b63142093d841d49a68c6800e58a89a",
            "245cef7ff1a547e998c733ac3b307cf2",
            "ff8db09258ee4686b82f2c7f0e6784ca",
            "f58b9805ae324df09cdb689cf902a239",
            "e123f2103c2c4c96a8ae236281b7dbc8",
            "6cec1424306e4cc9b4ac0f1f25c78eb6",
            "0e6e434ff5034e369c57bd41fac23795",
            "d9f8236a2d47464aac4ec8822425d42d",
            "41d944d0f3b24dda9d5be4715d2df279",
            "27b7d627b2444d55ad9a5feef52d7e1e",
            "4d0aa1514dbb412e846bc66d6b9aafd4",
            "89d56bdb346940da8dc75ae98fdf38f5",
            "1caf32b40fc641e391f2a61f46561024",
            "5c4c2020fed54648a1c1935b20faccd3",
            "b617fb1a27af442aae3abb7abc78e55c",
            "0fdc02b51e8948a7aa89c01442e702a8",
            "2d8766118dec44a18c8f9bf4d1e3cf1b",
            "2533df30a5a7487b98b097ce7a08839a",
            "e2895621c0024586ba5e65c2e93005b0",
            "fab914551cea4b84b3d8dc479f1fae31",
            "4958bc259e52445d8234726525f37e01",
            "2355060533284840baf92cfd34e2a68b",
            "7a454298be0e49b2bfc9b7d868e2c0c0"
          ]
        },
        "id": "e9122b34-81e5-45a6-bc4c-42a44849b8e0",
        "outputId": "f6d97a3d-9023-4c33-903d-453a5ecf58dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e4077c564254d659d9676a9e537a661"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ccc7097fc404c6e9bc3418d36fc2abf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48333a60a19040c89a950b20690d54ae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "245cef7ff1a547e998c733ac3b307cf2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1caf32b40fc641e391f2a61f46561024"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy: 0.0196078431372549\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        21\n",
            "           1       0.00      0.00      0.00        21\n",
            "           2       0.00      0.00      0.00        21\n",
            "           3       0.00      0.00      0.00        21\n",
            "           4       0.00      0.00      0.00        21\n",
            "           5       0.00      0.00      0.00        21\n",
            "           6       0.00      0.00      0.00        21\n",
            "           7       0.00      0.00      0.00        21\n",
            "           8       0.00      0.00      0.00        21\n",
            "           9       0.00      0.00      0.00        21\n",
            "          10       0.00      0.00      0.00        21\n",
            "          11       0.00      0.00      0.00        21\n",
            "          12       0.00      0.00      0.00        21\n",
            "          13       0.00      0.00      0.00        21\n",
            "          14       0.00      0.00      0.00        21\n",
            "          15       0.00      0.00      0.00        21\n",
            "          16       0.00      0.00      0.00        21\n",
            "          17       0.00      0.00      0.00        21\n",
            "          18       0.00      0.00      0.00        21\n",
            "          19       0.00      0.00      0.00        21\n",
            "          20       0.00      0.00      0.00        21\n",
            "          21       0.00      0.00      0.00        21\n",
            "          22       0.00      0.00      0.00        21\n",
            "          23       0.00      0.00      0.00        21\n",
            "          24       0.00      0.00      0.00        21\n",
            "          25       0.00      0.00      0.00        21\n",
            "          26       0.00      0.00      0.00        21\n",
            "          27       0.00      0.00      0.00        21\n",
            "          28       0.00      0.00      0.00        21\n",
            "          29       0.00      0.00      0.00        21\n",
            "          30       0.02      1.00      0.04        21\n",
            "          31       0.00      0.00      0.00        21\n",
            "          32       0.00      0.00      0.00        21\n",
            "          33       0.00      0.00      0.00        21\n",
            "          34       0.00      0.00      0.00        21\n",
            "          35       0.00      0.00      0.00        21\n",
            "          36       0.00      0.00      0.00        21\n",
            "          37       0.00      0.00      0.00        21\n",
            "          38       0.00      0.00      0.00        21\n",
            "          39       0.00      0.00      0.00        21\n",
            "          40       0.00      0.00      0.00        21\n",
            "          41       0.00      0.00      0.00        21\n",
            "          42       0.00      0.00      0.00        21\n",
            "          43       0.00      0.00      0.00        21\n",
            "          44       0.00      0.00      0.00        21\n",
            "          45       0.00      0.00      0.00        21\n",
            "          46       0.00      0.00      0.00        21\n",
            "          47       0.00      0.00      0.00        21\n",
            "          48       0.00      0.00      0.00        21\n",
            "          49       0.00      0.00      0.00        21\n",
            "          50       0.00      0.00      0.00        21\n",
            "\n",
            "    accuracy                           0.02      1071\n",
            "   macro avg       0.00      0.02      0.00      1071\n",
            "weighted avg       0.00      0.02      0.00      1071\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "#13.distilbert no fine tuning\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Check for CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Custom Dataset class, for tokenizing and encoding the text and returning the label\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Encoding the text for DistilBERT\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(data['text'], data['label'], test_size=0.3, random_state=42, stratify=data['label'])\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42, stratify=y_temp)\n",
        "\n",
        "# Create DataLoaders\n",
        "test_dataset = TextDataset(X_test.tolist(), y_test.tolist(), tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "num_labels = data['label'].nunique()\n",
        "# Load pre-trained DistilBERT model for sequence classification\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
        "model.to(device)\n",
        "\n",
        "model.eval()\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "# Calculate overall accuracy\n",
        "overall_accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Overall Accuracy: {overall_accuracy}\")\n",
        "\n",
        "# Convert numeric labels to strings for the classification report\n",
        "class_names = [str(label) for label in sorted(data['label'].unique())]\n",
        "\n",
        "# Classification Report\n",
        "print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "torch.save(model.state_dict(), 'distilbert_nofinetuning_model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d5f7e62-516d-4ba8-95f1-bcf505bca2c1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d5f7e62-516d-4ba8-95f1-bcf505bca2c1",
        "outputId": "c6177b64-3fff-4fb8-aafa-ca8bf187f6f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy for Epoch 1: 0.4880174291938998\n",
            "Validation Accuracy for Epoch 2: 0.6383442265795207\n",
            "Validation Accuracy for Epoch 3: 0.690631808278867\n",
            "Test Accuracy: 0.6797385620915033\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.95      0.89        21\n",
            "           1       0.40      0.90      0.56        21\n",
            "           2       0.67      0.19      0.30        21\n",
            "           3       0.27      0.57      0.36        21\n",
            "           4       0.57      0.19      0.29        21\n",
            "           5       0.75      0.57      0.65        21\n",
            "           6       0.62      0.95      0.75        21\n",
            "           7       0.75      0.57      0.65        21\n",
            "           8       0.65      0.81      0.72        21\n",
            "           9       0.38      0.29      0.32        21\n",
            "          10       1.00      1.00      1.00        21\n",
            "          11       0.73      0.90      0.81        21\n",
            "          12       1.00      0.52      0.69        21\n",
            "          13       0.33      0.38      0.36        21\n",
            "          14       0.75      0.29      0.41        21\n",
            "          15       1.00      1.00      1.00        21\n",
            "          16       0.50      0.86      0.63        21\n",
            "          17       0.75      0.29      0.41        21\n",
            "          18       0.89      0.81      0.85        21\n",
            "          19       1.00      0.86      0.92        21\n",
            "          20       0.95      1.00      0.98        21\n",
            "          21       0.74      0.95      0.83        21\n",
            "          22       0.78      0.67      0.72        21\n",
            "          23       0.56      0.24      0.33        21\n",
            "          24       0.75      0.71      0.73        21\n",
            "          25       0.77      0.81      0.79        21\n",
            "          26       0.95      0.90      0.93        21\n",
            "          27       0.89      0.76      0.82        21\n",
            "          28       0.95      1.00      0.98        21\n",
            "          29       0.75      0.29      0.41        21\n",
            "          30       0.40      0.90      0.55        21\n",
            "          31       0.74      0.67      0.70        21\n",
            "          32       1.00      1.00      1.00        21\n",
            "          33       0.94      0.76      0.84        21\n",
            "          34       0.27      0.29      0.28        21\n",
            "          35       0.77      0.95      0.85        21\n",
            "          36       0.74      0.81      0.77        21\n",
            "          37       0.45      0.95      0.62        21\n",
            "          38       0.53      0.86      0.65        21\n",
            "          39       0.56      0.67      0.61        21\n",
            "          40       0.95      0.90      0.93        21\n",
            "          41       0.57      0.76      0.65        21\n",
            "          42       0.81      0.62      0.70        21\n",
            "          43       0.62      0.24      0.34        21\n",
            "          44       0.82      0.67      0.74        21\n",
            "          45       0.75      0.14      0.24        21\n",
            "          46       0.87      0.62      0.72        21\n",
            "          47       0.77      0.81      0.79        21\n",
            "          48       0.68      0.81      0.74        21\n",
            "          49       0.00      0.00      0.00        21\n",
            "          50       0.95      1.00      0.98        21\n",
            "\n",
            "    accuracy                           0.68      1071\n",
            "   macro avg       0.71      0.68      0.66      1071\n",
            "weighted avg       0.71      0.68      0.66      1071\n",
            "\n",
            "Execution time:  539.156 seconds\n"
          ]
        }
      ],
      "source": [
        "#14.distilbert fine tuned\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import time\n",
        "\n",
        "# Check for CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Custom Dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "start_time=time.time()\n",
        "\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(data['text'], data['label'], test_size=0.3, random_state=42, stratify=data['label'])\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42, stratify=y_temp)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = TextDataset(X_train.tolist(), y_train.tolist(), tokenizer)\n",
        "val_dataset = TextDataset(X_val.tolist(), y_val.tolist(), tokenizer)\n",
        "test_dataset = TextDataset(X_test.tolist(), y_test.tolist(), tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# Load pre-trained DistilBERT model for sequence classification\n",
        "num_labels = data['label'].nunique()\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
        "model.to(device)\n",
        "\n",
        "# Training\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "num_epochs = 3\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_labels = []\n",
        "    val_preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
        "    print(f\"Validation Accuracy for Epoch {epoch+1}: {val_accuracy}\")\n",
        "\n",
        "# Save the fine-tuned model weights and the whole model\n",
        "torch.save(model.state_dict(), 'distilbert_finetuned_weight.pth')\n",
        "\n",
        "\n",
        "# Evaluation on test set\n",
        "model.eval()\n",
        "test_labels = []\n",
        "test_preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "        test_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "# Calculate overall accuracy and generate classification report\n",
        "overall_accuracy = accuracy_score(test_labels, test_preds)\n",
        "print(f\"Test Accuracy: {overall_accuracy}\")\n",
        "class_names = [str(label) for label in sorted(data['label'].unique())]  # Ensure class names match data\n",
        "print(classification_report(test_labels, test_preds, target_names=class_names))\n",
        "\n",
        "end_time=time.time()\n",
        "print(f\"Execution time: {end_time-start_time: .3f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15.distilbert but try with diff number of epochs, if 3=0.66,\n",
        "#distilbert fine tuned\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import time\n",
        "\n",
        "# Check for CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Custom Dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "start_time=time.time()\n",
        "\n",
        "# Load  data\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "\n",
        "# Splitting the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.3, random_state=42, stratify=data['label'])\n",
        "#X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42, stratify=y_temp)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = TextDataset(X_train.tolist(), y_train.tolist(), tokenizer)\n",
        "val_dataset = TextDataset(X_val.tolist(), y_val.tolist(), tokenizer)\n",
        "test_dataset = TextDataset(X_test.tolist(), y_test.tolist(), tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# Load pre-trained DistilBERT model for sequence classification\n",
        "num_labels = data['label'].nunique()\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
        "model.to(device)\n",
        "\n",
        "# Training\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_labels = []\n",
        "    val_preds = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "            val_labels.extend(labels.cpu().numpy())\n",
        "            val_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "    val_accuracy = accuracy_score(val_labels, val_preds)\n",
        "    print(f\"Validation Accuracy for Epoch {epoch+1}: {val_accuracy}\")\n",
        "\n",
        "# Save the fine-tuned model weights and the whole model\n",
        "torch.save(model.state_dict(), 'distilbert_finetuned_weight_5epochs.pth')\n",
        "\n",
        "# Evaluation on test set\n",
        "model.eval()\n",
        "test_labels = []\n",
        "test_preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "        test_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "# Calculate overall accuracy and generate classification report\n",
        "overall_accuracy = accuracy_score(test_labels, test_preds)\n",
        "print(f\"Test Accuracy: {overall_accuracy}\")\n",
        "class_names = [str(label) for label in sorted(data['label'].unique())]  # Ensure class names match data\n",
        "print(classification_report(test_labels, test_preds, target_names=class_names))\n",
        "\n",
        "end_time=time.time()\n",
        "print(f\"Execution time: {end_time-start_time: .3f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WNW7AjwyUMi",
        "outputId": "74d5252f-e51f-4461-d2d0-f1cab6a70833"
      },
      "id": "5WNW7AjwyUMi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy for Epoch 1: 0.5599128540305011\n",
            "Validation Accuracy for Epoch 2: 0.6230936819172114\n",
            "Validation Accuracy for Epoch 3: 0.664488017429194\n",
            "Validation Accuracy for Epoch 4: 0.7211328976034859\n",
            "Validation Accuracy for Epoch 5: 0.7189542483660131\n",
            "Test Accuracy: 0.7320261437908496\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.87      0.90        30\n",
            "           1       0.81      0.97      0.88        30\n",
            "           2       0.69      0.60      0.64        30\n",
            "           3       0.32      0.37      0.34        30\n",
            "           4       0.88      0.23      0.37        30\n",
            "           5       0.79      0.73      0.76        30\n",
            "           6       0.84      0.87      0.85        30\n",
            "           7       0.77      0.67      0.71        30\n",
            "           8       0.68      0.87      0.76        30\n",
            "           9       0.49      0.63      0.55        30\n",
            "          10       1.00      1.00      1.00        30\n",
            "          11       0.81      0.97      0.88        30\n",
            "          12       0.85      0.97      0.91        30\n",
            "          13       0.36      0.63      0.46        30\n",
            "          14       1.00      0.10      0.18        30\n",
            "          15       0.97      0.93      0.95        30\n",
            "          16       0.86      0.60      0.71        30\n",
            "          17       0.49      0.83      0.62        30\n",
            "          18       0.79      0.90      0.84        30\n",
            "          19       0.92      0.80      0.86        30\n",
            "          20       0.97      0.97      0.97        30\n",
            "          21       0.84      0.87      0.85        30\n",
            "          22       0.66      0.77      0.71        30\n",
            "          23       0.51      0.73      0.60        30\n",
            "          24       0.74      0.57      0.64        30\n",
            "          25       0.71      0.83      0.77        30\n",
            "          26       0.96      0.80      0.87        30\n",
            "          27       0.79      0.77      0.78        30\n",
            "          28       1.00      1.00      1.00        30\n",
            "          29       0.82      0.60      0.69        30\n",
            "          30       0.69      0.83      0.76        30\n",
            "          31       0.86      0.80      0.83        30\n",
            "          32       0.97      1.00      0.98        30\n",
            "          33       0.86      0.80      0.83        30\n",
            "          34       0.30      0.10      0.15        30\n",
            "          35       0.66      0.90      0.76        30\n",
            "          36       0.79      0.87      0.83        30\n",
            "          37       0.49      0.87      0.63        30\n",
            "          38       0.62      0.83      0.71        30\n",
            "          39       0.85      0.73      0.79        30\n",
            "          40       0.94      0.97      0.95        30\n",
            "          41       0.82      0.77      0.79        30\n",
            "          42       0.61      0.73      0.67        30\n",
            "          43       0.31      0.37      0.33        30\n",
            "          44       0.80      0.80      0.80        30\n",
            "          45       0.00      0.00      0.00        30\n",
            "          46       0.87      0.67      0.75        30\n",
            "          47       0.89      0.80      0.84        30\n",
            "          48       0.70      0.77      0.73        30\n",
            "          49       0.62      0.33      0.43        30\n",
            "          50       1.00      0.97      0.98        30\n",
            "\n",
            "    accuracy                           0.73      1530\n",
            "   macro avg       0.74      0.73      0.72      1530\n",
            "weighted avg       0.74      0.73      0.72      1530\n",
            "\n",
            "Execution time:  889.896 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16.roberta fine tuned\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "\n",
        "# Check for CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "\n",
        "# Custom Dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Encoding the text for RoBERTa\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.3, random_state=42, stratify=data['label'])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = TextDataset(X_train.tolist(), y_train.tolist(), tokenizer)\n",
        "test_dataset = TextDataset(X_test.tolist(), y_test.tolist(), tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "num_labels=data['label'].nunique()\n",
        "# Load pre-trained RoBERTa model for sequence classification\n",
        "model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels).to(device)\n",
        "\n",
        "# Training\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "num_epochs = 5\n",
        "\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['label'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        _, preds = torch.max(outputs.logits, dim=1)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "# Calculate overall accuracy\n",
        "overall_accuracy = accuracy_score(all_labels, all_preds)\n",
        "print(f\"Overall Accuracy: {overall_accuracy}\")\n",
        "\n",
        "# Convert numeric labels to strings for the classification report\n",
        "class_names = [str(label) for label in data['label'].unique()]\n",
        "\n",
        "# Classification Report\n",
        "print(classification_report(all_labels, all_preds, target_names=class_names))\n",
        "torch.save(model.state_dict(), 'roberta_finetuned_weight_5epochs.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "d5187545c65845718ae7b85b14d23aef",
            "ff186aa3696f438c82b1e472769aa0bf",
            "1ddcce33603f4e0d9251ed7ef77f8bb4",
            "a6ca013b8a9143fd97cc4ace9f3d44eb",
            "7a048262f77e47bf8e0c223374949d00",
            "e24b6d371e574d549fcc88bda6c28bfa",
            "8f29fa711da44fa492aae72f5e545ad9",
            "fbfb861b9f6a45fc98daf7ec0ec1e0ed",
            "e3478a4cb73f422d9f7da32850fd0263",
            "1dcc77efdfcd4a99b9660852d29a6a93",
            "ccdd0f705c2c4dc0bc2e2f40da42af70",
            "74a6edd44feb4962ac0c36e4133f1b15",
            "01de6fd5c4f94431a9a9f71e71ee2899",
            "fef6b7d83d154b808af242bceb3c05e3",
            "0e4dcfeb65da4d588ef291620fd6beca",
            "625a9ac712f3483e8e2e0b7ef87e7250",
            "fd8e268575ba4c9e84d0687f61cbaed6",
            "bf00d741d2e7497395fc48f220d31210",
            "16e330cea71f4781bdcbe4a825bdbf72",
            "346ef7bc08a640ae8b6d70a5573e3a05",
            "aa775951e8a44075b18d07f8d6fbb20e",
            "3f8c460bde484dc39525ddcd9a2cd6b3",
            "e14f0d7ed08a454f8c8c46179443f849",
            "306697d7aab744a6840f0930ace6cdee",
            "1b868d5ed1f7436a9101ba9714121575",
            "6aeb10e2ca5d4686b932f3f77049abbf",
            "fc03084d822e40629a6c5e64aed48722",
            "adcf218e347b413ea305730e7835b7e9",
            "d8ae5a57eba646bab6aeeeb7a7ea39bf",
            "eaea7db6ca494ee194d8e48ca2232495",
            "a2f5f6013a7649339b2a7a33246056ec",
            "41ae981540d24cdd84b18171dba640ec",
            "5c5b2ec0102b4300972dae852f8d67d5",
            "e2a402058514497480c63fdea50a7b0a",
            "0b200710fa7f406fae8314b172c056c0",
            "8306d5c54b1b49f0a7c86017847f604e",
            "ec2eb65b361b4a58a03eaeed65161dc7",
            "c200c2b857bf451cb9b9eac3b2ee5395",
            "c28c8e5586dd4e3fb8815624eae66c76",
            "0248373c416c405dbdc8fb13b15bf937",
            "a5fb4e47d63e4b6191009f0faac5f170",
            "90f9831538f74acc80db28ad2f40dba9",
            "6f3cf2c788544bbc9823ed90131f7fab",
            "2e64a0f51bc1496eae90f72e136389c6",
            "ff1587237fdc4723b2da8f262048b464",
            "cc3fbc16020541deac253c5e70955aef",
            "07f652d7233a4e84b72c223c92855858",
            "7e1f2c65cef14374a12f016288a010a0",
            "81721c5260584bb7969ba40861b41850",
            "4bd8088da1fb47ffa6ce567d0f639e09",
            "5e573dd3f8f546439df31c1c8cd4171f",
            "f9dfbc1407f845719f0e6d878bdb6247",
            "1c29bb312f9f495bbb61c8037d361c1f",
            "482bed4fdfd44e12b6db51efce190704",
            "d0d6e93ace444e27b584058fdc93a784",
            "2cabb4211cd84154977d5dfe140546ef",
            "40edf46af7f94fb396b527326a78944d",
            "506770d16a1947a79c9b623e3752b1f1",
            "59c202eb1f9d4015b29799e3c1710f2c",
            "05f7375ae9ee4e63a7d0898d88c5ae76",
            "7b74b184050740ecabba0f5d10658e0e",
            "6e7d94b2e6fc4c2daa97c60c4a15ea13",
            "ec5d2f7e7a5540a180e6b6b7c799a9aa",
            "1d424d024180449689d32bb4071fda0e",
            "3a3c4f4990b54791ad49b5ea5884411c",
            "4bdcac30e04047fa84855ea0d5669cb0"
          ]
        },
        "id": "5Rl5dbiZwtq1",
        "outputId": "70d95e2a-23b5-4d9d-fd23-11b1ff8a7299"
      },
      "id": "5Rl5dbiZwtq1",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d5187545c65845718ae7b85b14d23aef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74a6edd44feb4962ac0c36e4133f1b15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e14f0d7ed08a454f8c8c46179443f849",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e2a402058514497480c63fdea50a7b0a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff1587237fdc4723b2da8f262048b464",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2cabb4211cd84154977d5dfe140546ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy: 0.7542483660130719\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.97      0.95        30\n",
            "           1       1.00      0.77      0.87        30\n",
            "           2       0.54      0.63      0.58        30\n",
            "           3       0.38      0.70      0.49        30\n",
            "           4       0.62      0.77      0.69        30\n",
            "           5       0.82      0.77      0.79        30\n",
            "           6       0.73      0.90      0.81        30\n",
            "           7       0.67      0.80      0.73        30\n",
            "           8       0.76      0.93      0.84        30\n",
            "           9       0.54      0.43      0.48        30\n",
            "          10       1.00      1.00      1.00        30\n",
            "          11       0.77      1.00      0.87        30\n",
            "          12       0.83      0.80      0.81        30\n",
            "          13       0.46      0.20      0.28        30\n",
            "          14       0.57      0.97      0.72        30\n",
            "          15       1.00      0.93      0.97        30\n",
            "          16       0.89      0.53      0.67        30\n",
            "          17       0.75      0.40      0.52        30\n",
            "          18       0.87      0.87      0.87        30\n",
            "          19       0.96      0.87      0.91        30\n",
            "          20       1.00      1.00      1.00        30\n",
            "          21       0.85      0.97      0.91        30\n",
            "          22       0.74      0.77      0.75        30\n",
            "          23       0.90      0.30      0.45        30\n",
            "          24       0.83      0.63      0.72        30\n",
            "          25       0.92      0.80      0.86        30\n",
            "          26       0.96      0.83      0.89        30\n",
            "          27       0.86      0.80      0.83        30\n",
            "          28       0.97      1.00      0.98        30\n",
            "          29       0.68      0.90      0.77        30\n",
            "          30       0.67      0.93      0.78        30\n",
            "          31       1.00      0.77      0.87        30\n",
            "          32       0.97      1.00      0.98        30\n",
            "          33       0.78      0.83      0.81        30\n",
            "          34       0.33      0.53      0.41        30\n",
            "          35       0.71      0.97      0.82        30\n",
            "          36       0.84      0.70      0.76        30\n",
            "          37       0.65      0.43      0.52        30\n",
            "          38       0.89      0.57      0.69        30\n",
            "          39       0.88      0.77      0.82        30\n",
            "          40       1.00      0.97      0.98        30\n",
            "          41       0.61      0.90      0.73        30\n",
            "          42       0.67      0.87      0.75        30\n",
            "          43       0.50      0.43      0.46        30\n",
            "          44       0.79      0.77      0.78        30\n",
            "          45       0.48      0.47      0.47        30\n",
            "          46       0.66      0.83      0.74        30\n",
            "          47       0.90      0.90      0.90        30\n",
            "          48       0.75      0.60      0.67        30\n",
            "          49       0.00      0.00      0.00        30\n",
            "          50       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           0.75      1530\n",
            "   macro avg       0.76      0.75      0.74      1530\n",
            "weighted avg       0.76      0.75      0.74      1530\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Late Fusion\n",
        "combining predictions from separately trained models"
      ],
      "metadata": {
        "id": "Wab7FAzD4zXD"
      },
      "id": "Wab7FAzD4zXD"
    },
    {
      "cell_type": "code",
      "source": [
        "#17.softvoting using ml\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(data['label'])\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], y_encoded, test_size=0.3, random_state=42, stratify=y_encoded)\n",
        "\n",
        "# Initialize and fit the TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test).toarray()\n",
        "\n",
        "# Initialize machine learning models for the voting classifier\n",
        "xgb_classifier = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "rf_classifier = RandomForestClassifier()\n",
        "logistic_regression = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Combine them into a voting classifier with 'soft' voting\n",
        "voting_classifier = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_classifier),\n",
        "        ('rf', rf_classifier),\n",
        "        ('lr', logistic_regression)\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# Fit the voting classifier on the TF-IDF features\n",
        "voting_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Predictions using the voting classifier\n",
        "predictions = voting_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluation\n",
        "accuracy = accuracy_score(y_test, predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "\n",
        "# Convert class names to strings if they are not already\n",
        "class_names = [str(label) for label in label_encoder.inverse_transform(np.unique(y_test))]\n",
        "\n",
        "# Now print the classification report\n",
        "print(classification_report(y_test, predictions, target_names=class_names))\n",
        "\n",
        "\n",
        "# Save the voting classifier\n",
        "joblib.dump(voting_classifier, 'voting_classifier_xgb_rf_lr.pkl')\n",
        "\n",
        "# Save the label encoder\n",
        "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "\n",
        "# Save the TF-IDF vectorizer\n",
        "joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rh16QuuaCzSw",
        "outputId": "0478eaa3-d994-44d8-f96b-5dccf0dc00f5"
      },
      "id": "rh16QuuaCzSw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7313725490196078\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.83      0.85        30\n",
            "           1       0.92      0.77      0.84        30\n",
            "           2       0.52      0.77      0.62        30\n",
            "           3       0.47      0.47      0.47        30\n",
            "           4       0.79      0.73      0.76        30\n",
            "           5       0.59      0.63      0.61        30\n",
            "           6       0.80      0.80      0.80        30\n",
            "           7       0.63      0.87      0.73        30\n",
            "           8       0.59      0.57      0.58        30\n",
            "           9       0.62      0.33      0.43        30\n",
            "          10       0.97      1.00      0.98        30\n",
            "          11       0.81      1.00      0.90        30\n",
            "          12       0.76      0.87      0.81        30\n",
            "          13       0.44      0.37      0.40        30\n",
            "          14       0.71      0.73      0.72        30\n",
            "          15       1.00      0.93      0.97        30\n",
            "          16       0.73      0.53      0.62        30\n",
            "          17       0.68      0.77      0.72        30\n",
            "          18       0.73      0.80      0.76        30\n",
            "          19       0.75      0.80      0.77        30\n",
            "          20       1.00      1.00      1.00        30\n",
            "          21       0.92      0.80      0.86        30\n",
            "          22       0.60      0.80      0.69        30\n",
            "          23       0.61      0.63      0.62        30\n",
            "          24       0.82      0.60      0.69        30\n",
            "          25       0.69      0.83      0.76        30\n",
            "          26       0.89      0.83      0.86        30\n",
            "          27       0.88      0.73      0.80        30\n",
            "          28       0.83      1.00      0.91        30\n",
            "          29       0.70      0.87      0.78        30\n",
            "          30       0.77      0.80      0.79        30\n",
            "          31       0.67      0.73      0.70        30\n",
            "          32       0.97      1.00      0.98        30\n",
            "          33       0.88      0.70      0.78        30\n",
            "          34       0.42      0.47      0.44        30\n",
            "          35       0.79      0.87      0.83        30\n",
            "          36       0.80      0.67      0.73        30\n",
            "          37       0.54      0.67      0.60        30\n",
            "          38       0.84      0.70      0.76        30\n",
            "          39       0.78      0.70      0.74        30\n",
            "          40       0.88      0.93      0.90        30\n",
            "          41       0.65      0.67      0.66        30\n",
            "          42       0.68      0.77      0.72        30\n",
            "          43       0.50      0.37      0.42        30\n",
            "          44       0.70      0.70      0.70        30\n",
            "          45       0.60      0.50      0.55        30\n",
            "          46       0.68      0.77      0.72        30\n",
            "          47       0.85      0.77      0.81        30\n",
            "          48       0.73      0.53      0.62        30\n",
            "          49       0.50      0.33      0.40        30\n",
            "          50       0.94      1.00      0.97        30\n",
            "\n",
            "    accuracy                           0.73      1530\n",
            "   macro avg       0.73      0.73      0.73      1530\n",
            "weighted avg       0.73      0.73      0.73      1530\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tfidf_vectorizer.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39ee28aa-7398-4368-ad67-5989494cfb2b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39ee28aa-7398-4368-ad67-5989494cfb2b",
        "outputId": "a4cf849e-ed29-40d1-dab9-82b844cffcf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy (Soft Voting): 0.8084967320261438\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.93      0.95        30\n",
            "           1       1.00      0.90      0.95        30\n",
            "           2       0.60      0.60      0.60        30\n",
            "           3       0.51      0.60      0.55        30\n",
            "           4       0.82      0.77      0.79        30\n",
            "           5       0.84      0.87      0.85        30\n",
            "           6       0.85      0.93      0.89        30\n",
            "           7       0.75      0.80      0.77        30\n",
            "           8       0.81      0.97      0.88        30\n",
            "           9       0.69      0.60      0.64        30\n",
            "          10       1.00      1.00      1.00        30\n",
            "          11       0.77      1.00      0.87        30\n",
            "          12       0.97      0.97      0.97        30\n",
            "          13       0.48      0.53      0.51        30\n",
            "          14       0.78      0.83      0.81        30\n",
            "          15       1.00      1.00      1.00        30\n",
            "          16       0.86      0.63      0.73        30\n",
            "          17       0.81      0.83      0.82        30\n",
            "          18       0.87      0.90      0.89        30\n",
            "          19       0.96      0.87      0.91        30\n",
            "          20       1.00      1.00      1.00        30\n",
            "          21       0.85      0.97      0.91        30\n",
            "          22       0.83      0.80      0.81        30\n",
            "          23       0.84      0.70      0.76        30\n",
            "          24       0.82      0.60      0.69        30\n",
            "          25       0.87      0.87      0.87        30\n",
            "          26       0.96      0.83      0.89        30\n",
            "          27       0.83      0.83      0.83        30\n",
            "          28       0.97      1.00      0.98        30\n",
            "          29       0.76      0.87      0.81        30\n",
            "          30       0.75      0.90      0.82        30\n",
            "          31       0.86      0.83      0.85        30\n",
            "          32       1.00      1.00      1.00        30\n",
            "          33       0.97      0.93      0.95        30\n",
            "          34       0.50      0.50      0.50        30\n",
            "          35       0.72      0.97      0.83        30\n",
            "          36       0.90      0.90      0.90        30\n",
            "          37       0.57      0.80      0.67        30\n",
            "          38       0.88      0.73      0.80        30\n",
            "          39       0.93      0.83      0.88        30\n",
            "          40       1.00      0.97      0.98        30\n",
            "          41       0.68      0.87      0.76        30\n",
            "          42       0.71      0.83      0.77        30\n",
            "          43       0.48      0.43      0.46        30\n",
            "          44       0.77      0.77      0.77        30\n",
            "          45       0.53      0.30      0.38        30\n",
            "          46       0.80      0.80      0.80        30\n",
            "          47       0.87      0.87      0.87        30\n",
            "          48       0.73      0.73      0.73        30\n",
            "          49       0.62      0.27      0.37        30\n",
            "          50       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           0.81      1530\n",
            "   macro avg       0.81      0.81      0.80      1530\n",
            "weighted avg       0.81      0.81      0.80      1530\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#18.softvoting svm + roberta +distilbert\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import joblib\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "\n",
        "# Download necessary resources from NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Set the computation device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.3, random_state=42, stratify=data['label'])\n",
        "\n",
        "# Load trained models and vectorizers\n",
        "svm_model = joblib.load('svm_tfidf.pkl')\n",
        "tfidf_vectorizer = joblib.load('tfidf_vectorizer_svm.pkl')\n",
        "\n",
        "num_labels = len(np.unique(y_train))  # Get the number of unique labels\n",
        "\n",
        "# Load and prepare Roberta and DistilBert models\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels)\n",
        "roberta_model.load_state_dict(torch.load('roberta_finetuned_weight_5epochs.pth'))\n",
        "roberta_model.to(device)\n",
        "\n",
        "distilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
        "distilbert_model.load_state_dict(torch.load('distilbert_finetuned_weight_5epochs.pth'))\n",
        "distilbert_model.to(device)\n",
        "\n",
        "# Encode labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y_test_encoded = label_encoder.fit_transform(y_test)\n",
        "\n",
        "# Transform X_test data for SVM\n",
        "X_test_transformed = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Predict with SVM\n",
        "svm_probs = svm_model.predict_proba(X_test_transformed)\n",
        "\n",
        "# Function to generate predictions from BERT-based models\n",
        "def get_model_predictions(model, tokenizer, texts):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    for text in texts:\n",
        "        encoded_dict = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded_dict)\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=1).cpu().numpy()\n",
        "        predictions.append(probs)\n",
        "    return np.vstack(predictions)\n",
        "\n",
        "# Generate predictions for Roberta and DistilBert\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "roberta_probs = get_model_predictions(roberta_model, roberta_tokenizer, X_test.tolist())\n",
        "distilbert_probs = get_model_predictions(distilbert_model, distilbert_tokenizer, X_test.tolist())\n",
        "\n",
        "# Combine predictions from all models for soft voting\n",
        "average_probs = (svm_probs + roberta_probs + distilbert_probs) / 3\n",
        "\n",
        "# Determine the final predicted class based on the highest average probability\n",
        "final_predictions = np.argmax(average_probs, axis=1)\n",
        "\n",
        "# Evaluate ensemble model\n",
        "overall_accuracy = accuracy_score(y_test_encoded, final_predictions)\n",
        "print(f\"Overall Accuracy (Soft Voting): {overall_accuracy}\")\n",
        "\n",
        "# Convert numerical predictions back to original labels and print classification report\n",
        "final_predictions_labels = label_encoder.inverse_transform(final_predictions)\n",
        "print(classification_report(y_test, final_predictions_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19.soft voting using svm sdae, roberta, distilbert\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from collections import Counter\n",
        "import textstat\n",
        "import re\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "from joblib import load\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Set the computation device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Function definitions\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "def extract_stylometric_features(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    sentences = sent_tokenize(text)\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(tokens)\n",
        "    num_unique_words = len(set(tokens))\n",
        "    lexical_diversity = num_unique_words / num_words if num_words else 0\n",
        "    avg_word_length = np.mean([len(word) for word in tokens]) if tokens else 0\n",
        "    avg_sentence_length = num_words / num_sentences if num_sentences else 0\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    pos_counts = Counter(tag for word, tag in tagged_tokens)\n",
        "    total_pos_counts = sum(pos_counts.values())\n",
        "    noun_ratio = pos_counts['NN'] / total_pos_counts if 'NN' in pos_counts else 0\n",
        "    adjective_ratio = pos_counts['JJ'] / total_pos_counts if 'JJ' in pos_counts else 0\n",
        "    verb_ratio = pos_counts['VB'] / total_pos_counts if 'VB' in pos_counts else 0\n",
        "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
        "    return np.array([lexical_diversity, avg_word_length, avg_sentence_length, noun_ratio, adjective_ratio, verb_ratio, flesch_reading_ease])\n",
        "\n",
        "def transform_text_for_svm_sdae(texts, tfidf_vectorizer, denoising_autoencoder, scaler):\n",
        "    preprocessed_texts = [preprocess_text(text) for text in texts]\n",
        "    tfidf_features = tfidf_vectorizer.transform(preprocessed_texts).toarray()\n",
        "    stylometric_features = np.array([extract_stylometric_features(text) for text in preprocessed_texts])\n",
        "    stylometric_features_normalized = scaler.transform(stylometric_features)\n",
        "    encoded_features = denoising_autoencoder.encoder.predict(stylometric_features_normalized)\n",
        "    combined_features = np.hstack((tfidf_features, encoded_features))\n",
        "    return combined_features\n",
        "\n",
        "def get_model_predictions(model, tokenizer, texts):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    for text in texts:\n",
        "        encoded_dict = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded_dict)\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=1).cpu().numpy()\n",
        "        predictions.append(probs)\n",
        "    return np.vstack(predictions)\n",
        "\n",
        "# Main code\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.3, random_state=42, stratify=data['label'])\n",
        "\n",
        "# Load trained models and components\n",
        "save_dir = 'svm_sdae'\n",
        "tfidf_vectorizer = load(os.path.join(save_dir, 'tfidf_vectorizer.joblib'))\n",
        "scaler = load(os.path.join(save_dir, 'scaler.joblib'))\n",
        "svm_sdae_model = load(os.path.join(save_dir, 'svm_model.joblib'))\n",
        "denoising_autoencoder = tf.keras.models.load_model(os.path.join(save_dir, 'denoising_autoencoder'))\n",
        "\n",
        "# Load and prepare RoBERTa and DistilBERT models\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(np.unique(y_train))).to(device)\n",
        "roberta_model.load_state_dict(torch.load('roberta_finetuned_weight_5epochs.pth'))\n",
        "\n",
        "distilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(np.unique(y_train))).to(device)\n",
        "distilbert_model.load_state_dict(torch.load('distilbert_finetuned_weight_5epochs.pth'))\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_test_encoded = label_encoder.fit_transform(y_test)\n",
        "\n",
        "# Transform X_test data for SVM_SDAE and make predictions\n",
        "X_test_transformed_for_svm_sdae = transform_text_for_svm_sdae(X_test, tfidf_vectorizer, denoising_autoencoder, scaler)\n",
        "svm_sdae_probs = svm_sdae_model.predict_proba(X_test_transformed_for_svm_sdae)\n",
        "\n",
        "# Generate predictions for RoBERTa and DistilBERT\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "roberta_probs = get_model_predictions(roberta_model, roberta_tokenizer, X_test.tolist())\n",
        "distilbert_probs = get_model_predictions(distilbert_model, distilbert_tokenizer, X_test.tolist())\n",
        "\n",
        "# Combine predictions from all models for soft voting\n",
        "average_probs = (svm_sdae_probs + roberta_probs + distilbert_probs) / 3\n",
        "\n",
        "# Determine final predicted class and evaluate ensemble model\n",
        "final_predictions = np.argmax(average_probs, axis=1)\n",
        "overall_accuracy = accuracy_score(y_test_encoded, final_predictions)\n",
        "print(f\"Overall Accuracy (Soft Voting): {overall_accuracy}\")\n",
        "final_predictions_labels = label_encoder.inverse_transform(final_predictions)\n",
        "print(classification_report(y_test, final_predictions_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qXNPTwoii7s",
        "outputId": "10752683-0b8c-4d57-9ff8-6af94b2fe0d7"
      },
      "id": "0qXNPTwoii7s",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48/48 [==============================] - 0s 1ms/step\n",
            "Overall Accuracy (Soft Voting): 0.8026143790849674\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.97      0.97        30\n",
            "           1       1.00      0.87      0.93        30\n",
            "           2       0.61      0.63      0.62        30\n",
            "           3       0.47      0.63      0.54        30\n",
            "           4       0.74      0.77      0.75        30\n",
            "           5       0.86      0.83      0.85        30\n",
            "           6       0.82      0.93      0.87        30\n",
            "           7       0.73      0.80      0.76        30\n",
            "           8       0.78      0.97      0.87        30\n",
            "           9       0.61      0.57      0.59        30\n",
            "          10       1.00      1.00      1.00        30\n",
            "          11       0.77      1.00      0.87        30\n",
            "          12       0.97      0.93      0.95        30\n",
            "          13       0.50      0.47      0.48        30\n",
            "          14       0.76      0.87      0.81        30\n",
            "          15       1.00      1.00      1.00        30\n",
            "          16       0.90      0.60      0.72        30\n",
            "          17       0.77      0.77      0.77        30\n",
            "          18       0.87      0.90      0.89        30\n",
            "          19       0.96      0.87      0.91        30\n",
            "          20       1.00      1.00      1.00        30\n",
            "          21       0.85      0.97      0.91        30\n",
            "          22       0.77      0.80      0.79        30\n",
            "          23       0.80      0.53      0.64        30\n",
            "          24       0.81      0.57      0.67        30\n",
            "          25       0.89      0.83      0.86        30\n",
            "          26       0.96      0.83      0.89        30\n",
            "          27       0.84      0.87      0.85        30\n",
            "          28       0.97      1.00      0.98        30\n",
            "          29       0.76      0.87      0.81        30\n",
            "          30       0.71      0.90      0.79        30\n",
            "          31       0.89      0.83      0.86        30\n",
            "          32       1.00      1.00      1.00        30\n",
            "          33       0.97      0.93      0.95        30\n",
            "          34       0.52      0.50      0.51        30\n",
            "          35       0.71      0.97      0.82        30\n",
            "          36       0.90      0.87      0.88        30\n",
            "          37       0.58      0.73      0.65        30\n",
            "          38       0.88      0.73      0.80        30\n",
            "          39       0.93      0.83      0.88        30\n",
            "          40       1.00      0.97      0.98        30\n",
            "          41       0.68      0.93      0.79        30\n",
            "          42       0.74      0.87      0.80        30\n",
            "          43       0.45      0.43      0.44        30\n",
            "          44       0.74      0.77      0.75        30\n",
            "          45       0.52      0.37      0.43        30\n",
            "          46       0.89      0.80      0.84        30\n",
            "          47       0.87      0.87      0.87        30\n",
            "          48       0.75      0.70      0.72        30\n",
            "          49       0.75      0.30      0.43        30\n",
            "          50       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           0.80      1530\n",
            "   macro avg       0.81      0.80      0.80      1530\n",
            "weighted avg       0.81      0.80      0.80      1530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20.soft voting xgb + roberta + distilbert\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import joblib\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "\n",
        "# Download necessary resources from NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Set the computation device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.3, random_state=42, stratify=data['label'])\n",
        "\n",
        "# Load trained models and vectorizers\n",
        "xgb_model = joblib.load('xgb_base.pkl')\n",
        "tfidf_vectorizer = joblib.load('tfidf_vectorizer_xgb.pkl')\n",
        "\n",
        "num_labels = len(np.unique(y_train))  # Get the number of unique labels\n",
        "\n",
        "# Load and prepare Roberta and DistilBert models\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels)\n",
        "roberta_model.load_state_dict(torch.load('roberta_finetuned_weight_5epochs.pth'))\n",
        "roberta_model.to(device)\n",
        "\n",
        "distilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
        "distilbert_model.load_state_dict(torch.load('distilbert_finetuned_weight_5epochs.pth'))\n",
        "distilbert_model.to(device)\n",
        "\n",
        "# Encode labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y_test_encoded = label_encoder.fit_transform(y_test)\n",
        "\n",
        "# Transform X_test data for xgb\n",
        "X_test_transformed = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Predict with xgb\n",
        "xgb_probs = xgb_model.predict_proba(X_test_transformed)\n",
        "\n",
        "# Function to generate predictions from BERT-based models\n",
        "def get_model_predictions(model, tokenizer, texts):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    for text in texts:\n",
        "        encoded_dict = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded_dict)\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=1).cpu().numpy()\n",
        "        predictions.append(probs)\n",
        "    return np.vstack(predictions)\n",
        "\n",
        "# Generate predictions for Roberta and DistilBert\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "roberta_probs = get_model_predictions(roberta_model, roberta_tokenizer, X_test.tolist())\n",
        "distilbert_probs = get_model_predictions(distilbert_model, distilbert_tokenizer, X_test.tolist())\n",
        "\n",
        "# Combine predictions from all models for soft voting\n",
        "average_probs = (xgb_probs + roberta_probs + distilbert_probs) / 3\n",
        "\n",
        "# Determine the final predicted class based on the highest average probability\n",
        "final_predictions = np.argmax(average_probs, axis=1)\n",
        "\n",
        "# Evaluate ensemble model\n",
        "overall_accuracy = accuracy_score(y_test_encoded, final_predictions)\n",
        "print(f\"Overall Accuracy (Soft Voting): {overall_accuracy}\")\n",
        "\n",
        "# Convert numerical predictions back to original labels and print classification report\n",
        "final_predictions_labels = label_encoder.inverse_transform(final_predictions)\n",
        "print(classification_report(y_test, final_predictions_labels))\n"
      ],
      "metadata": {
        "id": "jj_B5Ke_BSjg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5b34f7a-b98f-4612-d1ce-0284266ec680"
      },
      "id": "jj_B5Ke_BSjg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.0 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy (Soft Voting): 0.8973856209150327\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.93      0.95        30\n",
            "           1       0.93      0.93      0.93        30\n",
            "           2       0.83      0.67      0.74        30\n",
            "           3       0.90      0.60      0.72        30\n",
            "           4       0.92      0.80      0.86        30\n",
            "           5       0.90      0.90      0.90        30\n",
            "           6       0.90      0.90      0.90        30\n",
            "           7       0.90      0.87      0.88        30\n",
            "           8       0.85      0.97      0.91        30\n",
            "           9       0.79      0.77      0.78        30\n",
            "          10       1.00      1.00      1.00        30\n",
            "          11       0.83      1.00      0.91        30\n",
            "          12       0.94      1.00      0.97        30\n",
            "          13       0.74      0.77      0.75        30\n",
            "          14       0.93      0.90      0.92        30\n",
            "          15       1.00      1.00      1.00        30\n",
            "          16       0.93      0.93      0.93        30\n",
            "          17       0.94      0.97      0.95        30\n",
            "          18       0.90      0.93      0.92        30\n",
            "          19       0.97      0.97      0.97        30\n",
            "          20       1.00      1.00      1.00        30\n",
            "          21       0.91      1.00      0.95        30\n",
            "          22       0.83      0.83      0.83        30\n",
            "          23       0.93      0.87      0.90        30\n",
            "          24       0.92      0.77      0.84        30\n",
            "          25       0.91      1.00      0.95        30\n",
            "          26       0.96      0.90      0.93        30\n",
            "          27       0.93      0.90      0.92        30\n",
            "          28       0.97      1.00      0.98        30\n",
            "          29       0.83      1.00      0.91        30\n",
            "          30       0.83      1.00      0.91        30\n",
            "          31       0.93      0.87      0.90        30\n",
            "          32       0.97      1.00      0.98        30\n",
            "          33       0.97      0.93      0.95        30\n",
            "          34       0.82      0.90      0.86        30\n",
            "          35       0.77      1.00      0.87        30\n",
            "          36       0.93      0.87      0.90        30\n",
            "          37       0.81      0.83      0.82        30\n",
            "          38       0.96      0.80      0.87        30\n",
            "          39       0.96      0.90      0.93        30\n",
            "          40       1.00      0.97      0.98        30\n",
            "          41       0.82      0.90      0.86        30\n",
            "          42       0.88      0.97      0.92        30\n",
            "          43       0.73      0.80      0.76        30\n",
            "          44       0.87      0.87      0.87        30\n",
            "          45       0.83      0.80      0.81        30\n",
            "          46       0.89      0.80      0.84        30\n",
            "          47       0.93      0.93      0.93        30\n",
            "          48       0.84      0.90      0.87        30\n",
            "          49       0.91      0.67      0.77        30\n",
            "          50       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           0.90      1530\n",
            "   macro avg       0.90      0.90      0.90      1530\n",
            "weighted avg       0.90      0.90      0.90      1530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 21. softvoting using 4 (svm+xgb+ roberta+distilbert)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import joblib\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "\n",
        "# Download necessary resources from NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Set the computation device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.3, random_state=42, stratify=data['label'])\n",
        "\n",
        "# Load trained models and vectorizers\n",
        "svm_model = joblib.load('svm_base.pkl')\n",
        "xgboost_model = joblib.load('xgb_base.pkl')  # Load the XGBoost model\n",
        "tfidf_vectorizer = joblib.load('tfidf_vectorizer_svm.pkl')\n",
        "\n",
        "num_labels = len(np.unique(y_train))  # Get the number of unique labels\n",
        "\n",
        "# Load and prepare Roberta and DistilBert models\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels)\n",
        "roberta_model.load_state_dict(torch.load('roberta_finetuned_weight_5epochs.pth'))\n",
        "roberta_model.to(device)\n",
        "\n",
        "distilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
        "distilbert_model.load_state_dict(torch.load('distilbert_finetuned_weight_5epochs.pth'))\n",
        "distilbert_model.to(device)\n",
        "\n",
        "# Encode labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y_test_encoded = label_encoder.fit_transform(y_test)\n",
        "\n",
        "# Transform X_test data for SVM and XGBoost\n",
        "X_test_transformed = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Predict with SVM and XGBoost\n",
        "svm_probs = svm_model.predict_proba(X_test_transformed)\n",
        "xgboost_probs = xgboost_model.predict_proba(X_test_transformed)  # Predict with XGBoost\n",
        "\n",
        "# Function to generate predictions from BERT-based models\n",
        "def get_model_predictions(model, tokenizer, texts):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    for text in texts:\n",
        "        encoded_dict = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded_dict)\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=1).cpu().numpy()\n",
        "        predictions.append(probs)\n",
        "    return np.vstack(predictions)\n",
        "\n",
        "# Generate predictions for Roberta and DistilBert\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "roberta_probs = get_model_predictions(roberta_model, roberta_tokenizer, X_test.tolist())\n",
        "distilbert_probs = get_model_predictions(distilbert_model, distilbert_tokenizer, X_test.tolist())\n",
        "\n",
        "# Combine predictions from all models for soft voting\n",
        "average_probs = (svm_probs + roberta_probs + distilbert_probs + xgboost_probs) / 4\n",
        "\n",
        "# Determine the final predicted class based on the highest average probability\n",
        "final_predictions = np.argmax(average_probs, axis=1)\n",
        "\n",
        "# Evaluate ensemble model\n",
        "overall_accuracy = accuracy_score(y_test_encoded, final_predictions)\n",
        "print(f\"Overall Accuracy (Soft Voting): {overall_accuracy}\")\n",
        "\n",
        "# Convert numerical predictions back to original labels and print classification report\n",
        "final_predictions_labels = label_encoder.inverse_transform(final_predictions)\n",
        "print(classification_report(y_test, final_predictions_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cjStE39jyOOL",
        "outputId": "78b643f1-8648-454c-e396-407e68500240"
      },
      "id": "cjStE39jyOOL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy (Soft Voting): 0.7372549019607844\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.93      0.95        30\n",
            "           1       1.00      0.87      0.93        30\n",
            "           2       0.61      0.63      0.62        30\n",
            "           3       0.64      0.30      0.41        30\n",
            "           4       0.62      0.70      0.66        30\n",
            "           5       0.88      0.73      0.80        30\n",
            "           6       0.76      0.87      0.81        30\n",
            "           7       0.70      0.77      0.73        30\n",
            "           8       0.87      0.87      0.87        30\n",
            "           9       0.58      0.47      0.52        30\n",
            "          10       0.97      1.00      0.98        30\n",
            "          11       0.79      1.00      0.88        30\n",
            "          12       0.90      0.87      0.88        30\n",
            "          13       0.55      0.20      0.29        30\n",
            "          14       0.72      0.87      0.79        30\n",
            "          15       1.00      1.00      1.00        30\n",
            "          16       0.88      0.47      0.61        30\n",
            "          17       0.84      0.70      0.76        30\n",
            "          18       0.85      0.93      0.89        30\n",
            "          19       0.96      0.87      0.91        30\n",
            "          20       0.97      1.00      0.98        30\n",
            "          21       0.88      0.97      0.92        30\n",
            "          22       0.80      0.80      0.80        30\n",
            "          23       0.64      0.30      0.41        30\n",
            "          24       0.86      0.63      0.73        30\n",
            "          25       0.86      0.83      0.85        30\n",
            "          26       0.96      0.80      0.87        30\n",
            "          27       0.85      0.73      0.79        30\n",
            "          28       1.00      1.00      1.00        30\n",
            "          29       0.71      0.83      0.77        30\n",
            "          30       0.68      0.90      0.77        30\n",
            "          31       0.89      0.80      0.84        30\n",
            "          32       0.88      1.00      0.94        30\n",
            "          33       0.93      0.90      0.92        30\n",
            "          34       0.50      0.07      0.12        30\n",
            "          35       0.70      0.93      0.80        30\n",
            "          36       0.89      0.80      0.84        30\n",
            "          37       0.60      0.70      0.65        30\n",
            "          38       0.84      0.70      0.76        30\n",
            "          39       0.96      0.83      0.89        30\n",
            "          40       1.00      0.97      0.98        30\n",
            "          41       0.75      0.90      0.82        30\n",
            "          42       0.75      0.80      0.77        30\n",
            "          43       0.50      0.03      0.06        30\n",
            "          44       0.74      0.77      0.75        30\n",
            "          45       0.70      0.23      0.35        30\n",
            "          46       0.79      0.77      0.78        30\n",
            "          47       0.87      0.87      0.87        30\n",
            "          48       0.71      0.67      0.69        30\n",
            "          49       1.00      0.03      0.06        30\n",
            "          50       0.16      1.00      0.28        30\n",
            "\n",
            "    accuracy                           0.74      1530\n",
            "   macro avg       0.79      0.74      0.73      1530\n",
            "weighted avg       0.79      0.74      0.73      1530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22.softvoting using 2(svm+distilbert)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import joblib\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "\n",
        "# Download necessary resources from NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Set the computation device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.3, random_state=42, stratify=data['label'])\n",
        "\n",
        "# Load trained models and vectorizers\n",
        "svm_model = joblib.load('svm_base.pkl')\n",
        "\n",
        "tfidf_vectorizer = joblib.load('tfidf_vectorizer_svm.pkl')\n",
        "\n",
        "num_labels = len(np.unique(y_train))  # Get the number of unique labels\n",
        "\n",
        "\n",
        "distilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
        "distilbert_model.load_state_dict(torch.load('distilbert_finetuned_weight_5epochs.pth'))\n",
        "distilbert_model.to(device)\n",
        "\n",
        "# Encode labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y_test_encoded = label_encoder.fit_transform(y_test)\n",
        "\n",
        "X_test_transformed = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Predict with SVM\n",
        "svm_probs = svm_model.predict_proba(X_test_transformed)\n",
        "\n",
        "# Function to generate predictions from BERT-based models\n",
        "def get_model_predictions(model, tokenizer, texts):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    for text in texts:\n",
        "        encoded_dict = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded_dict)\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=1).cpu().numpy()\n",
        "        predictions.append(probs)\n",
        "    return np.vstack(predictions)\n",
        "\n",
        "# Generate predictions for Roberta and DistilBert\n",
        "\n",
        "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "distilbert_probs = get_model_predictions(distilbert_model, distilbert_tokenizer, X_test.tolist())\n",
        "\n",
        "# Combine predictions from all models for soft voting\n",
        "average_probs = (svm_probs + distilbert_probs) / 2\n",
        "\n",
        "# Determine the final predicted class based on the highest average probability\n",
        "final_predictions = np.argmax(average_probs, axis=1)\n",
        "\n",
        "# Evaluate ensemble model\n",
        "overall_accuracy = accuracy_score(y_test_encoded, final_predictions)\n",
        "print(f\"Overall Accuracy (Soft Voting): {overall_accuracy}\")\n",
        "\n",
        "# Convert numerical predictions back to original labels and print classification report\n",
        "final_predictions_labels = label_encoder.inverse_transform(final_predictions)\n",
        "print(classification_report(y_test, final_predictions_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHthz4n8zNvL",
        "outputId": "11fece6e-f29c-4d4e-bde0-54a53e5c7c04"
      },
      "id": "jHthz4n8zNvL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy (Soft Voting): 0.7274509803921568\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.87      0.90        30\n",
            "           1       0.81      0.97      0.88        30\n",
            "           2       0.67      0.60      0.63        30\n",
            "           3       0.31      0.37      0.34        30\n",
            "           4       0.88      0.23      0.37        30\n",
            "           5       0.78      0.70      0.74        30\n",
            "           6       0.84      0.87      0.85        30\n",
            "           7       0.77      0.67      0.71        30\n",
            "           8       0.70      0.87      0.78        30\n",
            "           9       0.49      0.63      0.55        30\n",
            "          10       1.00      1.00      1.00        30\n",
            "          11       0.81      0.97      0.88        30\n",
            "          12       0.85      0.93      0.89        30\n",
            "          13       0.36      0.60      0.45        30\n",
            "          14       1.00      0.10      0.18        30\n",
            "          15       0.97      0.93      0.95        30\n",
            "          16       0.85      0.57      0.68        30\n",
            "          17       0.49      0.83      0.62        30\n",
            "          18       0.79      0.90      0.84        30\n",
            "          19       0.92      0.80      0.86        30\n",
            "          20       0.97      0.97      0.97        30\n",
            "          21       0.81      0.87      0.84        30\n",
            "          22       0.65      0.80      0.72        30\n",
            "          23       0.51      0.77      0.61        30\n",
            "          24       0.74      0.57      0.64        30\n",
            "          25       0.69      0.83      0.76        30\n",
            "          26       0.96      0.80      0.87        30\n",
            "          27       0.79      0.77      0.78        30\n",
            "          28       1.00      1.00      1.00        30\n",
            "          29       0.82      0.60      0.69        30\n",
            "          30       0.71      0.80      0.75        30\n",
            "          31       0.86      0.80      0.83        30\n",
            "          32       0.97      1.00      0.98        30\n",
            "          33       0.77      0.80      0.79        30\n",
            "          34       0.27      0.10      0.15        30\n",
            "          35       0.68      0.90      0.77        30\n",
            "          36       0.74      0.87      0.80        30\n",
            "          37       0.49      0.87      0.63        30\n",
            "          38       0.64      0.83      0.72        30\n",
            "          39       0.85      0.73      0.79        30\n",
            "          40       0.94      0.97      0.95        30\n",
            "          41       0.82      0.77      0.79        30\n",
            "          42       0.60      0.70      0.65        30\n",
            "          43       0.29      0.37      0.32        30\n",
            "          44       0.80      0.80      0.80        30\n",
            "          45       0.00      0.00      0.00        30\n",
            "          46       0.86      0.60      0.71        30\n",
            "          47       0.89      0.80      0.84        30\n",
            "          48       0.70      0.77      0.73        30\n",
            "          49       0.64      0.30      0.41        30\n",
            "          50       1.00      0.97      0.98        30\n",
            "\n",
            "    accuracy                           0.73      1530\n",
            "   macro avg       0.74      0.73      0.71      1530\n",
            "weighted avg       0.74      0.73      0.71      1530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23. soft voting using 2(roberta +distilbert)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import joblib\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "\n",
        "# Download necessary resources from NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Set the computation device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.3, random_state=42, stratify=data['label'])\n",
        "\n",
        "\n",
        "\n",
        "num_labels = len(np.unique(y_train))  # Get the number of unique labels\n",
        "\n",
        "# Load and prepare Roberta and DistilBert models\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels)\n",
        "roberta_model.load_state_dict(torch.load('roberta_finetuned_weight_5epochs.pth'))\n",
        "roberta_model.to(device)\n",
        "\n",
        "distilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
        "distilbert_model.load_state_dict(torch.load('distilbert_finetuned_weight_5epochs.pth'))\n",
        "distilbert_model.to(device)\n",
        "\n",
        "# Encode labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y_test_encoded = label_encoder.fit_transform(y_test)\n",
        "\n",
        "\n",
        "# Function to generate predictions from BERT-based models\n",
        "def get_model_predictions(model, tokenizer, texts):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    for text in texts:\n",
        "        encoded_dict = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded_dict)\n",
        "        probs = torch.nn.functional.softmax(outputs.logits, dim=1).cpu().numpy()\n",
        "        predictions.append(probs)\n",
        "    return np.vstack(predictions)\n",
        "\n",
        "# Generate predictions for Roberta and DistilBert\n",
        "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "roberta_probs = get_model_predictions(roberta_model, roberta_tokenizer, X_test.tolist())\n",
        "distilbert_probs = get_model_predictions(distilbert_model, distilbert_tokenizer, X_test.tolist())\n",
        "\n",
        "# Combine predictions from all models for soft voting\n",
        "average_probs = (roberta_probs + distilbert_probs ) / 2\n",
        "\n",
        "# Determine the final predicted class based on the highest average probability\n",
        "final_predictions = np.argmax(average_probs, axis=1)\n",
        "\n",
        "# Evaluate ensemble model\n",
        "overall_accuracy = accuracy_score(y_test_encoded, final_predictions)\n",
        "print(f\"Overall Accuracy (Soft Voting): {overall_accuracy}\")\n",
        "\n",
        "# Convert numerical predictions back to original labels and print classification report\n",
        "final_predictions_labels = label_encoder.inverse_transform(final_predictions)\n",
        "print(classification_report(y_test, final_predictions_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ga4qJa_AzmDt",
        "outputId": "46fc9a6b-a35d-4341-8a42-5215b82a2762"
      },
      "id": "Ga4qJa_AzmDt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy (Soft Voting): 0.7790849673202614\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.93      0.95        30\n",
            "           1       1.00      0.90      0.95        30\n",
            "           2       0.54      0.63      0.58        30\n",
            "           3       0.40      0.57      0.47        30\n",
            "           4       0.64      0.70      0.67        30\n",
            "           5       0.85      0.77      0.81        30\n",
            "           6       0.76      0.87      0.81        30\n",
            "           7       0.70      0.77      0.73        30\n",
            "           8       0.82      0.93      0.87        30\n",
            "           9       0.54      0.47      0.50        30\n",
            "          10       1.00      1.00      1.00        30\n",
            "          11       0.79      1.00      0.88        30\n",
            "          12       0.90      0.87      0.88        30\n",
            "          13       0.52      0.53      0.52        30\n",
            "          14       0.74      0.93      0.82        30\n",
            "          15       1.00      1.00      1.00        30\n",
            "          16       0.89      0.53      0.67        30\n",
            "          17       0.84      0.70      0.76        30\n",
            "          18       0.85      0.93      0.89        30\n",
            "          19       0.96      0.87      0.91        30\n",
            "          20       1.00      1.00      1.00        30\n",
            "          21       0.85      0.97      0.91        30\n",
            "          22       0.73      0.80      0.76        30\n",
            "          23       0.71      0.33      0.45        30\n",
            "          24       0.83      0.63      0.72        30\n",
            "          25       0.86      0.83      0.85        30\n",
            "          26       0.96      0.83      0.89        30\n",
            "          27       0.81      0.87      0.84        30\n",
            "          28       0.97      1.00      0.98        30\n",
            "          29       0.74      0.83      0.78        30\n",
            "          30       0.67      0.93      0.78        30\n",
            "          31       0.89      0.83      0.86        30\n",
            "          32       1.00      1.00      1.00        30\n",
            "          33       0.93      0.90      0.92        30\n",
            "          34       0.33      0.30      0.32        30\n",
            "          35       0.70      0.93      0.80        30\n",
            "          36       0.86      0.83      0.85        30\n",
            "          37       0.57      0.77      0.66        30\n",
            "          38       0.84      0.70      0.76        30\n",
            "          39       0.89      0.83      0.86        30\n",
            "          40       1.00      0.97      0.98        30\n",
            "          41       0.66      0.90      0.76        30\n",
            "          42       0.69      0.83      0.76        30\n",
            "          43       0.45      0.47      0.46        30\n",
            "          44       0.79      0.77      0.78        30\n",
            "          45       0.57      0.27      0.36        30\n",
            "          46       0.79      0.77      0.78        30\n",
            "          47       0.87      0.87      0.87        30\n",
            "          48       0.71      0.67      0.69        30\n",
            "          49       0.67      0.20      0.31        30\n",
            "          50       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           0.78      1530\n",
            "   macro avg       0.79      0.78      0.77      1530\n",
            "weighted avg       0.79      0.78      0.77      1530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36a04c82-4ce5-471f-96ab-e229467ce7e1",
      "metadata": {
        "id": "36a04c82-4ce5-471f-96ab-e229467ce7e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fe48613-39fc-4621-a87b-f5b656dcda0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy (Hard Voting): 0.7196078431372549\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.97      0.94        30\n",
            "           1       0.81      0.97      0.88        30\n",
            "           2       0.56      0.67      0.61        30\n",
            "           3       0.34      0.73      0.47        30\n",
            "           4       0.42      0.67      0.51        30\n",
            "           5       0.73      0.80      0.76        30\n",
            "           6       0.68      0.93      0.79        30\n",
            "           7       0.66      0.77      0.71        30\n",
            "           8       0.69      0.97      0.81        30\n",
            "           9       0.42      0.60      0.49        30\n",
            "          10       1.00      1.00      1.00        30\n",
            "          11       0.75      1.00      0.86        30\n",
            "          12       0.89      0.80      0.84        30\n",
            "          13       0.34      0.33      0.34        30\n",
            "          14       0.62      0.80      0.70        30\n",
            "          15       1.00      0.90      0.95        30\n",
            "          16       0.89      0.57      0.69        30\n",
            "          17       0.90      0.30      0.45        30\n",
            "          18       0.81      0.87      0.84        30\n",
            "          19       0.96      0.80      0.87        30\n",
            "          20       1.00      1.00      1.00        30\n",
            "          21       0.82      0.90      0.86        30\n",
            "          22       0.46      0.73      0.56        30\n",
            "          23       0.52      0.40      0.45        30\n",
            "          24       0.81      0.57      0.67        30\n",
            "          25       0.76      0.83      0.79        30\n",
            "          26       0.96      0.77      0.85        30\n",
            "          27       0.83      0.80      0.81        30\n",
            "          28       0.97      1.00      0.98        30\n",
            "          29       0.68      0.83      0.75        30\n",
            "          30       0.78      0.83      0.81        30\n",
            "          31       0.85      0.77      0.81        30\n",
            "          32       0.97      1.00      0.98        30\n",
            "          33       0.43      0.87      0.58        30\n",
            "          34       0.32      0.20      0.24        30\n",
            "          35       0.71      0.90      0.79        30\n",
            "          36       0.57      0.80      0.67        30\n",
            "          37       0.63      0.40      0.49        30\n",
            "          38       0.89      0.57      0.69        30\n",
            "          39       0.95      0.67      0.78        30\n",
            "          40       1.00      0.97      0.98        30\n",
            "          41       0.85      0.73      0.79        30\n",
            "          42       0.73      0.73      0.73        30\n",
            "          43       0.53      0.30      0.38        30\n",
            "          44       0.88      0.77      0.82        30\n",
            "          45       0.00      0.00      0.00        30\n",
            "          46       0.86      0.60      0.71        30\n",
            "          47       0.92      0.80      0.86        30\n",
            "          48       0.81      0.57      0.67        30\n",
            "          49       0.00      0.00      0.00        30\n",
            "          50       1.00      0.97      0.98        30\n",
            "\n",
            "    accuracy                           0.72      1530\n",
            "   macro avg       0.72      0.72      0.71      1530\n",
            "weighted avg       0.72      0.72      0.71      1530\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "#24.hardvoting (svm, roberta, distilbert)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import joblib\n",
        "from scipy.stats import mode\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, DistilBertTokenizer, DistilBertForSequenceClassification\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk\n",
        "\n",
        "# Download necessary resources from NLTK\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Set the computation device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.3, random_state=42, stratify=data['label'])\n",
        "\n",
        "# Load trained models and vectorizers\n",
        "svm_model = joblib.load('svm_base.pkl')\n",
        "tfidf_vectorizer = joblib.load('tfidf_vectorizer_svm.pkl')\n",
        "\n",
        "num_labels = len(np.unique(y_train))  # Get the number of unique labels\n",
        "\n",
        "# Load and prepare Roberta and DistilBert models\n",
        "roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=num_labels)\n",
        "roberta_model.load_state_dict(torch.load('roberta_finetuned_weight_5epochs.pth'))\n",
        "roberta_model.to(device)\n",
        "\n",
        "distilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
        "distilbert_model.load_state_dict(torch.load('distilbert_finetuned_weight_5epochs.pth'))\n",
        "distilbert_model.to(device)\n",
        "\n",
        "# Encode labels to numerical values\n",
        "label_encoder = LabelEncoder()\n",
        "y_test_encoded = label_encoder.fit_transform(y_test)\n",
        "\n",
        "# Transform X_test data for SVM\n",
        "X_test_transformed = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Predict with SVM (Use predict instead of predict_proba for hard voting)\n",
        "svm_predictions = svm_model.predict(X_test_transformed)\n",
        "\n",
        "# Function to generate class predictions from BERT-based models\n",
        "def get_model_class_predictions(model, tokenizer, texts):\n",
        "    model.eval()\n",
        "    class_predictions = []\n",
        "    for text in texts:\n",
        "        encoded_dict = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512).to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**encoded_dict)\n",
        "        preds = outputs.logits.argmax(dim=1).cpu().numpy()\n",
        "        class_predictions.extend(preds)\n",
        "    return np.array(class_predictions)\n",
        "\n",
        "# Generate class predictions for Roberta and DistilBert\n",
        "roberta_predictions = get_model_class_predictions(roberta_model, roberta_tokenizer, X_test.tolist())\n",
        "distilbert_predictions = get_model_class_predictions(distilbert_model, distilbert_tokenizer, X_test.tolist())\n",
        "\n",
        "# Combine predictions from all models for hard voting\n",
        "all_predictions = np.vstack([svm_predictions, roberta_predictions, distilbert_predictions]).T\n",
        "\n",
        "# Determine the final predicted class based on the most common prediction (mode)\n",
        "final_predictions, _ = mode(all_predictions, axis=1)\n",
        "final_predictions = final_predictions.flatten()\n",
        "\n",
        "# Evaluate ensemble model\n",
        "overall_accuracy = accuracy_score(y_test_encoded, final_predictions)\n",
        "print(f\"Overall Accuracy (Hard Voting): {overall_accuracy}\")\n",
        "\n",
        "# Convert numerical predictions back to original labels and print classification report\n",
        "final_predictions_labels = label_encoder.inverse_transform(final_predictions)\n",
        "print(classification_report(y_test, final_predictions_labels))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25.soft voting using ml,takes Distilbert embedding as input instead of tfidf,\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "\n",
        "# Check for CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load data\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "\n",
        "# Initialize DistilBERT tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Custom Dataset class to process the text so that it can be use by Distilbert model\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "data['encoded_labels'] = label_encoder.fit_transform(data['label'])\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['encoded_labels'], test_size=0.3, random_state=42, stratify=data['encoded_labels'])\n",
        "\n",
        "# Create DataLoaders to allow processing by batches\n",
        "train_dataset = TextDataset(X_train.tolist(), y_train.tolist(), tokenizer)\n",
        "test_dataset = TextDataset(X_test.tolist(), y_test.tolist(), tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# Initialize the feature extraction model\n",
        "feature_extractor = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "\n",
        "def extract_features(model, data_loader, device):\n",
        "    model.eval()\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels_batch = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            features.append(outputs.last_hidden_state[:,0,:].cpu().numpy())  # Extract the embeddings of the [CLS] token\n",
        "            labels.append(labels_batch.cpu().numpy())\n",
        "\n",
        "    features = np.vstack(features)\n",
        "    labels = np.concatenate(labels)\n",
        "    return features, labels\n",
        "\n",
        "# Extract features\n",
        "train_features, train_labels = extract_features(feature_extractor, train_loader, device)\n",
        "test_features, test_labels = extract_features(feature_extractor, test_loader, device)\n",
        "\n",
        "# Initialize machine learning models for the voting classifier\n",
        "xgb_classifier = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "rf_classifier = RandomForestClassifier()\n",
        "logistic_regression = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Combine them into a voting classifier with 'soft' voting\n",
        "voting_classifier = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_classifier),\n",
        "        ('rf', rf_classifier),\n",
        "        ('lr', logistic_regression)\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# Fit the voting classifier on the extracted features\n",
        "voting_classifier.fit(train_features, train_labels)\n",
        "\n",
        "# Predictions using the voting classifier\n",
        "predictions = voting_classifier.predict(test_features)\n",
        "\n",
        "# Evaluation\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "class_names = [str(label) for label in label_encoder.inverse_transform(np.unique(test_labels))]\n",
        "print(classification_report(test_labels, predictions, target_names=class_names))\n",
        "\n",
        "# Save the voting classifier\n",
        "joblib.dump(voting_classifier, 'voting_classifier_distilbertfuse_xgb_rf_lr.pkl')\n",
        "\n",
        "# Save the label encoder\n",
        "joblib.dump(label_encoder, 'label_encoder.pkl')\n",
        "\n",
        "feature_extractor.save_pretrained('distilbert_feature_extractor')\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wxZNkdwW0fde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aba236eb-fa02-4ee1-ea5f-45f15edbd031"
      },
      "id": "wxZNkdwW0fde",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7633986928104575\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.93      0.93        30\n",
            "           1       0.93      0.93      0.93        30\n",
            "           2       0.56      0.60      0.58        30\n",
            "           3       0.55      0.53      0.54        30\n",
            "           4       0.73      0.63      0.68        30\n",
            "           5       0.77      0.80      0.79        30\n",
            "           6       0.86      0.83      0.85        30\n",
            "           7       0.71      0.80      0.75        30\n",
            "           8       0.75      0.80      0.77        30\n",
            "           9       0.54      0.43      0.48        30\n",
            "          10       1.00      1.00      1.00        30\n",
            "          11       0.79      0.90      0.84        30\n",
            "          12       0.88      0.97      0.92        30\n",
            "          13       0.45      0.47      0.46        30\n",
            "          14       0.79      0.77      0.78        30\n",
            "          15       0.91      0.97      0.94        30\n",
            "          16       0.74      0.67      0.70        30\n",
            "          17       0.70      0.70      0.70        30\n",
            "          18       0.86      0.80      0.83        30\n",
            "          19       0.87      0.90      0.89        30\n",
            "          20       1.00      0.97      0.98        30\n",
            "          21       0.82      0.90      0.86        30\n",
            "          22       0.64      0.70      0.67        30\n",
            "          23       0.74      0.67      0.70        30\n",
            "          24       0.73      0.63      0.68        30\n",
            "          25       0.74      0.87      0.80        30\n",
            "          26       0.90      0.87      0.88        30\n",
            "          27       0.85      0.77      0.81        30\n",
            "          28       0.97      1.00      0.98        30\n",
            "          29       0.72      0.87      0.79        30\n",
            "          30       0.82      0.90      0.86        30\n",
            "          31       0.88      0.70      0.78        30\n",
            "          32       0.97      1.00      0.98        30\n",
            "          33       0.83      0.83      0.83        30\n",
            "          34       0.57      0.53      0.55        30\n",
            "          35       0.66      0.90      0.76        30\n",
            "          36       0.81      0.70      0.75        30\n",
            "          37       0.58      0.63      0.60        30\n",
            "          38       0.77      0.67      0.71        30\n",
            "          39       0.78      0.70      0.74        30\n",
            "          40       1.00      0.97      0.98        30\n",
            "          41       0.65      0.67      0.66        30\n",
            "          42       0.83      0.63      0.72        30\n",
            "          43       0.48      0.43      0.46        30\n",
            "          44       0.80      0.80      0.80        30\n",
            "          45       0.44      0.37      0.40        30\n",
            "          46       0.59      0.77      0.67        30\n",
            "          47       0.83      0.83      0.83        30\n",
            "          48       0.88      0.70      0.78        30\n",
            "          49       0.49      0.57      0.52        30\n",
            "          50       0.97      0.97      0.97        30\n",
            "\n",
            "    accuracy                           0.76      1530\n",
            "   macro avg       0.77      0.76      0.76      1530\n",
            "weighted avg       0.77      0.76      0.76      1530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#26.distilbert embedding +stylo ->ml soft voting\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Check for CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "\n",
        "# Initialize DistilBERT tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Define stylometry feature extraction function\n",
        "def extract_stylometry_features(text):\n",
        "    words = word_tokenize(text)\n",
        "    sentences = sent_tokenize(text)\n",
        "    avg_word_length = np.mean([len(word) for word in words]) if words else 0\n",
        "    avg_sentence_length = np.mean([len(sent.split()) for sent in sentences]) if sentences else 0\n",
        "    sentence_count = len(sentences)\n",
        "    word_count = len(words)\n",
        "    unique_words = len(set(words))\n",
        "    return np.array([avg_word_length, avg_sentence_length, sentence_count, word_count, unique_words])\n",
        "\n",
        "# Custom Dataset class for text and stylometry features\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "        stylometry_features = extract_stylometry_features(text)\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long),\n",
        "            'stylometry_features': torch.tensor(stylometry_features, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "data['encoded_labels'] = label_encoder.fit_transform(data['label'])\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['encoded_labels'], test_size=0.3, random_state=42, stratify=data['encoded_labels'])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = TextDataset(X_train.tolist(), y_train.tolist(), tokenizer)\n",
        "test_dataset = TextDataset(X_test.tolist(), y_test.tolist(), tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# Initialize the feature extraction model\n",
        "feature_extractor = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "\n",
        "# Function to extract features and stylometry\n",
        "def extract_features_and_stylometry(model, data_loader, device):\n",
        "    model.eval()\n",
        "    all_features = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            stylometry_features = batch['stylometry_features'].to(device)\n",
        "            labels_batch = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            distilbert_features = outputs.last_hidden_state[:,0,:].cpu().numpy()\n",
        "            stylometry_features = stylometry_features.cpu().numpy()\n",
        "\n",
        "            combined_features = np.hstack((distilbert_features, stylometry_features))\n",
        "            all_features.append(combined_features)\n",
        "            labels.append(labels_batch.cpu().numpy())\n",
        "\n",
        "    all_features = np.vstack(all_features)\n",
        "    labels = np.concatenate(labels)\n",
        "    return all_features, labels\n",
        "\n",
        "# Extract features\n",
        "train_features, train_labels = extract_features_and_stylometry(feature_extractor, train_loader, device)\n",
        "test_features, test_labels = extract_features_and_stylometry(feature_extractor, test_loader, device)\n",
        "\n",
        "# Initialize machine learning models for the voting classifier\n",
        "xgb_classifier = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "rf_classifier = RandomForestClassifier()\n",
        "logistic_regression = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Combine them into a voting classifier with 'soft' voting\n",
        "voting_classifier = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('xgb', xgb_classifier),\n",
        "        ('rf', rf_classifier),\n",
        "        ('lr', logistic_regression)\n",
        "    ],\n",
        "    voting='soft'\n",
        ")\n",
        "\n",
        "# Fit the voting classifier on the extracted features\n",
        "voting_classifier.fit(train_features, train_labels)\n",
        "\n",
        "# Predictions using the voting classifier\n",
        "predictions = voting_classifier.predict(test_features)\n",
        "\n",
        "# Evaluation\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "class_names = [str(label) for label in label_encoder.inverse_transform(np.unique(test_labels))]\n",
        "print(classification_report(test_labels, predictions, target_names=class_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUKZjq7p7ZUX",
        "outputId": "89e2acb6-15c7-4ef0-ff6c-635e884322b6"
      },
      "id": "pUKZjq7p7ZUX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7568627450980392\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.90      0.90        30\n",
            "           1       0.93      0.90      0.92        30\n",
            "           2       0.57      0.57      0.57        30\n",
            "           3       0.54      0.50      0.52        30\n",
            "           4       0.73      0.63      0.68        30\n",
            "           5       0.68      0.77      0.72        30\n",
            "           6       0.87      0.87      0.87        30\n",
            "           7       0.77      0.77      0.77        30\n",
            "           8       0.75      0.80      0.77        30\n",
            "           9       0.48      0.40      0.44        30\n",
            "          10       1.00      1.00      1.00        30\n",
            "          11       0.78      0.93      0.85        30\n",
            "          12       0.88      0.93      0.90        30\n",
            "          13       0.43      0.50      0.46        30\n",
            "          14       0.70      0.77      0.73        30\n",
            "          15       0.93      0.93      0.93        30\n",
            "          16       0.67      0.60      0.63        30\n",
            "          17       0.69      0.60      0.64        30\n",
            "          18       0.89      0.80      0.84        30\n",
            "          19       0.82      0.93      0.87        30\n",
            "          20       1.00      0.97      0.98        30\n",
            "          21       0.77      0.90      0.83        30\n",
            "          22       0.68      0.70      0.69        30\n",
            "          23       0.73      0.63      0.68        30\n",
            "          24       0.74      0.57      0.64        30\n",
            "          25       0.80      0.80      0.80        30\n",
            "          26       0.93      0.90      0.92        30\n",
            "          27       0.89      0.80      0.84        30\n",
            "          28       1.00      1.00      1.00        30\n",
            "          29       0.72      0.87      0.79        30\n",
            "          30       0.79      0.90      0.84        30\n",
            "          31       0.76      0.73      0.75        30\n",
            "          32       0.94      1.00      0.97        30\n",
            "          33       0.83      0.80      0.81        30\n",
            "          34       0.44      0.53      0.48        30\n",
            "          35       0.68      0.87      0.76        30\n",
            "          36       0.74      0.67      0.70        30\n",
            "          37       0.57      0.67      0.62        30\n",
            "          38       0.78      0.70      0.74        30\n",
            "          39       0.83      0.67      0.74        30\n",
            "          40       1.00      0.97      0.98        30\n",
            "          41       0.58      0.73      0.65        30\n",
            "          42       0.83      0.67      0.74        30\n",
            "          43       0.52      0.37      0.43        30\n",
            "          44       0.75      0.80      0.77        30\n",
            "          45       0.46      0.40      0.43        30\n",
            "          46       0.67      0.80      0.73        30\n",
            "          47       0.86      0.83      0.85        30\n",
            "          48       0.86      0.80      0.83        30\n",
            "          49       0.54      0.47      0.50        30\n",
            "          50       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           0.76      1530\n",
            "   macro avg       0.76      0.76      0.75      1530\n",
            "weighted avg       0.76      0.76      0.75      1530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Early Fusion\n",
        " combining different types of features or raw data before feeding them into a learning algorithm"
      ],
      "metadata": {
        "id": "pxPJl2JhX4Fy"
      },
      "id": "pxPJl2JhX4Fy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stacking\n",
        "the output of one model (DistilBERT, serving as the base-level model) is used as input for another model"
      ],
      "metadata": {
        "id": "5qNLWatOGuJb"
      },
      "id": "5qNLWatOGuJb"
    },
    {
      "cell_type": "code",
      "source": [
        "#27.distilbert embedding with xgb\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Check for CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "\n",
        "# Initialize DistilBERT tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Custom Dataset class for text\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "data['encoded_labels'] = label_encoder.fit_transform(data['label'])\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['encoded_labels'], test_size=0.3, random_state=42, stratify=data['encoded_labels'])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_dataset = TextDataset(X_train.tolist(), y_train.tolist(), tokenizer)\n",
        "test_dataset = TextDataset(X_test.tolist(), y_test.tolist(), tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# Initialize the feature extraction model\n",
        "feature_extractor = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "\n",
        "def extract_features(model, data_loader, device):\n",
        "    model.eval()\n",
        "    features = []\n",
        "    labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels_batch = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            features.append(outputs.last_hidden_state[:,0,:].cpu().numpy())  # Extract the embeddings of the [CLS] token\n",
        "            labels.append(labels_batch.cpu().numpy())\n",
        "\n",
        "    features = np.vstack(features)\n",
        "    labels = np.concatenate(labels)\n",
        "    return features, labels\n",
        "\n",
        "# Extract features\n",
        "train_features, train_labels = extract_features(feature_extractor, train_loader, device)\n",
        "test_features, test_labels = extract_features(feature_extractor, test_loader, device)\n",
        "\n",
        "# Train XGBoost on the extracted features\n",
        "xgb_classifier = XGBClassifier()\n",
        "xgb_classifier.fit(train_features, train_labels)\n",
        "\n",
        "# Predictions using XGBoost\n",
        "predictions = xgb_classifier.predict(test_features)\n",
        "\n",
        "# Evaluation\n",
        "print(f\"Accuracy: {accuracy_score(test_labels, predictions)}\")\n",
        "\n",
        "# Correctly obtain class_names for the classification report\n",
        "# Ensure class_names is a list of strings\n",
        "class_names = [str(label) for label in label_encoder.inverse_transform(label_encoder.classes_)]\n",
        "\n",
        "# Classification Report with Corrected class_names\n",
        "print(classification_report(test_labels, predictions, target_names=class_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYImTJYj5WGB",
        "outputId": "a23af16c-a725-4fe0-b64b-1b4f139006d2"
      },
      "id": "MYImTJYj5WGB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6928104575163399\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.82      0.90      0.86        30\n",
            "           1       1.00      0.83      0.91        30\n",
            "           2       0.45      0.50      0.48        30\n",
            "           3       0.43      0.33      0.38        30\n",
            "           4       0.66      0.63      0.64        30\n",
            "           5       0.65      0.87      0.74        30\n",
            "           6       0.81      0.73      0.77        30\n",
            "           7       0.72      0.70      0.71        30\n",
            "           8       0.67      0.67      0.67        30\n",
            "           9       0.50      0.40      0.44        30\n",
            "          10       1.00      1.00      1.00        30\n",
            "          11       0.73      0.90      0.81        30\n",
            "          12       0.80      0.80      0.80        30\n",
            "          13       0.31      0.37      0.33        30\n",
            "          14       0.60      0.70      0.65        30\n",
            "          15       0.96      0.90      0.93        30\n",
            "          16       0.59      0.53      0.56        30\n",
            "          17       0.65      0.57      0.61        30\n",
            "          18       0.77      0.77      0.77        30\n",
            "          19       0.89      0.80      0.84        30\n",
            "          20       0.97      0.93      0.95        30\n",
            "          21       0.79      0.87      0.83        30\n",
            "          22       0.61      0.67      0.63        30\n",
            "          23       0.64      0.60      0.62        30\n",
            "          24       0.72      0.60      0.65        30\n",
            "          25       0.69      0.67      0.68        30\n",
            "          26       0.79      0.87      0.83        30\n",
            "          27       0.77      0.67      0.71        30\n",
            "          28       0.94      1.00      0.97        30\n",
            "          29       0.72      0.77      0.74        30\n",
            "          30       0.67      0.67      0.67        30\n",
            "          31       0.76      0.63      0.69        30\n",
            "          32       1.00      0.97      0.98        30\n",
            "          33       0.69      0.83      0.76        30\n",
            "          34       0.30      0.30      0.30        30\n",
            "          35       0.65      0.87      0.74        30\n",
            "          36       0.70      0.63      0.67        30\n",
            "          37       0.52      0.53      0.52        30\n",
            "          38       0.75      0.70      0.72        30\n",
            "          39       0.95      0.60      0.73        30\n",
            "          40       1.00      0.90      0.95        30\n",
            "          41       0.55      0.60      0.57        30\n",
            "          42       0.75      0.60      0.67        30\n",
            "          43       0.44      0.37      0.40        30\n",
            "          44       0.71      0.80      0.75        30\n",
            "          45       0.36      0.30      0.33        30\n",
            "          46       0.54      0.73      0.62        30\n",
            "          47       0.77      0.80      0.79        30\n",
            "          48       0.73      0.73      0.73        30\n",
            "          49       0.29      0.33      0.31        30\n",
            "          50       0.87      0.90      0.89        30\n",
            "\n",
            "    accuracy                           0.69      1530\n",
            "   macro avg       0.70      0.69      0.69      1530\n",
            "weighted avg       0.70      0.69      0.69      1530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#28. distilbert with logreg\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from xgboost import XGBClassifier\n",
        "import joblib\n",
        "\n",
        "# Set computation device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load dataset\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "labels = LabelEncoder().fit_transform(data['label'])\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(data['text'], labels, test_size=0.3, random_state=42, stratify=labels)\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Dataset class\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 8  # Reduced batch size\n",
        "train_dataset = TextDataset(train_texts.tolist(), train_labels.tolist(), tokenizer)\n",
        "test_dataset = TextDataset(test_texts.tolist(), test_labels.tolist(), tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "# Load and configure the DistilBert model\n",
        "distilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(np.unique(labels)))\n",
        "distilbert_model.to(device)\n",
        "distilbert_model.eval()\n",
        "\n",
        "# Function to extract features\n",
        "def extract_features(model, data_loader):\n",
        "    model.eval()\n",
        "    features = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in data_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            batch_labels = batch['labels'].to(device)\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            features.extend(outputs.logits.detach().cpu().numpy())\n",
        "            labels.extend(batch_labels.detach().cpu().numpy())\n",
        "    return np.array(features), np.array(labels)\n",
        "\n",
        "# Extract features\n",
        "train_features, train_labels = extract_features(distilbert_model, train_loader)\n",
        "test_features, test_labels = extract_features(distilbert_model, test_loader)\n",
        "\n",
        "# Define and train the meta-learner\n",
        "meta_learner = LogisticRegression(max_iter=6000)\n",
        "meta_learner.fit(train_features, train_labels)\n",
        "\n",
        "# Predictions and evaluation\n",
        "predictions = meta_learner.predict(test_features)\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "# Before generating the classification report, convert label encoder classes from numbers to strings if necessary\n",
        "class_names = [str(label) for label in LabelEncoder().fit(data['label']).classes_]\n",
        "print(classification_report(test_labels, predictions, target_names=class_names))\n",
        "\n",
        "# Save models and tokenizers\n",
        "distilbert_model.save_pretrained('saved_distilbert_model')\n",
        "meta_learner = joblib.dump(meta_learner, 'saved_meta_learner.pkl')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW-iMwb97aTk",
        "outputId": "9ce81141-e6ee-44f8-bd9f-3d85fd678032"
      },
      "id": "pW-iMwb97aTk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.94      0.50      0.65        30\n",
            "           1       0.75      0.50      0.60        30\n",
            "           2       0.46      0.20      0.28        30\n",
            "           3       0.37      0.50      0.42        30\n",
            "           4       0.47      0.27      0.34        30\n",
            "           5       0.71      0.17      0.27        30\n",
            "           6       0.50      0.10      0.17        30\n",
            "           7       0.52      0.77      0.62        30\n",
            "           8       1.00      0.03      0.06        30\n",
            "           9       0.57      0.13      0.22        30\n",
            "          10       0.68      0.93      0.79        30\n",
            "          11       0.42      0.93      0.58        30\n",
            "          12       0.82      0.47      0.60        30\n",
            "          13       0.35      0.27      0.30        30\n",
            "          14       0.43      0.83      0.57        30\n",
            "          15       0.64      0.93      0.76        30\n",
            "          16       0.36      0.57      0.44        30\n",
            "          17       0.50      0.07      0.12        30\n",
            "          18       0.47      0.77      0.58        30\n",
            "          19       0.57      0.57      0.57        30\n",
            "          20       0.47      0.90      0.61        30\n",
            "          21       0.61      0.37      0.46        30\n",
            "          22       0.64      0.23      0.34        30\n",
            "          23       0.00      0.00      0.00        30\n",
            "          24       0.52      0.53      0.52        30\n",
            "          25       0.28      0.90      0.43        30\n",
            "          26       0.42      0.83      0.56        30\n",
            "          27       0.80      0.53      0.64        30\n",
            "          28       0.77      0.80      0.79        30\n",
            "          29       0.64      0.70      0.67        30\n",
            "          30       0.48      0.33      0.39        30\n",
            "          31       0.64      0.23      0.34        30\n",
            "          32       0.56      1.00      0.71        30\n",
            "          33       0.46      0.53      0.49        30\n",
            "          34       0.33      0.10      0.15        30\n",
            "          35       0.48      0.70      0.57        30\n",
            "          36       0.67      0.47      0.55        30\n",
            "          37       0.30      0.90      0.45        30\n",
            "          38       0.58      0.50      0.54        30\n",
            "          39       0.59      0.53      0.56        30\n",
            "          40       0.74      0.83      0.78        30\n",
            "          41       0.29      0.07      0.11        30\n",
            "          42       0.67      0.13      0.22        30\n",
            "          43       0.27      0.13      0.18        30\n",
            "          44       0.47      0.67      0.55        30\n",
            "          45       0.33      0.13      0.19        30\n",
            "          46       0.60      0.20      0.30        30\n",
            "          47       0.57      0.57      0.57        30\n",
            "          48       0.29      0.33      0.31        30\n",
            "          49       0.50      0.03      0.06        30\n",
            "          50       0.36      1.00      0.53        30\n",
            "\n",
            "    accuracy                           0.48      1530\n",
            "   macro avg       0.53      0.48      0.44      1530\n",
            "weighted avg       0.53      0.48      0.44      1530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#29.distilbert stylo fusion , integration within unified model\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from collections import Counter\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Check for CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Stylometric Feature Extraction Function\n",
        "def extract_stylometric_features(text, feature_length=34):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "    num_words = len(tokens)\n",
        "    num_unique_words = len(set(tokens))\n",
        "    avg_word_length = np.mean([len(word) for word in tokens]) if tokens else 0\n",
        "    lexical_diversity = num_unique_words / num_words if num_words else 0\n",
        "    pos_counts = Counter(tag for word, tag in tagged_tokens)\n",
        "\n",
        "    pos_features = [pos_counts[tag] / num_words if num_words else 0 for tag in sorted(pos_counts)]\n",
        "\n",
        "    features = [num_words, avg_word_length, lexical_diversity] + pos_features\n",
        "    # Ensure the features array is exactly 'feature_length' long\n",
        "    features = np.array(features)\n",
        "    if len(features) > feature_length:\n",
        "        features = features[:feature_length]  # Truncate if too long\n",
        "    elif len(features) < feature_length:\n",
        "        # Extend features array with zeros (or another placeholder value) if too short\n",
        "        features = np.pad(features, (0, feature_length - len(features)), 'constant')\n",
        "\n",
        "    return features\n",
        "\n",
        "# Dataset Preparation\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512, feature_length=34):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.feature_length = feature_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "        stylometric_features = extract_stylometric_features(text, self.feature_length)\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long),\n",
        "            'stylometric_features': torch.tensor(stylometric_features, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "# Model Definition\n",
        "class DistilBertStyloModel(nn.Module):\n",
        "    def __init__(self, num_labels, num_stylometric_features):\n",
        "        super(DistilBertStyloModel, self).__init__()\n",
        "        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "        self.pre_classifier = nn.Linear(self.distilbert.config.dim, self.distilbert.config.dim)\n",
        "        self.classifier = nn.Linear(self.distilbert.config.dim + num_stylometric_features, num_labels)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, stylometric_features):\n",
        "        distilbert_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = distilbert_output[0]  # (bs, seq_len, dim)\n",
        "        pooled_output = hidden_state[:, 0]  # (bs, dim)\n",
        "        pooled_output = self.pre_classifier(pooled_output)  # (bs, dim)\n",
        "        pooled_output = nn.ReLU()(pooled_output)  # (bs, dim)\n",
        "        pooled_output = self.dropout(pooled_output)  # (bs, dim)\n",
        "        combined_features = torch.cat((pooled_output, stylometric_features), 1)\n",
        "        logits = self.classifier(combined_features)\n",
        "        return logits\n",
        "\n",
        "# Load data and preprocess\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(data['label'])\n",
        "X_train, X_val, y_train, y_val = train_test_split(data['text'], encoded_labels, test_size=0.3, stratify=encoded_labels, random_state=42)\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "train_dataset = TextDataset(X_train.tolist(), y_train.tolist(), tokenizer)\n",
        "val_dataset = TextDataset(X_val.tolist(), y_val.tolist(), tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8)\n",
        "\n",
        "model = DistilBertStyloModel(num_labels=len(label_encoder.classes_), num_stylometric_features=34)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        stylometric_features = batch['stylometric_features'].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask, stylometric_features)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Validation loop\n",
        "model.eval()\n",
        "val_labels = []\n",
        "val_preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        stylometric_features = batch['stylometric_features'].to(device)\n",
        "        outputs = model(input_ids, attention_mask, stylometric_features)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        val_labels.extend(labels.cpu().numpy())\n",
        "        val_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "# Convert the original numeric labels to text labels for a more interpretable classification report\n",
        "text_labels = [label_encoder.inverse_transform([label])[0] for label in val_labels]\n",
        "predicted_text_labels = [label_encoder.inverse_transform([pred])[0] for pred in val_preds]\n",
        "\n",
        "# Calculate and print the validation accuracy\n",
        "val_accuracy = accuracy_score(val_labels, val_preds)\n",
        "print(f\"Validation Accuracy for Epoch {epoch+1}: {val_accuracy}\")\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(text_labels, predicted_text_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQpSm4NxOhAi",
        "outputId": "f44950d6-dcc0-4ef3-965f-6906fe67ca08"
      },
      "id": "zQpSm4NxOhAi",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy for Epoch 5: 0.7673202614379085\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.93      0.87      0.90        30\n",
            "           1       0.73      1.00      0.85        30\n",
            "           2       0.72      0.60      0.65        30\n",
            "           3       0.67      0.07      0.12        30\n",
            "           4       0.77      0.57      0.65        30\n",
            "           5       0.77      0.77      0.77        30\n",
            "           6       0.76      0.83      0.79        30\n",
            "           7       0.75      0.80      0.77        30\n",
            "           8       0.64      0.97      0.77        30\n",
            "           9       0.54      0.50      0.52        30\n",
            "          10       1.00      1.00      1.00        30\n",
            "          11       0.81      0.87      0.84        30\n",
            "          12       0.76      0.97      0.85        30\n",
            "          13       0.41      0.53      0.46        30\n",
            "          14       0.93      0.90      0.92        30\n",
            "          15       1.00      0.90      0.95        30\n",
            "          16       0.67      0.67      0.67        30\n",
            "          17       0.85      0.57      0.68        30\n",
            "          18       0.93      0.87      0.90        30\n",
            "          19       0.89      0.80      0.84        30\n",
            "          20       1.00      1.00      1.00        30\n",
            "          21       0.92      0.73      0.81        30\n",
            "          22       0.63      0.87      0.73        30\n",
            "          23       0.73      0.63      0.68        30\n",
            "          24       0.77      0.57      0.65        30\n",
            "          25       0.84      0.90      0.87        30\n",
            "          26       0.95      0.70      0.81        30\n",
            "          27       0.90      0.63      0.75        30\n",
            "          28       1.00      1.00      1.00        30\n",
            "          29       0.85      0.93      0.89        30\n",
            "          30       0.74      0.97      0.84        30\n",
            "          31       0.88      0.73      0.80        30\n",
            "          32       1.00      1.00      1.00        30\n",
            "          33       0.93      0.87      0.90        30\n",
            "          34       0.43      0.20      0.27        30\n",
            "          35       0.78      0.93      0.85        30\n",
            "          36       0.76      0.83      0.79        30\n",
            "          37       0.77      0.57      0.65        30\n",
            "          38       0.92      0.77      0.84        30\n",
            "          39       1.00      0.67      0.80        30\n",
            "          40       1.00      0.97      0.98        30\n",
            "          41       0.71      0.90      0.79        30\n",
            "          42       0.86      0.60      0.71        30\n",
            "          43       0.44      0.50      0.47        30\n",
            "          44       0.76      0.83      0.79        30\n",
            "          45       0.42      0.73      0.54        30\n",
            "          46       0.79      0.77      0.78        30\n",
            "          47       0.85      0.93      0.89        30\n",
            "          48       0.81      0.73      0.77        30\n",
            "          49       0.30      0.63      0.41        30\n",
            "          50       0.97      1.00      0.98        30\n",
            "\n",
            "    accuracy                           0.77      1530\n",
            "   macro avg       0.79      0.77      0.76      1530\n",
            "weighted avg       0.79      0.77      0.76      1530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#30.roberta stylo combined , integration within single unified model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Check for CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Stylometric Feature Extraction Function\n",
        "def extract_stylometric_features(text, feature_length=34):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    num_words = len(tokens)\n",
        "    num_unique_words = len(set(tokens))\n",
        "    avg_word_length = np.mean([len(word) for word in tokens]) if tokens else 0\n",
        "    lexical_diversity = num_unique_words / num_words if num_words else 0\n",
        "    pos_counts = Counter(tag for word, tag in tagged_tokens)\n",
        "    pos_features = [pos_counts[tag] / num_words if num_words else 0 for tag in sorted(pos_counts)]\n",
        "    features = [num_words, avg_word_length, lexical_diversity] + pos_features\n",
        "    features = np.array(features)\n",
        "    if len(features) > feature_length:\n",
        "        features = features[:feature_length]\n",
        "    elif len(features) < feature_length:\n",
        "        features = np.pad(features, (0, feature_length - len(features)), 'constant')\n",
        "    return features\n",
        "\n",
        "# Dataset Preparation\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512, feature_length=34):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.feature_length = feature_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(text, add_special_tokens=True, max_length=self.max_len, return_token_type_ids=False, padding='max_length', return_attention_mask=True, return_tensors='pt', truncation=True)\n",
        "        stylometric_features = extract_stylometric_features(text, self.feature_length)\n",
        "        return {'input_ids': encoding['input_ids'].squeeze(0), 'attention_mask': encoding['attention_mask'].squeeze(0), 'labels': torch.tensor(label, dtype=torch.long), 'stylometric_features': torch.tensor(stylometric_features, dtype=torch.float)}\n",
        "\n",
        "# Model Definition\n",
        "class RobertaStyloModel(nn.Module):\n",
        "    def __init__(self, num_labels, num_stylometric_features):\n",
        "        super(RobertaStyloModel, self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-base').to(device)\n",
        "        self.classifier = nn.Linear(768 + num_stylometric_features, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, stylometric_features):\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs.last_hidden_state\n",
        "        pooled_output = sequence_output[:, 0]\n",
        "        combined_features = torch.cat((pooled_output, stylometric_features), 1)\n",
        "        logits = self.classifier(combined_features)\n",
        "        return logits\n",
        "\n",
        "# Load data and preprocess\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(data['label'])\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(data['text'], encoded_labels, test_size=0.3, random_state=42, stratify=encoded_labels)\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "train_dataset = TextDataset(X_train.tolist(), y_train.tolist(), tokenizer)\n",
        "val_dataset = TextDataset(X_val.tolist(), y_val.tolist(), tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8)\n",
        "\n",
        "model = RobertaStyloModel(num_labels=len(np.unique(encoded_labels)), num_stylometric_features=34).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3  # Define your number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        stylometric_features = batch['stylometric_features'].to(device)\n",
        "        outputs = model(input_ids, attention_mask, stylometric_features)\n",
        "        loss = nn.CrossEntropyLoss()(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1} of {num_epochs} completed.\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "val_labels = []\n",
        "val_preds = []\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        stylometric_features = batch['stylometric_features'].to(device)\n",
        "        outputs = model(input_ids, attention_mask, stylometric_features)\n",
        "        _, preds = torch.max(outputs, dim=1)\n",
        "        val_labels.extend(labels.cpu().numpy())\n",
        "        val_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "text_labels = [label_encoder.inverse_transform([label])[0] for label in val_labels]\n",
        "predicted_text_labels = [label_encoder.inverse_transform([pred])[0] for pred in val_preds]\n",
        "\n",
        "val_accuracy = accuracy_score(val_labels, val_preds)\n",
        "print(f\"Validation Accuracy: {val_accuracy}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(text_labels, predicted_text_labels))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Pf5WrM_EfBX",
        "outputId": "0c0d2f69-11c2-46b1-c918-87553729f1a2"
      },
      "id": "5Pf5WrM_EfBX",
      "execution_count": 4,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 of 3 completed.\n",
            "Epoch 2 of 3 completed.\n",
            "Epoch 3 of 3 completed.\n",
            "Validation Accuracy: 0.6627450980392157\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.90      0.90        30\n",
            "           1       0.96      0.83      0.89        30\n",
            "           2       0.43      0.77      0.55        30\n",
            "           3       0.27      0.40      0.32        30\n",
            "           4       0.56      0.63      0.59        30\n",
            "           5       0.71      0.90      0.79        30\n",
            "           6       0.91      0.70      0.79        30\n",
            "           7       0.56      0.90      0.69        30\n",
            "           8       0.80      0.67      0.73        30\n",
            "           9       0.40      0.07      0.11        30\n",
            "          10       0.97      1.00      0.98        30\n",
            "          11       0.72      0.87      0.79        30\n",
            "          12       0.77      0.80      0.79        30\n",
            "          13       0.29      0.40      0.34        30\n",
            "          14       0.53      1.00      0.69        30\n",
            "          15       1.00      0.87      0.93        30\n",
            "          16       0.00      0.00      0.00        30\n",
            "          17       1.00      0.17      0.29        30\n",
            "          18       0.88      0.73      0.80        30\n",
            "          19       0.81      0.83      0.82        30\n",
            "          20       0.91      1.00      0.95        30\n",
            "          21       0.91      0.70      0.79        30\n",
            "          22       0.65      0.73      0.69        30\n",
            "          23       0.56      0.33      0.42        30\n",
            "          24       0.81      0.43      0.57        30\n",
            "          25       0.75      0.70      0.72        30\n",
            "          26       0.93      0.87      0.90        30\n",
            "          27       0.78      0.83      0.81        30\n",
            "          28       1.00      1.00      1.00        30\n",
            "          29       0.58      0.87      0.69        30\n",
            "          30       0.61      0.73      0.67        30\n",
            "          31       0.89      0.80      0.84        30\n",
            "          32       0.86      1.00      0.92        30\n",
            "          33       0.77      0.80      0.79        30\n",
            "          34       0.18      0.67      0.28        30\n",
            "          35       0.60      0.80      0.69        30\n",
            "          36       0.81      0.73      0.77        30\n",
            "          37       0.00      0.00      0.00        30\n",
            "          38       0.71      0.50      0.59        30\n",
            "          39       0.90      0.60      0.72        30\n",
            "          40       0.90      0.93      0.92        30\n",
            "          41       0.41      0.80      0.55        30\n",
            "          42       0.74      0.57      0.64        30\n",
            "          43       0.00      0.00      0.00        30\n",
            "          44       0.87      0.67      0.75        30\n",
            "          45       0.32      0.33      0.33        30\n",
            "          46       0.71      0.73      0.72        30\n",
            "          47       0.64      0.83      0.72        30\n",
            "          48       0.64      0.47      0.54        30\n",
            "          49       0.00      0.00      0.00        30\n",
            "          50       1.00      0.93      0.97        30\n",
            "\n",
            "    accuracy                           0.66      1530\n",
            "   macro avg       0.67      0.66      0.64      1530\n",
            "weighted avg       0.67      0.66      0.64      1530\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#31.distilbert with log reg\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import pandas as pd\n",
        "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AdamW\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import joblib\n",
        "# Setup and downloads\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Check for CUDA\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize tokenizer\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "# Custom Dataset class for DistilBERT\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Data loading\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(data['text'], data['label'], test_size=0.3, random_state=42, stratify=data['label'])\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42, stratify=y_temp)\n",
        "\n",
        "# DataLoaders for DistilBERT\n",
        "train_dataset = TextDataset(X_train.tolist(), y_train.tolist(), tokenizer)\n",
        "val_dataset = TextDataset(X_val.tolist(), y_val.tolist(), tokenizer)\n",
        "test_dataset = TextDataset(X_test.tolist(), y_test.tolist(), tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# DistilBERT model setup\n",
        "num_labels = data['label'].nunique()\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)\n",
        "model.to(device)\n",
        "\n",
        "# Training setup\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "num_epochs = 3\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} completed.\")\n",
        "\n",
        "# Prepare DistilBERT outputs for logistic regression\n",
        "model.eval()  # Set model to evaluation mode\n",
        "distilbert_features = []  # Features based on DistilBERT predictions\n",
        "test_labels = []  # Actual labels\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        distilbert_features.extend(outputs.logits.cpu().numpy())  # Use logits as features\n",
        "        test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Logistic Regression based on DistilBERT outputs\n",
        "distilbert_features = np.array(distilbert_features)\n",
        "logistic_model = LogisticRegression(max_iter=1000)\n",
        "logistic_model.fit(distilbert_features, test_labels)\n",
        "final_predictions = logistic_model.predict(distilbert_features)\n",
        "\n",
        "# Evaluation\n",
        "print(classification_report(test_labels, final_predictions))\n",
        "# Save the Logistic Regression model\n",
        "logistic_model_save_path = 'logistic_regression_fordistilbert.joblib'\n",
        "joblib.dump(logistic_model, logistic_model_save_path)\n"
      ],
      "metadata": {
        "id": "FDxXdHJiICzs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ba66fc5-ce7a-47bb-b871-f35ce56f0490"
      },
      "id": "FDxXdHJiICzs",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3 completed.\n",
            "Epoch 2/3 completed.\n",
            "Epoch 3/3 completed.\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      1.00      0.98        21\n",
            "           1       1.00      0.90      0.95        21\n",
            "           2       0.59      0.48      0.53        21\n",
            "           3       0.56      0.48      0.51        21\n",
            "           4       0.79      0.71      0.75        21\n",
            "           5       0.70      0.76      0.73        21\n",
            "           6       0.91      0.95      0.93        21\n",
            "           7       0.77      0.81      0.79        21\n",
            "           8       0.88      1.00      0.93        21\n",
            "           9       0.30      0.14      0.19        21\n",
            "          10       1.00      1.00      1.00        21\n",
            "          11       0.80      0.95      0.87        21\n",
            "          12       1.00      0.95      0.98        21\n",
            "          13       0.50      0.43      0.46        21\n",
            "          14       0.67      0.86      0.75        21\n",
            "          15       1.00      0.95      0.98        21\n",
            "          16       0.62      0.71      0.67        21\n",
            "          17       0.82      0.67      0.74        21\n",
            "          18       1.00      0.95      0.98        21\n",
            "          19       1.00      0.90      0.95        21\n",
            "          20       1.00      1.00      1.00        21\n",
            "          21       0.83      0.95      0.89        21\n",
            "          22       0.77      0.81      0.79        21\n",
            "          23       0.71      0.57      0.63        21\n",
            "          24       0.77      0.81      0.79        21\n",
            "          25       0.76      0.76      0.76        21\n",
            "          26       0.95      0.90      0.93        21\n",
            "          27       0.89      0.81      0.85        21\n",
            "          28       1.00      1.00      1.00        21\n",
            "          29       0.76      0.76      0.76        21\n",
            "          30       0.65      0.81      0.72        21\n",
            "          31       0.78      0.86      0.82        21\n",
            "          32       1.00      1.00      1.00        21\n",
            "          33       0.95      0.90      0.93        21\n",
            "          34       0.53      0.76      0.63        21\n",
            "          35       0.78      0.86      0.82        21\n",
            "          36       0.86      0.90      0.88        21\n",
            "          37       0.48      0.76      0.59        21\n",
            "          38       0.80      0.76      0.78        21\n",
            "          39       0.84      0.76      0.80        21\n",
            "          40       1.00      0.95      0.98        21\n",
            "          41       0.63      0.81      0.71        21\n",
            "          42       0.76      0.76      0.76        21\n",
            "          43       0.40      0.29      0.33        21\n",
            "          44       0.75      0.71      0.73        21\n",
            "          45       0.64      0.33      0.44        21\n",
            "          46       0.82      0.86      0.84        21\n",
            "          47       0.91      0.95      0.93        21\n",
            "          48       0.84      0.76      0.80        21\n",
            "          49       0.52      0.52      0.52        21\n",
            "          50       1.00      1.00      1.00        21\n",
            "\n",
            "    accuracy                           0.79      1071\n",
            "   macro avg       0.79      0.79      0.79      1071\n",
            "weighted avg       0.79      0.79      0.79      1071\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['logistic_regression_model.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#32.roberta (processed stylo) fusion\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from nltk import word_tokenize, pos_tag, download\n",
        "from collections import Counter\n",
        "\n",
        "download('punkt')\n",
        "download('averaged_perceptron_tagger')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Stylometric Feature Extraction Function\n",
        "def extract_stylometric_features(text, feature_length=34):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "    num_words = len(tokens)\n",
        "    num_unique_words = len(set(tokens))\n",
        "    avg_word_length = np.mean([len(word) for word in tokens]) if tokens else 0\n",
        "    lexical_diversity = num_unique_words / num_words if num_words else 0\n",
        "    pos_counts = Counter(tag for word, tag in tagged_tokens)\n",
        "\n",
        "    pos_features = [pos_counts[tag] / num_words if num_words else 0 for tag in sorted(pos_counts)]\n",
        "\n",
        "    features = [num_words, avg_word_length, lexical_diversity] + pos_features\n",
        "    # Ensure the features array is exactly 'feature_length' long\n",
        "    features = np.array(features)\n",
        "    if len(features) > feature_length:\n",
        "        features = features[:feature_length]  # Truncate if too long\n",
        "    elif len(features) < feature_length:\n",
        "        # Extend features array with zeros (or another placeholder value) if too short\n",
        "        features = np.pad(features, (0, feature_length - len(features)), 'constant')\n",
        "\n",
        "    return features\n",
        "\n",
        "class TextStyloDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=512, feature_length=34):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        self.feature_length = feature_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text, add_special_tokens=True, max_length=self.max_len,\n",
        "            return_token_type_ids=False, padding='max_length',\n",
        "            return_attention_mask=True, return_tensors='pt', truncation=True\n",
        "        )\n",
        "        stylometric_features = extract_stylometric_features(text, self.feature_length)\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long),\n",
        "            'stylometric_features': torch.tensor(stylometric_features, dtype=torch.float)\n",
        "        }\n",
        "\n",
        "class RobertaStyloModel(nn.Module):\n",
        "    def __init__(self, num_labels, feature_length=34):\n",
        "        super(RobertaStyloModel, self).__init__()\n",
        "        self.roberta = RobertaModel.from_pretrained('roberta-base')\n",
        "        self.feature_processor = nn.Linear(feature_length, 128)\n",
        "        self.classifier = nn.Linear(self.roberta.config.hidden_size + 128, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, stylometric_features):\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        stylo_features = torch.relu(self.feature_processor(stylometric_features))\n",
        "        combined_features = torch.cat((pooled_output, stylo_features), dim=1)\n",
        "        logits = self.classifier(combined_features)\n",
        "        return logits\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data['text'], data['label'], test_size=0.3, random_state=42, stratify=data['label'])\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "train_dataset = TextStyloDataset(X_train.tolist(), y_train.tolist(), tokenizer)\n",
        "test_dataset = TextStyloDataset(X_test.tolist(), y_test.tolist(), tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "model = RobertaStyloModel(num_labels=data['label'].nunique()).to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    for batch in train_loader:\n",
        "        input_ids, attention_mask, labels, stylometric_features = (\n",
        "            batch['input_ids'].to(device),\n",
        "            batch['attention_mask'].to(device),\n",
        "            batch['labels'].to(device),\n",
        "            batch['stylometric_features'].to(device)\n",
        "        )\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask, stylometric_features)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    # Validation loop\n",
        "    model.eval()\n",
        "    total, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids, attention_mask, labels, stylometric_features = (\n",
        "                batch['input_ids'].to(device),\n",
        "                batch['attention_mask'].to(device),\n",
        "                batch['labels'].to(device),\n",
        "                batch['stylometric_features'].to(device)\n",
        "            )\n",
        "            outputs = model(input_ids, attention_mask, stylometric_features)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}: Accuracy on test set = {100 * correct / total}%')\n"
      ],
      "metadata": {
        "id": "5e-hgkj3AT0j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c3b9f68-fc8e-45cd-d2ea-462e033ca5a8"
      },
      "id": "5e-hgkj3AT0j",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Accuracy on test set = 35.947712418300654%\n",
            "Epoch 2: Accuracy on test set = 63.00653594771242%\n",
            "Epoch 3: Accuracy on test set = 68.16993464052288%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hybrid Fusion"
      ],
      "metadata": {
        "id": "waXXLkJ8HiuT"
      },
      "id": "waXXLkJ8HiuT"
    },
    {
      "cell_type": "code",
      "source": [
        "#33.roberta stylo attention svm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Ensure CUDA is available for PyTorch, replace 'cuda' with 'cpu' if not using GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Function to extract RoBERTa features\n",
        "def get_roberta_features(texts, model, tokenizer, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    features = []\n",
        "    with torch.no_grad():  # No need to track gradients\n",
        "        for text in texts:\n",
        "            encoded_input = tokenizer(text, return_tensors='pt', truncation=True, max_length=512, padding=True).to(device)\n",
        "            output = model(**encoded_input)\n",
        "            features.append(output.pooler_output[0].cpu().numpy())  # Using the pooled output\n",
        "    return np.array(features)\n",
        "\n",
        "\n",
        "# Define an attention layer\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.bias = bias\n",
        "        self.feature_dim = feature_dim\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "\n",
        "        weight = torch.zeros(feature_dim, 1)\n",
        "        nn.init.xavier_uniform_(weight)\n",
        "        self.weight = nn.Parameter(weight)\n",
        "\n",
        "        if bias:\n",
        "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        eij = torch.mm(\n",
        "            x.contiguous().view(-1, self.feature_dim),\n",
        "            self.weight\n",
        "        ).view(-1, self.step_dim)\n",
        "\n",
        "        if self.bias:\n",
        "            eij = eij + self.b\n",
        "\n",
        "        eij = torch.tanh(eij)\n",
        "        a = torch.exp(eij)\n",
        "\n",
        "        if mask is not None:\n",
        "            a = a * mask\n",
        "\n",
        "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
        "\n",
        "        weighted_input = x * torch.unsqueeze(a, -1)\n",
        "        return torch.sum(weighted_input, 1)\n",
        "\n",
        "class RobertaStyloModel(nn.Module):\n",
        "    def __init__(self, roberta_model, num_features, num_labels):\n",
        "        super(RobertaStyloModel, self).__init__()\n",
        "        self.roberta = roberta_model\n",
        "        self.attention = Attention(feature_dim=768, step_dim=num_features)\n",
        "        self.classifier = nn.Linear(768 + num_features, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, stylometric_features):\n",
        "        # RoBERTa model\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "        pooled_output = last_hidden_state[:, 0]  # Use the representation of [CLS] token\n",
        "\n",
        "        # Concatenate RoBERTa embeddings with stylometric features before applying attention\n",
        "        combined_features_before_attention = torch.cat((pooled_output.unsqueeze(1), stylometric_features.unsqueeze(1)), 2)\n",
        "\n",
        "        # Apply attention over combined features\n",
        "        combined_features_with_attention = self.attention(combined_features_before_attention).squeeze(1)\n",
        "\n",
        "        # Classifier\n",
        "        logits = self.classifier(combined_features_with_attention)\n",
        "        return logits\n",
        "\n",
        "# Stylometric Feature Extraction Function\n",
        "def extract_stylometric_features(text, feature_length=34):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "    num_words = len(tokens)\n",
        "    num_unique_words = len(set(tokens))\n",
        "    avg_word_length = np.mean([len(word) for word in tokens]) if tokens else 0\n",
        "    lexical_diversity = num_unique_words / num_words if num_words else 0\n",
        "    pos_counts = Counter(tag for word, tag in tagged_tokens)\n",
        "\n",
        "    pos_features = [pos_counts[tag] / num_words if num_words else 0 for tag in sorted(pos_counts)]\n",
        "\n",
        "    features = [num_words, avg_word_length, lexical_diversity] + pos_features\n",
        "    # Ensure the features array is exactly 'feature_length' long\n",
        "    features = np.array(features)  # Convert features list to a NumPy array if it's not already\n",
        "    if len(features) > feature_length:\n",
        "        features = features[:feature_length]  # Truncate if too long\n",
        "    elif len(features) < feature_length:\n",
        "        # Extend features array with zeros (or another placeholder value) if too short\n",
        "        features = np.pad(features, (0, feature_length - len(features)), 'constant')\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "texts = data['text'].tolist()\n",
        "labels = data['label'].tolist()\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "roberta_model = RobertaModel.from_pretrained('roberta-base').to(device)\n",
        "\n",
        "roberta_features = get_roberta_features(texts, roberta_model, tokenizer, device)\n",
        "stylo_features = np.array([extract_stylometric_features(text) for text in texts])\n",
        "\n",
        "# Combine features and split dataset\n",
        "X = np.hstack((roberta_features, stylo_features))\n",
        "y = np.array(labels)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify=y)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the SVM\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = svm_classifier.predict(X_test_scaled)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "jZ258tvZss5M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d89b0f0c-8c75-401c-b182-daecfbcba826"
      },
      "id": "jZ258tvZss5M",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.97      0.94        30\n",
            "           1       0.77      1.00      0.87        30\n",
            "           2       0.58      0.70      0.64        30\n",
            "           3       0.50      0.67      0.57        30\n",
            "           4       0.70      0.77      0.73        30\n",
            "           5       0.69      0.73      0.71        30\n",
            "           6       0.81      0.87      0.84        30\n",
            "           7       0.82      0.77      0.79        30\n",
            "           8       0.76      0.93      0.84        30\n",
            "           9       0.60      0.70      0.65        30\n",
            "          10       1.00      1.00      1.00        30\n",
            "          11       0.87      0.90      0.89        30\n",
            "          12       1.00      0.87      0.93        30\n",
            "          13       0.48      0.50      0.49        30\n",
            "          14       0.83      0.80      0.81        30\n",
            "          15       1.00      0.97      0.98        30\n",
            "          16       0.77      0.67      0.71        30\n",
            "          17       0.70      0.47      0.56        30\n",
            "          18       0.90      0.90      0.90        30\n",
            "          19       0.96      0.77      0.85        30\n",
            "          20       1.00      0.97      0.98        30\n",
            "          21       0.90      0.87      0.88        30\n",
            "          22       0.77      0.80      0.79        30\n",
            "          23       0.70      0.63      0.67        30\n",
            "          24       0.80      0.80      0.80        30\n",
            "          25       0.87      0.90      0.89        30\n",
            "          26       0.90      0.87      0.88        30\n",
            "          27       0.92      0.77      0.84        30\n",
            "          28       0.97      1.00      0.98        30\n",
            "          29       0.78      0.83      0.81        30\n",
            "          30       0.82      0.93      0.87        30\n",
            "          31       0.82      0.77      0.79        30\n",
            "          32       0.94      1.00      0.97        30\n",
            "          33       0.78      0.83      0.81        30\n",
            "          34       0.39      0.47      0.42        30\n",
            "          35       0.86      0.80      0.83        30\n",
            "          36       0.96      0.90      0.93        30\n",
            "          37       0.58      0.70      0.64        30\n",
            "          38       0.89      0.80      0.84        30\n",
            "          39       0.79      0.77      0.78        30\n",
            "          40       0.97      0.93      0.95        30\n",
            "          41       0.67      0.67      0.67        30\n",
            "          42       0.83      0.67      0.74        30\n",
            "          43       0.44      0.37      0.40        30\n",
            "          44       0.81      0.73      0.77        30\n",
            "          45       0.61      0.57      0.59        30\n",
            "          46       0.75      0.70      0.72        30\n",
            "          47       0.93      0.90      0.92        30\n",
            "          48       0.81      0.87      0.84        30\n",
            "          49       0.50      0.40      0.44        30\n",
            "          50       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           0.79      1530\n",
            "   macro avg       0.79      0.79      0.79      1530\n",
            "weighted avg       0.79      0.79      0.79      1530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#34.roberta (stylo attention) svm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Ensure CUDA is available for PyTorch, replace 'cuda' with 'cpu' if not using GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define an attention layer\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.bias = bias\n",
        "        self.feature_dim = feature_dim\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "\n",
        "        weight = torch.zeros(feature_dim, 1)\n",
        "        nn.init.xavier_uniform_(weight)\n",
        "        self.weight = nn.Parameter(weight)\n",
        "\n",
        "        if bias:\n",
        "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        eij = torch.mm(\n",
        "            x.contiguous().view(-1, self.feature_dim),\n",
        "            self.weight\n",
        "        ).view(-1, self.step_dim)\n",
        "\n",
        "        if self.bias:\n",
        "            eij = eij + self.b\n",
        "\n",
        "        eij = torch.tanh(eij)\n",
        "        a = torch.exp(eij)\n",
        "\n",
        "        if mask is not None:\n",
        "            a = a * mask\n",
        "\n",
        "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
        "\n",
        "        weighted_input = x * torch.unsqueeze(a, -1)\n",
        "        return torch.sum(weighted_input, 1)\n",
        "\n",
        "class RobertaStyloModel(nn.Module):\n",
        "    def __init__(self, roberta_model, num_features, num_labels):\n",
        "        super(RobertaStyloModel, self).__init__()\n",
        "        self.roberta = roberta_model\n",
        "        self.attention = Attention(feature_dim=768, step_dim=num_features)\n",
        "        self.classifier = nn.Linear(768 + num_features, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, stylometric_features):\n",
        "        # RoBERTa model\n",
        "        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "        pooled_output = last_hidden_state[:, 0]  # Use the representation of [CLS] token\n",
        "\n",
        "        # Apply attention over stylometric features\n",
        "        stylo_with_attention = self.attention(stylometric_features)\n",
        "\n",
        "        # Concatenate RoBERTa embeddings with stylometric features\n",
        "        combined_features = torch.cat((pooled_output, stylo_with_attention), 1)\n",
        "\n",
        "        # Classifier\n",
        "        logits = self.classifier(combined_features)\n",
        "        return logits\n",
        "\n",
        "# Stylometric Feature Extraction Function\n",
        "def extract_stylometric_features(text, feature_length=34):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "    num_words = len(tokens)\n",
        "    num_unique_words = len(set(tokens))\n",
        "    avg_word_length = np.mean([len(word) for word in tokens]) if tokens else 0\n",
        "    lexical_diversity = num_unique_words / num_words if num_words else 0\n",
        "    pos_counts = Counter(tag for word, tag in tagged_tokens)\n",
        "\n",
        "    pos_features = [pos_counts[tag] / num_words if num_words else 0 for tag in sorted(pos_counts)]\n",
        "\n",
        "    features = [num_words, avg_word_length, lexical_diversity] + pos_features\n",
        "    # Ensure the features array is exactly 'feature_length' long\n",
        "    features = np.array(features)  # Convert features list to a NumPy array if it's not already\n",
        "    if len(features) > feature_length:\n",
        "        features = features[:feature_length]  # Truncate if too long\n",
        "    elif len(features) < feature_length:\n",
        "        # Extend features array with zeros (or another placeholder value) if too short\n",
        "        features = np.pad(features, (0, feature_length - len(features)), 'constant')\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "texts = data['text'].tolist()\n",
        "labels = data['label'].tolist()\n",
        "\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "roberta_model = RobertaModel.from_pretrained('roberta-base').to(device)\n",
        "\n",
        "roberta_features = get_roberta_features(texts, roberta_model, tokenizer, device)\n",
        "stylo_features = np.array([extract_stylometric_features(text) for text in texts])\n",
        "\n",
        "# Combine features and split dataset\n",
        "X = np.hstack((roberta_features, stylo_features))\n",
        "y = np.array(labels)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the SVM\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = svm_classifier.predict(X_test_scaled)\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n"
      ],
      "metadata": {
        "id": "mZrKVYIZ7hmt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28a4480f-bfbb-4464-adca-c20b4f4b52a6"
      },
      "id": "mZrKVYIZ7hmt",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.90      0.90        30\n",
            "           1       0.81      1.00      0.90        30\n",
            "           2       0.58      0.63      0.60        30\n",
            "           3       0.48      0.53      0.51        30\n",
            "           4       0.67      0.67      0.67        30\n",
            "           5       0.73      0.73      0.73        30\n",
            "           6       0.90      0.93      0.92        30\n",
            "           7       0.80      0.80      0.80        30\n",
            "           8       0.76      0.93      0.84        30\n",
            "           9       0.60      0.70      0.65        30\n",
            "          10       1.00      1.00      1.00        30\n",
            "          11       0.78      0.83      0.81        30\n",
            "          12       0.91      0.97      0.94        30\n",
            "          13       0.45      0.50      0.48        30\n",
            "          14       0.81      0.73      0.77        30\n",
            "          15       0.93      0.93      0.93        30\n",
            "          16       0.88      0.70      0.78        30\n",
            "          17       0.67      0.53      0.59        30\n",
            "          18       0.93      0.93      0.93        30\n",
            "          19       0.90      0.87      0.88        30\n",
            "          20       1.00      0.97      0.98        30\n",
            "          21       0.96      0.83      0.89        30\n",
            "          22       0.73      0.80      0.76        30\n",
            "          23       0.77      0.67      0.71        30\n",
            "          24       0.80      0.80      0.80        30\n",
            "          25       0.87      0.87      0.87        30\n",
            "          26       0.96      0.90      0.93        30\n",
            "          27       0.96      0.80      0.87        30\n",
            "          28       0.97      1.00      0.98        30\n",
            "          29       0.80      0.93      0.86        30\n",
            "          30       0.76      0.93      0.84        30\n",
            "          31       0.78      0.70      0.74        30\n",
            "          32       0.94      1.00      0.97        30\n",
            "          33       0.79      0.87      0.83        30\n",
            "          34       0.42      0.50      0.45        30\n",
            "          35       0.90      0.87      0.88        30\n",
            "          36       0.87      0.90      0.89        30\n",
            "          37       0.57      0.77      0.66        30\n",
            "          38       0.96      0.77      0.85        30\n",
            "          39       0.85      0.73      0.79        30\n",
            "          40       1.00      0.93      0.97        30\n",
            "          41       0.69      0.73      0.71        30\n",
            "          42       0.73      0.53      0.62        30\n",
            "          43       0.40      0.33      0.36        30\n",
            "          44       0.75      0.80      0.77        30\n",
            "          45       0.57      0.53      0.55        30\n",
            "          46       0.73      0.73      0.73        30\n",
            "          47       0.96      0.87      0.91        30\n",
            "          48       0.82      0.77      0.79        30\n",
            "          49       0.61      0.47      0.53        30\n",
            "          50       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           0.79      1530\n",
            "   macro avg       0.79      0.79      0.79      1530\n",
            "weighted avg       0.79      0.79      0.79      1530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#35.distilbert stylo attention svm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Ensure CUDA is available for PyTorch, replace 'cuda' with 'cpu' if not using GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Function to extract DistilBERT features\n",
        "def get_distilbert_features(texts, model, tokenizer, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    features = []\n",
        "    with torch.no_grad():  # No need to track gradients\n",
        "        for text in texts:\n",
        "            encoded_input = tokenizer(text, return_tensors='pt', truncation=True, max_length=512, padding=True).to(device)\n",
        "            output = model(**encoded_input)\n",
        "            # Change here: Ensure the output is squeezed and converted to 2D array\n",
        "            feature = output.last_hidden_state.mean(dim=1).squeeze().detach().cpu().numpy()\n",
        "            features.append(feature)\n",
        "    # This should now be a 2D array\n",
        "    return np.array(features)\n",
        "\n",
        "\n",
        "\n",
        "# Define an attention layer\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.bias = bias\n",
        "        self.feature_dim = feature_dim\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "\n",
        "        weight = torch.zeros(feature_dim, 1)\n",
        "        nn.init.xavier_uniform_(weight)\n",
        "        self.weight = nn.Parameter(weight)\n",
        "\n",
        "        if bias:\n",
        "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        eij = torch.mm(\n",
        "            x.contiguous().view(-1, self.feature_dim),\n",
        "            self.weight\n",
        "        ).view(-1, self.step_dim)\n",
        "\n",
        "        if self.bias:\n",
        "            eij = eij + self.b\n",
        "\n",
        "        eij = torch.tanh(eij)\n",
        "        a = torch.exp(eij)\n",
        "\n",
        "        if mask is not None:\n",
        "            a = a * mask\n",
        "\n",
        "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
        "\n",
        "        weighted_input = x * torch.unsqueeze(a, -1)\n",
        "        return torch.sum(weighted_input, 1)\n",
        "\n",
        "class DistilBertStyloModel(nn.Module):\n",
        "    def __init__(self, distilbert_model, num_features, num_labels):\n",
        "        super(DistilBertStyloModel, self).__init__()\n",
        "        self.distilbert = distilbert_model\n",
        "        self.attention = Attention(feature_dim=768, step_dim=num_features)\n",
        "        self.classifier = nn.Linear(768 + num_features, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, stylometric_features):\n",
        "        # DistilBERT model\n",
        "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "        pooled_output = last_hidden_state[:, 0]  # Use the representation of [CLS] token\n",
        "\n",
        "        # Concatenate DistilBERT embeddings with stylometric features before applying attention\n",
        "        combined_features_before_attention = torch.cat((pooled_output.unsqueeze(1), stylometric_features.unsqueeze(1)), 2)\n",
        "\n",
        "        # Apply attention over combined features\n",
        "        combined_features_with_attention = self.attention(combined_features_before_attention).squeeze(1)\n",
        "\n",
        "        # Classifier\n",
        "        logits = self.classifier(combined_features_with_attention)\n",
        "        return logits\n",
        "\n",
        "# Stylometric Feature Extraction Function\n",
        "def extract_stylometric_features(text, feature_length=34):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "\n",
        "    num_words = len(tokens)\n",
        "    num_unique_words = len(set(tokens))\n",
        "    avg_word_length = np.mean([len(word) for word in tokens]) if tokens else 0\n",
        "    lexical_diversity = num_unique_words / num_words if num_words else 0\n",
        "    pos_counts = Counter(tag for word, tag in tagged_tokens)\n",
        "\n",
        "    pos_features = [pos_counts[tag] / num_words if num_words else 0 for tag in sorted(pos_counts)]\n",
        "\n",
        "    features = [num_words, avg_word_length, lexical_diversity] + pos_features\n",
        "    # Ensure the features array is exactly 'feature_length' long\n",
        "    features = np.array(features)  # Convert features list to a NumPy array if it's not already\n",
        "    if len(features) > feature_length:\n",
        "        features = features[:feature_length]  # Truncate if too long\n",
        "    elif len(features) < feature_length:\n",
        "        # Extend features array with zeros (or another placeholder value) if too short\n",
        "        features = np.pad(features, (0, feature_length - len(features)), 'constant')\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "texts = data['text'].tolist()\n",
        "labels = data['label'].tolist()\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "\n",
        "distilbert_features = get_distilbert_features(texts, distilbert_model, tokenizer, device)\n",
        "stylo_features = np.array([extract_stylometric_features(text) for text in texts])\n",
        "\n",
        "# Combine features and split dataset\n",
        "X = np.hstack((distilbert_features, stylo_features))\n",
        "y = np.array(labels)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify=y)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the SVM\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = svm_classifier.predict(X_test_scaled)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "XAroYCp7eY7Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8137c8d-37fb-485c-f9e8-a3c477cefd2f"
      },
      "id": "XAroYCp7eY7Y",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.93      0.95        30\n",
            "           1       0.78      0.97      0.87        30\n",
            "           2       0.69      0.67      0.68        30\n",
            "           3       0.65      0.67      0.66        30\n",
            "           4       0.68      0.63      0.66        30\n",
            "           5       0.72      0.87      0.79        30\n",
            "           6       0.90      0.93      0.92        30\n",
            "           7       0.81      0.87      0.84        30\n",
            "           8       0.61      0.77      0.68        30\n",
            "           9       0.68      0.57      0.62        30\n",
            "          10       1.00      1.00      1.00        30\n",
            "          11       0.79      0.87      0.83        30\n",
            "          12       0.97      0.97      0.97        30\n",
            "          13       0.44      0.53      0.48        30\n",
            "          14       0.83      0.67      0.74        30\n",
            "          15       0.97      1.00      0.98        30\n",
            "          16       0.77      0.67      0.71        30\n",
            "          17       0.75      0.70      0.72        30\n",
            "          18       0.70      0.77      0.73        30\n",
            "          19       0.88      0.73      0.80        30\n",
            "          20       1.00      0.97      0.98        30\n",
            "          21       0.96      0.90      0.93        30\n",
            "          22       0.81      0.87      0.84        30\n",
            "          23       0.75      0.70      0.72        30\n",
            "          24       0.81      0.70      0.75        30\n",
            "          25       0.80      0.93      0.86        30\n",
            "          26       0.93      0.87      0.90        30\n",
            "          27       0.80      0.80      0.80        30\n",
            "          28       1.00      1.00      1.00        30\n",
            "          29       0.82      0.93      0.87        30\n",
            "          30       0.82      0.93      0.87        30\n",
            "          31       0.72      0.77      0.74        30\n",
            "          32       0.97      1.00      0.98        30\n",
            "          33       0.81      0.83      0.82        30\n",
            "          34       0.45      0.43      0.44        30\n",
            "          35       0.78      0.93      0.85        30\n",
            "          36       0.89      0.83      0.86        30\n",
            "          37       0.62      0.77      0.69        30\n",
            "          38       0.93      0.83      0.88        30\n",
            "          39       0.96      0.73      0.83        30\n",
            "          40       1.00      0.97      0.98        30\n",
            "          41       0.69      0.73      0.71        30\n",
            "          42       0.87      0.67      0.75        30\n",
            "          43       0.43      0.40      0.41        30\n",
            "          44       0.84      0.90      0.87        30\n",
            "          45       0.63      0.57      0.60        30\n",
            "          46       0.81      0.73      0.77        30\n",
            "          47       0.86      0.83      0.85        30\n",
            "          48       0.88      0.77      0.82        30\n",
            "          49       0.48      0.43      0.46        30\n",
            "          50       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           0.79      1530\n",
            "   macro avg       0.80      0.79      0.79      1530\n",
            "weighted avg       0.80      0.79      0.79      1530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example Sentence Processing:\n",
        "\n",
        "\n",
        "**RoBERTa-Stylo Fusion**: The sentence \"I love sunny days but hate the rain.\" would be tokenized and passed through the RoBERTa model to derive contextual embeddings. Concurrently, stylometric features (like average word length, part-of-speech frequencies, etc.) would be extracted from the raw text. Both sets of features would be combined and then input into a classifier within the same neural network model for final prediction.\n",
        "\n",
        "**DistilBert Embedding + Stylometry with Soft Voting**: For the same sentence, separate embeddings would be extracted via DistilBert, and stylometric features would be calculated separately. These would then serve as inputs to different traditional classifiers (e.g., XGBoost, RandomForest, LogisticRegression). Each classifier would provide a prediction, and these predictions would be merged through soft voting, considering the probability estimates from each classifier to determine the final output.\n",
        "\n",
        "**DistilBert-Stylo Fusion with Attention**: In this method, the sentence would also be tokenized and processed through DistilBert, and stylometric features would be extracted. These would then be combined into one feature vector. However, before classification, this combined vector would pass through an attention mechanism, allowing the model to weigh different features according to their relevance. The weighted features would then be used for classification."
      ],
      "metadata": {
        "id": "a3ngLhfelgA9"
      },
      "id": "a3ngLhfelgA9"
    },
    {
      "cell_type": "code",
      "source": [
        "#36.distilbert stylo2 attention svm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, sent_tokenize\n",
        "from collections import Counter\n",
        "import textstat\n",
        "\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Ensure CUDA is available for PyTorch, replace 'cuda' with 'cpu' if not using GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "# Function to extract DistilBERT features\n",
        "def get_distilbert_features(texts, model, tokenizer, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    features = []\n",
        "    with torch.no_grad():  # No need to track gradients\n",
        "        for text in texts:\n",
        "            encoded_input = tokenizer(text, return_tensors='pt', truncation=True, max_length=512, padding=True).to(device)\n",
        "            output = model(**encoded_input)\n",
        "            # Change here: Ensure the output is squeezed and converted to 2D array\n",
        "            feature = output.last_hidden_state.mean(dim=1).squeeze().detach().cpu().numpy()\n",
        "            features.append(feature)\n",
        "    # This should now be a 2D array\n",
        "    return np.array(features)\n",
        "\n",
        "\n",
        "\n",
        "# Define an attention layer\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "        self.supports_masking = True\n",
        "        self.bias = bias\n",
        "        self.feature_dim = feature_dim\n",
        "        self.step_dim = step_dim\n",
        "        self.features_dim = 0\n",
        "\n",
        "        weight = torch.zeros(feature_dim, 1)\n",
        "        nn.init.xavier_uniform_(weight)\n",
        "        self.weight = nn.Parameter(weight)\n",
        "\n",
        "        if bias:\n",
        "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        eij = torch.mm(\n",
        "            x.contiguous().view(-1, self.feature_dim),\n",
        "            self.weight\n",
        "        ).view(-1, self.step_dim)\n",
        "\n",
        "        if self.bias:\n",
        "            eij = eij + self.b\n",
        "\n",
        "        eij = torch.tanh(eij)\n",
        "        a = torch.exp(eij)\n",
        "\n",
        "        if mask is not None:\n",
        "            a = a * mask\n",
        "\n",
        "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
        "\n",
        "        weighted_input = x * torch.unsqueeze(a, -1)\n",
        "        return torch.sum(weighted_input, 1)\n",
        "\n",
        "class DistilBertStyloModel(nn.Module):\n",
        "    def __init__(self, distilbert_model, num_features, num_labels):\n",
        "        super(DistilBertStyloModel, self).__init__()\n",
        "        self.distilbert = distilbert_model\n",
        "        self.attention = Attention(feature_dim=768, step_dim=num_features)\n",
        "        self.classifier = nn.Linear(768 + num_features, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, stylometric_features):\n",
        "        # DistilBERT model\n",
        "        outputs = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "        pooled_output = last_hidden_state[:, 0]  # Use the representation of [CLS] token\n",
        "\n",
        "        # Concatenate DistilBERT embeddings with stylometric features before applying attention\n",
        "        combined_features_before_attention = torch.cat((pooled_output.unsqueeze(1), stylometric_features.unsqueeze(1)), 2)\n",
        "\n",
        "        # Apply attention over combined features\n",
        "        combined_features_with_attention = self.attention(combined_features_before_attention).squeeze(1)\n",
        "\n",
        "        # Classifier\n",
        "        logits = self.classifier(combined_features_with_attention)\n",
        "        return logits\n",
        "\n",
        "# Stylometric Feature Extraction Function\n",
        "# Function to extract stylometric features\n",
        "def extract_stylometric_features(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    sentences = sent_tokenize(text)\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(tokens)\n",
        "    num_unique_words = len(set(tokens))\n",
        "    num_characters = len(text)\n",
        "    lexical_diversity = num_unique_words / num_words if num_words else 0\n",
        "    avg_word_length = np.mean([len(word) for word in tokens]) if tokens else 0\n",
        "    avg_sentence_length = num_words / num_sentences if num_sentences else 0\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    pos_counts = Counter(tag for word, tag in tagged_tokens)\n",
        "    total_pos_counts = sum(pos_counts.values())\n",
        "    noun_ratio = pos_counts['NN'] / total_pos_counts if 'NN' in pos_counts else 0\n",
        "    adjective_ratio = pos_counts['JJ'] / total_pos_counts if 'JJ' in pos_counts else 0\n",
        "    verb_ratio = pos_counts['VB'] / total_pos_counts if 'VB' in pos_counts else 0\n",
        "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
        "    return np.array([lexical_diversity, avg_word_length, avg_sentence_length, noun_ratio, adjective_ratio, verb_ratio, flesch_reading_ease])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "texts = data['text'].tolist()\n",
        "labels = data['label'].tolist()\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "\n",
        "distilbert_features = get_distilbert_features(texts, distilbert_model, tokenizer, device)\n",
        "stylo_features = np.array([extract_stylometric_features(text) for text in texts])\n",
        "\n",
        "# Combine features and split dataset\n",
        "X = np.hstack((distilbert_features, stylo_features))\n",
        "y = np.array(labels)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify=y)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the SVM\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = svm_classifier.predict(X_test_scaled)\n",
        "print(classification_report(y_test, y_pred))\n",
        "# Save the trained models and scaler\n",
        "import joblib\n",
        "joblib.dump(svm_classifier, 'svm_distilbert_stylo2_attention_model.pkl')\n",
        "joblib.dump(scaler, 'scaler_distilbert_stylo2_attention.pkl')\n"
      ],
      "metadata": {
        "id": "CIr2mIXQgU0L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "506b588a-da67-42c5-a3b5-8ab7d5d75e20"
      },
      "id": "CIr2mIXQgU0L",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.93      0.95        30\n",
            "           1       0.90      0.90      0.90        30\n",
            "           2       0.59      0.67      0.62        30\n",
            "           3       0.66      0.63      0.64        30\n",
            "           4       0.70      0.63      0.67        30\n",
            "           5       0.74      0.87      0.80        30\n",
            "           6       0.93      0.93      0.93        30\n",
            "           7       0.89      0.83      0.86        30\n",
            "           8       0.66      0.77      0.71        30\n",
            "           9       0.67      0.53      0.59        30\n",
            "          10       1.00      1.00      1.00        30\n",
            "          11       0.74      0.83      0.78        30\n",
            "          12       1.00      0.93      0.97        30\n",
            "          13       0.41      0.57      0.48        30\n",
            "          14       0.83      0.80      0.81        30\n",
            "          15       0.97      1.00      0.98        30\n",
            "          16       0.79      0.63      0.70        30\n",
            "          17       0.77      0.77      0.77        30\n",
            "          18       0.70      0.77      0.73        30\n",
            "          19       0.86      0.80      0.83        30\n",
            "          20       1.00      0.97      0.98        30\n",
            "          21       0.93      0.93      0.93        30\n",
            "          22       0.83      0.83      0.83        30\n",
            "          23       0.70      0.70      0.70        30\n",
            "          24       0.83      0.63      0.72        30\n",
            "          25       0.79      0.90      0.84        30\n",
            "          26       0.93      0.87      0.90        30\n",
            "          27       0.83      0.80      0.81        30\n",
            "          28       1.00      1.00      1.00        30\n",
            "          29       0.79      0.90      0.84        30\n",
            "          30       0.85      0.97      0.91        30\n",
            "          31       0.76      0.73      0.75        30\n",
            "          32       0.97      1.00      0.98        30\n",
            "          33       0.79      0.87      0.83        30\n",
            "          34       0.56      0.50      0.53        30\n",
            "          35       0.80      0.93      0.86        30\n",
            "          36       0.92      0.77      0.84        30\n",
            "          37       0.69      0.73      0.71        30\n",
            "          38       0.86      0.80      0.83        30\n",
            "          39       0.85      0.77      0.81        30\n",
            "          40       1.00      0.97      0.98        30\n",
            "          41       0.65      0.73      0.69        30\n",
            "          42       0.81      0.73      0.77        30\n",
            "          43       0.50      0.43      0.46        30\n",
            "          44       0.87      0.87      0.87        30\n",
            "          45       0.67      0.60      0.63        30\n",
            "          46       0.75      0.80      0.77        30\n",
            "          47       0.86      0.83      0.85        30\n",
            "          48       0.87      0.87      0.87        30\n",
            "          49       0.55      0.53      0.54        30\n",
            "          50       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           0.80      1530\n",
            "   macro avg       0.80      0.80      0.80      1530\n",
            "weighted avg       0.80      0.80      0.80      1530\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scaler_distilbert_stylo2_attention.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#37.distilbert stylo svm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, sent_tokenize\n",
        "from collections import Counter\n",
        "import textstat\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Set CUDA device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Function to extract DistilBERT features\n",
        "def get_distilbert_features(texts, model, tokenizer, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    features = []\n",
        "    with torch.no_grad():  # No need to track gradients\n",
        "        for text in texts:\n",
        "            encoded_input = tokenizer(text, return_tensors='pt', truncation=True, max_length=512, padding=True).to(device)\n",
        "            output = model(**encoded_input)\n",
        "            feature = output.last_hidden_state.mean(dim=1).cpu().numpy()  # Using mean pooling\n",
        "            features.append(feature)\n",
        "    return np.vstack(features)  # Shape: (num_texts, feature_dim)\n",
        "\n",
        "\n",
        "# Function to extract stylometric features\n",
        "def extract_stylometric_features(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    sentences = sent_tokenize(text)\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(tokens)\n",
        "    num_unique_words = len(set(tokens))\n",
        "    num_characters = len(text)\n",
        "    lexical_diversity = num_unique_words / num_words if num_words else 0\n",
        "    avg_word_length = np.mean([len(word) for word in tokens]) if tokens else 0\n",
        "    avg_sentence_length = num_words / num_sentences if num_sentences else 0\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    pos_counts = Counter(tag for word, tag in tagged_tokens)\n",
        "    total_pos_counts = sum(pos_counts.values())\n",
        "    noun_ratio = pos_counts['NN'] / total_pos_counts if 'NN' in pos_counts else 0\n",
        "    adjective_ratio = pos_counts['JJ'] / total_pos_counts if 'JJ' in pos_counts else 0\n",
        "    verb_ratio = pos_counts['VB'] / total_pos_counts if 'VB' in pos_counts else 0\n",
        "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
        "    return np.array([lexical_diversity, avg_word_length, avg_sentence_length, noun_ratio, adjective_ratio, verb_ratio, flesch_reading_ease])\n",
        "\n",
        "\n",
        "# Load and prepare dataset\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "texts = data['text'].tolist()\n",
        "labels = LabelEncoder().fit_transform(data['label'])  # Encode labels\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "\n",
        "# Extract features\n",
        "distilbert_features = get_distilbert_features(texts, distilbert_model, tokenizer, device)\n",
        "stylo_features = np.array([extract_stylometric_features(text) for text in texts])\n",
        "\n",
        "# Combine features and split dataset\n",
        "X = np.hstack((distilbert_features, stylo_features))  # Ensure same dimensions\n",
        "y = labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train the SVM\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions and evaluate the model\n",
        "y_pred = svm_classifier.predict(X_test_scaled)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "nADBptFoTVe-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbec203b-1b2e-4ab0-e0c8-2778a541861f"
      },
      "id": "nADBptFoTVe-",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.93      0.95        30\n",
            "           1       0.90      0.90      0.90        30\n",
            "           2       0.59      0.67      0.62        30\n",
            "           3       0.66      0.63      0.64        30\n",
            "           4       0.70      0.63      0.67        30\n",
            "           5       0.74      0.87      0.80        30\n",
            "           6       0.93      0.93      0.93        30\n",
            "           7       0.89      0.83      0.86        30\n",
            "           8       0.66      0.77      0.71        30\n",
            "           9       0.67      0.53      0.59        30\n",
            "          10       1.00      1.00      1.00        30\n",
            "          11       0.74      0.83      0.78        30\n",
            "          12       1.00      0.93      0.97        30\n",
            "          13       0.41      0.57      0.48        30\n",
            "          14       0.83      0.80      0.81        30\n",
            "          15       0.97      1.00      0.98        30\n",
            "          16       0.79      0.63      0.70        30\n",
            "          17       0.77      0.77      0.77        30\n",
            "          18       0.70      0.77      0.73        30\n",
            "          19       0.86      0.80      0.83        30\n",
            "          20       1.00      0.97      0.98        30\n",
            "          21       0.93      0.93      0.93        30\n",
            "          22       0.83      0.83      0.83        30\n",
            "          23       0.70      0.70      0.70        30\n",
            "          24       0.83      0.63      0.72        30\n",
            "          25       0.79      0.90      0.84        30\n",
            "          26       0.93      0.87      0.90        30\n",
            "          27       0.83      0.80      0.81        30\n",
            "          28       1.00      1.00      1.00        30\n",
            "          29       0.79      0.90      0.84        30\n",
            "          30       0.85      0.97      0.91        30\n",
            "          31       0.76      0.73      0.75        30\n",
            "          32       0.97      1.00      0.98        30\n",
            "          33       0.79      0.87      0.83        30\n",
            "          34       0.56      0.50      0.53        30\n",
            "          35       0.80      0.93      0.86        30\n",
            "          36       0.92      0.77      0.84        30\n",
            "          37       0.69      0.73      0.71        30\n",
            "          38       0.86      0.80      0.83        30\n",
            "          39       0.85      0.77      0.81        30\n",
            "          40       1.00      0.97      0.98        30\n",
            "          41       0.65      0.73      0.69        30\n",
            "          42       0.81      0.73      0.77        30\n",
            "          43       0.50      0.43      0.46        30\n",
            "          44       0.87      0.87      0.87        30\n",
            "          45       0.67      0.60      0.63        30\n",
            "          46       0.75      0.80      0.77        30\n",
            "          47       0.86      0.83      0.85        30\n",
            "          48       0.87      0.87      0.87        30\n",
            "          49       0.55      0.53      0.54        30\n",
            "          50       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           0.80      1530\n",
            "   macro avg       0.80      0.80      0.80      1530\n",
            "weighted avg       0.80      0.80      0.80      1530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#38.distilbert stylo neural network , 10 epochs, cc=0.8\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, sent_tokenize\n",
        "from collections import Counter\n",
        "import textstat\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "from nltk import word_tokenize, pos_tag, sent_tokenize\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import textstat\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "def extract_stylometric_features(text):\n",
        "    # Basic processing\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    sentences = sent_tokenize(text)\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(tokens)\n",
        "    num_unique_words = len(set(tokens))\n",
        "    num_characters = len(text)\n",
        "    words = [word for word in tokens if word.isalpha()]  # Filter out punctuation and numbers\n",
        "\n",
        "    # Lexical features\n",
        "    lexical_diversity = num_unique_words / num_words if num_words else 0\n",
        "    avg_word_length = np.mean([len(word) for word in tokens]) if tokens else 0\n",
        "    avg_sentence_length = num_words / num_sentences if num_sentences else 0\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    num_stop_words = len([word for word in tokens if word in stop_words])\n",
        "    stop_words_ratio = num_stop_words / num_words if num_words else 0\n",
        "    punctuation_ratio = len([char for char in text if char in punctuation]) / num_characters if num_characters else 0\n",
        "\n",
        "    # POS tagging and related features\n",
        "    tagged_tokens = pos_tag(words)\n",
        "    pos_counts = Counter(tag for word, tag in tagged_tokens)\n",
        "    total_pos_counts = sum(pos_counts.values())\n",
        "    noun_ratio = pos_counts.get('NN', 0) / total_pos_counts if total_pos_counts else 0\n",
        "    adjective_ratio = pos_counts.get('JJ', 0) / total_pos_counts if total_pos_counts else 0\n",
        "    verb_ratio = pos_counts.get('VB', 0) / total_pos_counts if total_pos_counts else 0\n",
        "\n",
        "    # Readability scores\n",
        "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
        "    smog_index = textstat.smog_index(text)\n",
        "    coleman_liau_index = textstat.coleman_liau_index(text)\n",
        "\n",
        "    # Syntactic features\n",
        "    sentence_lengths = [len(word_tokenize(sentence)) for sentence in sentences]\n",
        "    avg_sentence_complexity = np.mean(sentence_lengths) if sentence_lengths else 0\n",
        "    sentence_length_variance = np.var(sentence_lengths) if sentence_lengths else 0\n",
        "    long_word_ratio = len([word for word in tokens if len(word) > 6]) / num_words if num_words else 0\n",
        "\n",
        "    # Return all features as a numpy array\n",
        "    return np.array([\n",
        "        num_sentences, num_words, num_unique_words, num_characters, lexical_diversity,\n",
        "        avg_word_length, avg_sentence_length, num_stop_words, stop_words_ratio,\n",
        "        punctuation_ratio, noun_ratio, adjective_ratio, verb_ratio,\n",
        "        flesch_reading_ease, smog_index, coleman_liau_index, avg_sentence_complexity,\n",
        "        sentence_length_variance, long_word_ratio\n",
        "    ])\n",
        "\n",
        "\n",
        "# Set CUDA device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class DNNClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(DNNClassifier, self).__init__()\n",
        "        # Define the first fully connected layer\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        # Define the second fully connected layer (output layer)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        # Define ReLU activation\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the first layer and activation\n",
        "        x = self.relu(self.fc1(x))\n",
        "\n",
        "        # Apply the third layer (no activation for the output layer)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Dataset Preparation\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "# Function to extract DistilBERT and stylometric features remains unchanged\n",
        "\n",
        "# Load and prepare dataset\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "texts = data['text'].tolist()\n",
        "labels = LabelEncoder().fit_transform(data['label'])  # Encode labels\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "\n",
        "# Extract features\n",
        "distilbert_features = get_distilbert_features(texts, distilbert_model, tokenizer, device)\n",
        "stylo_features = np.array([extract_stylometric_features(text) for text in texts])\n",
        "\n",
        "# Combine features and split dataset\n",
        "X = np.hstack((distilbert_features, stylo_features))  # Ensure same dimensions\n",
        "y = labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert arrays to PyTorch tensors\n",
        "X_train_scaled = torch.tensor(X_train_scaled, dtype=torch.float).to(device)\n",
        "X_test_scaled = torch.tensor(X_test_scaled, dtype=torch.float).to(device)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "train_dataset = TextDataset(X_train_scaled, y_train)\n",
        "test_dataset = TextDataset(X_test_scaled, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Define the deep neural network and optimizer\n",
        "input_dim = X_train_scaled.size(1)\n",
        "hidden_dim = 512\n",
        "output_dim = len(set(y))  # Number of classes\n",
        "model = DNNClassifier(input_dim, hidden_dim, output_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct, total = 0, 0\n",
        "    for features, labels in test_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    print(f'Accuracy: {100 * correct / total}%')\n",
        "\n",
        "# Detailed classification report\n",
        "y_true = []\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for features, labels in test_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "print(classification_report(y_true, y_pred))\n"
      ],
      "metadata": {
        "id": "af-Nvu0tTlX8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a67136d-22c5-423f-e960-6705e899c837"
      },
      "id": "af-Nvu0tTlX8",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 0.9514871835708618\n",
            "Epoch 2/20, Loss: 0.8618295192718506\n",
            "Epoch 3/20, Loss: 0.21630647778511047\n",
            "Epoch 4/20, Loss: 0.34545639157295227\n",
            "Epoch 5/20, Loss: 0.19869953393936157\n",
            "Epoch 6/20, Loss: 0.08594764024019241\n",
            "Epoch 7/20, Loss: 0.04593190550804138\n",
            "Epoch 8/20, Loss: 0.012848220765590668\n",
            "Epoch 9/20, Loss: 0.05576566606760025\n",
            "Epoch 10/20, Loss: 0.004815959371626377\n",
            "Epoch 11/20, Loss: 0.008511090651154518\n",
            "Epoch 12/20, Loss: 0.005422000773251057\n",
            "Epoch 13/20, Loss: 0.0041274442337453365\n",
            "Epoch 14/20, Loss: 0.0026776727754622698\n",
            "Epoch 15/20, Loss: 0.0038865243550390005\n",
            "Epoch 16/20, Loss: 0.0028091780841350555\n",
            "Epoch 17/20, Loss: 0.0020248438231647015\n",
            "Epoch 18/20, Loss: 0.0015689351130276918\n",
            "Epoch 19/20, Loss: 0.002871413715183735\n",
            "Epoch 20/20, Loss: 0.002159492112696171\n",
            "Accuracy: 81.04575163398692%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.90      0.93        30\n",
            "           1       0.88      0.97      0.92        30\n",
            "           2       0.69      0.60      0.64        30\n",
            "           3       0.65      0.67      0.66        30\n",
            "           4       0.71      0.67      0.69        30\n",
            "           5       0.73      0.80      0.76        30\n",
            "           6       0.90      0.93      0.92        30\n",
            "           7       0.83      0.83      0.83        30\n",
            "           8       0.77      0.80      0.79        30\n",
            "           9       0.75      0.70      0.72        30\n",
            "          10       1.00      1.00      1.00        30\n",
            "          11       0.78      0.83      0.81        30\n",
            "          12       0.93      0.90      0.92        30\n",
            "          13       0.44      0.50      0.47        30\n",
            "          14       0.82      0.77      0.79        30\n",
            "          15       1.00      0.97      0.98        30\n",
            "          16       0.77      0.80      0.79        30\n",
            "          17       0.70      0.70      0.70        30\n",
            "          18       0.84      0.87      0.85        30\n",
            "          19       0.82      0.90      0.86        30\n",
            "          20       1.00      0.97      0.98        30\n",
            "          21       0.96      0.90      0.93        30\n",
            "          22       0.90      0.87      0.88        30\n",
            "          23       0.70      0.63      0.67        30\n",
            "          24       0.81      0.73      0.77        30\n",
            "          25       0.85      0.93      0.89        30\n",
            "          26       0.93      0.90      0.92        30\n",
            "          27       0.92      0.80      0.86        30\n",
            "          28       0.97      1.00      0.98        30\n",
            "          29       0.79      0.90      0.84        30\n",
            "          30       0.80      0.93      0.86        30\n",
            "          31       0.79      0.77      0.78        30\n",
            "          32       0.97      1.00      0.98        30\n",
            "          33       0.76      0.83      0.79        30\n",
            "          34       0.60      0.60      0.60        30\n",
            "          35       0.84      0.90      0.87        30\n",
            "          36       0.83      0.83      0.83        30\n",
            "          37       0.77      0.77      0.77        30\n",
            "          38       0.89      0.80      0.84        30\n",
            "          39       0.96      0.80      0.87        30\n",
            "          40       0.97      0.97      0.97        30\n",
            "          41       0.68      0.83      0.75        30\n",
            "          42       0.78      0.70      0.74        30\n",
            "          43       0.46      0.43      0.45        30\n",
            "          44       0.81      0.83      0.82        30\n",
            "          45       0.61      0.67      0.63        30\n",
            "          46       0.79      0.77      0.78        30\n",
            "          47       0.93      0.83      0.88        30\n",
            "          48       0.86      0.80      0.83        30\n",
            "          49       0.55      0.53      0.54        30\n",
            "          50       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           0.81      1530\n",
            "   macro avg       0.81      0.81      0.81      1530\n",
            "weighted avg       0.81      0.81      0.81      1530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#39.distilbert stylo neural network with more hidden layers , 10 epochs, cc=0.8\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, sent_tokenize\n",
        "from collections import Counter\n",
        "import textstat\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "from nltk import word_tokenize, pos_tag, sent_tokenize\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import textstat\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "\n",
        "def extract_stylometric_features(text):\n",
        "    # Basic processing\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    sentences = sent_tokenize(text)\n",
        "    num_sentences = len(sentences)\n",
        "    num_words = len(tokens)\n",
        "    num_unique_words = len(set(tokens))\n",
        "    num_characters = len(text)\n",
        "    words = [word for word in tokens if word.isalpha()]  # Filter out punctuation and numbers\n",
        "\n",
        "    # Lexical features\n",
        "    lexical_diversity = num_unique_words / num_words if num_words else 0\n",
        "    avg_word_length = np.mean([len(word) for word in tokens]) if tokens else 0\n",
        "    avg_sentence_length = num_words / num_sentences if num_sentences else 0\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    num_stop_words = len([word for word in tokens if word in stop_words])\n",
        "    stop_words_ratio = num_stop_words / num_words if num_words else 0\n",
        "    punctuation_ratio = len([char for char in text if char in punctuation]) / num_characters if num_characters else 0\n",
        "\n",
        "    # POS tagging and related features\n",
        "    tagged_tokens = pos_tag(words)\n",
        "    pos_counts = Counter(tag for word, tag in tagged_tokens)\n",
        "    total_pos_counts = sum(pos_counts.values())\n",
        "    noun_ratio = pos_counts.get('NN', 0) / total_pos_counts if total_pos_counts else 0\n",
        "    adjective_ratio = pos_counts.get('JJ', 0) / total_pos_counts if total_pos_counts else 0\n",
        "    verb_ratio = pos_counts.get('VB', 0) / total_pos_counts if total_pos_counts else 0\n",
        "\n",
        "    # Readability scores\n",
        "    flesch_reading_ease = textstat.flesch_reading_ease(text)\n",
        "    smog_index = textstat.smog_index(text)\n",
        "    coleman_liau_index = textstat.coleman_liau_index(text)\n",
        "\n",
        "    # Syntactic features\n",
        "    sentence_lengths = [len(word_tokenize(sentence)) for sentence in sentences]\n",
        "    avg_sentence_complexity = np.mean(sentence_lengths) if sentence_lengths else 0\n",
        "    sentence_length_variance = np.var(sentence_lengths) if sentence_lengths else 0\n",
        "    long_word_ratio = len([word for word in tokens if len(word) > 6]) / num_words if num_words else 0\n",
        "\n",
        "    # Return all features as a numpy array\n",
        "    return np.array([\n",
        "        num_sentences, num_words, num_unique_words, num_characters, lexical_diversity,\n",
        "        avg_word_length, avg_sentence_length, num_stop_words, stop_words_ratio,\n",
        "        punctuation_ratio, noun_ratio, adjective_ratio, verb_ratio,\n",
        "        flesch_reading_ease, smog_index, coleman_liau_index, avg_sentence_complexity,\n",
        "        sentence_length_variance, long_word_ratio\n",
        "    ])\n",
        "\n",
        "\n",
        "# Set CUDA device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class DNNClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, hidden_dim2, output_dim):\n",
        "        super(DNNClassifier, self).__init__()\n",
        "        # Define the first fully connected layer\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "        # Define the second fully connected layer\n",
        "        self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "        # Define the third fully connected layer (output layer)\n",
        "        self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "        # Define ReLU activation\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the first layer and activation\n",
        "        x = self.relu(self.fc1(x))\n",
        "        # Apply the second layer and activation\n",
        "        x = self.relu(self.fc2(x))\n",
        "        # Apply the third layer (no activation for the output layer)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Dataset Preparation\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "# Function to extract DistilBERT and stylometric features remains unchanged\n",
        "\n",
        "# Load and prepare dataset\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "texts = data['text'].tolist()\n",
        "labels = LabelEncoder().fit_transform(data['label'])  # Encode labels\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "\n",
        "# Extract features\n",
        "distilbert_features = get_distilbert_features(texts, distilbert_model, tokenizer, device)\n",
        "stylo_features = np.array([extract_stylometric_features(text) for text in texts])\n",
        "\n",
        "# Combine features and split dataset\n",
        "X = np.hstack((distilbert_features, stylo_features))  # Ensure same dimensions\n",
        "y = labels\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Convert arrays to PyTorch tensors\n",
        "X_train_scaled = torch.tensor(X_train_scaled, dtype=torch.float).to(device)\n",
        "X_test_scaled = torch.tensor(X_test_scaled, dtype=torch.float).to(device)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "train_dataset = TextDataset(X_train_scaled, y_train)\n",
        "test_dataset = TextDataset(X_test_scaled, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Define the deep neural network and optimizer\n",
        "input_dim = X_train_scaled.size(1)\n",
        "hidden_dim1 = 512\n",
        "hidden_dim2 = 10\n",
        "output_dim = len(set(y))  # Number of classes\n",
        "model = DNNClassifier(input_dim, hidden_dim1, hidden_dim2, output_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters())\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct, total = 0, 0\n",
        "    for features, labels in test_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    print(f'Accuracy: {100 * correct / total}%')\n",
        "\n",
        "# Detailed classification report\n",
        "y_true = []\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for features, labels in test_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "print(classification_report(y_true, y_pred))\n"
      ],
      "metadata": {
        "id": "gQ79e_yYEDsH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9939892d-a82d-4015-9c25-45af3cf8227b"
      },
      "id": "gQ79e_yYEDsH",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 2.5504586696624756\n",
            "Epoch 2/20, Loss: 1.5448086261749268\n",
            "Epoch 3/20, Loss: 0.9748360514640808\n",
            "Epoch 4/20, Loss: 0.6877453327178955\n",
            "Epoch 5/20, Loss: 0.3202219307422638\n",
            "Epoch 6/20, Loss: 0.3711627721786499\n",
            "Epoch 7/20, Loss: 0.31467360258102417\n",
            "Epoch 8/20, Loss: 0.2847411632537842\n",
            "Epoch 9/20, Loss: 0.10691787302494049\n",
            "Epoch 10/20, Loss: 0.21420596539974213\n",
            "Epoch 11/20, Loss: 0.12262323498725891\n",
            "Epoch 12/20, Loss: 0.047847531735897064\n",
            "Epoch 13/20, Loss: 0.03752594441175461\n",
            "Epoch 14/20, Loss: 0.016160883009433746\n",
            "Epoch 15/20, Loss: 0.04577929526567459\n",
            "Epoch 16/20, Loss: 0.013877897523343563\n",
            "Epoch 17/20, Loss: 0.013495879247784615\n",
            "Epoch 18/20, Loss: 0.013190333731472492\n",
            "Epoch 19/20, Loss: 0.0032484102994203568\n",
            "Epoch 20/20, Loss: 0.004343257751315832\n",
            "Accuracy: 74.640522875817%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.83      0.86        30\n",
            "           1       0.82      0.93      0.87        30\n",
            "           2       0.61      0.47      0.53        30\n",
            "           3       0.50      0.40      0.44        30\n",
            "           4       0.69      0.73      0.71        30\n",
            "           5       0.61      0.67      0.63        30\n",
            "           6       0.76      0.93      0.84        30\n",
            "           7       0.79      0.77      0.78        30\n",
            "           8       0.69      0.73      0.71        30\n",
            "           9       0.58      0.63      0.60        30\n",
            "          10       0.94      0.97      0.95        30\n",
            "          11       0.89      0.83      0.86        30\n",
            "          12       0.82      0.90      0.86        30\n",
            "          13       0.42      0.67      0.51        30\n",
            "          14       0.86      0.83      0.85        30\n",
            "          15       0.96      0.87      0.91        30\n",
            "          16       0.67      0.67      0.67        30\n",
            "          17       0.77      0.67      0.71        30\n",
            "          18       0.77      0.80      0.79        30\n",
            "          19       0.77      0.77      0.77        30\n",
            "          20       1.00      0.97      0.98        30\n",
            "          21       0.85      0.93      0.89        30\n",
            "          22       0.62      0.77      0.69        30\n",
            "          23       0.67      0.67      0.67        30\n",
            "          24       0.74      0.67      0.70        30\n",
            "          25       0.87      0.87      0.87        30\n",
            "          26       0.91      0.70      0.79        30\n",
            "          27       0.84      0.70      0.76        30\n",
            "          28       1.00      1.00      1.00        30\n",
            "          29       0.79      0.87      0.83        30\n",
            "          30       0.89      0.80      0.84        30\n",
            "          31       0.63      0.63      0.63        30\n",
            "          32       0.97      1.00      0.98        30\n",
            "          33       0.80      0.80      0.80        30\n",
            "          34       0.39      0.43      0.41        30\n",
            "          35       0.63      0.90      0.74        30\n",
            "          36       0.81      0.73      0.77        30\n",
            "          37       0.64      0.70      0.67        30\n",
            "          38       0.92      0.73      0.81        30\n",
            "          39       0.86      0.60      0.71        30\n",
            "          40       0.89      0.83      0.86        30\n",
            "          41       0.62      0.77      0.69        30\n",
            "          42       0.83      0.67      0.74        30\n",
            "          43       0.44      0.40      0.42        30\n",
            "          44       0.83      0.80      0.81        30\n",
            "          45       0.54      0.50      0.52        30\n",
            "          46       0.66      0.63      0.64        30\n",
            "          47       0.79      0.77      0.78        30\n",
            "          48       0.79      0.73      0.76        30\n",
            "          49       0.52      0.47      0.49        30\n",
            "          50       0.97      0.97      0.97        30\n",
            "\n",
            "    accuracy                           0.75      1530\n",
            "   macro avg       0.75      0.75      0.75      1530\n",
            "weighted avg       0.75      0.75      0.75      1530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#40.distilbert with deep learning\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Set CUDA device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Neural Network Classifier\n",
        "class DNNClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(DNNClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Dataset Preparation\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "# Function to extract DistilBERT features\n",
        "def get_distilbert_features(texts, model, tokenizer, device):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    features = []\n",
        "    with torch.no_grad():  # No need to track gradients\n",
        "        for text in texts:\n",
        "            encoded_input = tokenizer(text, return_tensors='pt', truncation=True, max_length=512, padding=True).to(device)\n",
        "            output = model(**encoded_input)\n",
        "            feature = output.last_hidden_state.mean(dim=1).cpu().numpy()  # Using mean pooling\n",
        "            features.append(feature)\n",
        "    return np.vstack(features)  # Shape: (num_texts, feature_dim)\n",
        "\n",
        "# Load and prepare dataset\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "texts = data['text'].tolist()\n",
        "labels = LabelEncoder().fit_transform(data['label'])  # Encode labels\n",
        "\n",
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "distilbert_model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "\n",
        "# Extract features\n",
        "distilbert_features = get_distilbert_features(texts, distilbert_model, tokenizer, device)\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(distilbert_features, labels, test_size=0.3, random_state=42, stratify=labels)\n",
        "\n",
        "# Convert arrays to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float).to(device)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float).to(device)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "test_dataset = TextDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Define the deep neural network and optimizer\n",
        "input_dim = X_train.size(1)\n",
        "hidden_dim = 512\n",
        "output_dim = len(np.unique(labels))  # Number of classes\n",
        "model = DNNClassifier(input_dim, hidden_dim, output_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20  # Number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct, total = 0, 0\n",
        "    for features, labels in test_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    print(f'Accuracy: {100 * correct / total}%')\n",
        "\n",
        "# Detailed classification report\n",
        "y_true = []\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for features, labels in test_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "print(classification_report(y_true, y_pred))\n"
      ],
      "metadata": {
        "id": "Ax3RlSfEJzS0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bb48a68-27ef-499d-9554-2b0a49d3886f"
      },
      "id": "Ax3RlSfEJzS0",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 2.380303382873535\n",
            "Epoch 2/20, Loss: 1.3905844688415527\n",
            "Epoch 3/20, Loss: 1.353975772857666\n",
            "Epoch 4/20, Loss: 1.0998156070709229\n",
            "Epoch 5/20, Loss: 0.851585328578949\n",
            "Epoch 6/20, Loss: 1.196790337562561\n",
            "Epoch 7/20, Loss: 0.8140201568603516\n",
            "Epoch 8/20, Loss: 0.8356301784515381\n",
            "Epoch 9/20, Loss: 0.4170796573162079\n",
            "Epoch 10/20, Loss: 0.5127095580101013\n",
            "Epoch 11/20, Loss: 0.7359209656715393\n",
            "Epoch 12/20, Loss: 0.7723331451416016\n",
            "Epoch 13/20, Loss: 0.6500920057296753\n",
            "Epoch 14/20, Loss: 0.5931839346885681\n",
            "Epoch 15/20, Loss: 0.6926267147064209\n",
            "Epoch 16/20, Loss: 0.5288317203521729\n",
            "Epoch 17/20, Loss: 0.563762903213501\n",
            "Epoch 18/20, Loss: 0.46042191982269287\n",
            "Epoch 19/20, Loss: 0.35611802339553833\n",
            "Epoch 20/20, Loss: 0.23834456503391266\n",
            "Accuracy: 74.57516339869281%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.83      0.86        30\n",
            "           1       0.90      0.90      0.90        30\n",
            "           2       0.65      0.73      0.69        30\n",
            "           3       0.53      0.53      0.53        30\n",
            "           4       0.93      0.47      0.62        30\n",
            "           5       0.73      0.73      0.73        30\n",
            "           6       0.82      0.93      0.87        30\n",
            "           7       0.76      0.73      0.75        30\n",
            "           8       0.83      0.63      0.72        30\n",
            "           9       0.62      0.53      0.57        30\n",
            "          10       1.00      1.00      1.00        30\n",
            "          11       0.74      0.83      0.78        30\n",
            "          12       1.00      0.90      0.95        30\n",
            "          13       0.32      0.53      0.40        30\n",
            "          14       0.68      0.87      0.76        30\n",
            "          15       0.96      0.87      0.91        30\n",
            "          16       0.79      0.50      0.61        30\n",
            "          17       0.80      0.53      0.64        30\n",
            "          18       0.90      0.87      0.88        30\n",
            "          19       0.63      0.90      0.74        30\n",
            "          20       1.00      0.93      0.97        30\n",
            "          21       1.00      0.83      0.91        30\n",
            "          22       0.70      0.77      0.73        30\n",
            "          23       0.62      0.80      0.70        30\n",
            "          24       0.75      0.80      0.77        30\n",
            "          25       0.71      0.90      0.79        30\n",
            "          26       0.89      0.83      0.86        30\n",
            "          27       0.88      0.77      0.82        30\n",
            "          28       0.97      1.00      0.98        30\n",
            "          29       0.56      0.93      0.70        30\n",
            "          30       0.69      0.97      0.81        30\n",
            "          31       0.77      0.67      0.71        30\n",
            "          32       0.94      1.00      0.97        30\n",
            "          33       0.73      0.73      0.73        30\n",
            "          34       0.50      0.37      0.42        30\n",
            "          35       0.83      0.83      0.83        30\n",
            "          36       0.88      0.70      0.78        30\n",
            "          37       0.55      0.80      0.65        30\n",
            "          38       0.85      0.37      0.51        30\n",
            "          39       1.00      0.73      0.85        30\n",
            "          40       0.93      0.93      0.93        30\n",
            "          41       0.55      0.73      0.63        30\n",
            "          42       0.79      0.63      0.70        30\n",
            "          43       0.38      0.30      0.33        30\n",
            "          44       0.70      0.87      0.78        30\n",
            "          45       0.57      0.40      0.47        30\n",
            "          46       0.60      0.70      0.65        30\n",
            "          47       0.87      0.67      0.75        30\n",
            "          48       0.71      0.73      0.72        30\n",
            "          49       0.62      0.50      0.56        30\n",
            "          50       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           0.75      1530\n",
            "   macro avg       0.77      0.75      0.74      1530\n",
            "weighted avg       0.77      0.75      0.74      1530\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#41.tfidf with deep learning\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import joblib\n",
        "\n",
        "\n",
        "# Set CUDA device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Neural Network Classifier\n",
        "class DNNClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(DNNClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Dataset Preparation\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "# Load and prepare dataset\n",
        "data = pd.read_excel('50human100eachgpt4withnumber.xlsx')\n",
        "texts = data['text'].tolist()\n",
        "labels = LabelEncoder().fit_transform(data['label'])  # Encode labels\n",
        "\n",
        "# Extract TF-IDF features\n",
        "vectorizer = TfidfVectorizer(max_features=5000)\n",
        "tfidf_features = vectorizer.fit_transform(texts).toarray()\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_features, labels, test_size=0.3, random_state=42, stratify=labels)\n",
        "\n",
        "# Convert arrays to PyTorch tensors\n",
        "X_train = torch.tensor(X_train, dtype=torch.float).to(device)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float).to(device)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
        "\n",
        "# Create Datasets and DataLoaders\n",
        "train_dataset = TextDataset(X_train, y_train)\n",
        "test_dataset = TextDataset(X_test, y_test)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "# Define the deep neural network and optimizer\n",
        "input_dim = X_train.size(1)  # This should now match the number of TF-IDF features\n",
        "hidden_dim = 512\n",
        "output_dim = len(np.unique(labels))  # Number of classes\n",
        "model = DNNClassifier(input_dim, hidden_dim, output_dim).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 20  # Number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for features, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(features)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}')\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct, total = 0, 0\n",
        "    for features, labels in test_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "    print(f'Accuracy: {100 * correct / total}%')\n",
        "\n",
        "# Detailed classification report\n",
        "y_true = []\n",
        "y_pred = []\n",
        "with torch.no_grad():\n",
        "    for features, labels in test_loader:\n",
        "        outputs = model(features)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(predicted.cpu().numpy())\n",
        "print(classification_report(y_true, y_pred))\n",
        "print(input_dim, hidden_dim)\n",
        "joblib.dump(vectorizer, 'tfidf_vectorizer_dnn.pkl')\n",
        "torch.save(model.state_dict(), 'dnn_classifier.pth')\n"
      ],
      "metadata": {
        "id": "FI27h6RVUNn9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56741859-fb35-4374-96cd-5e0e67251ac1"
      },
      "id": "FI27h6RVUNn9",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 2.85404634475708\n",
            "Epoch 2/20, Loss: 1.405126929283142\n",
            "Epoch 3/20, Loss: 0.7566953301429749\n",
            "Epoch 4/20, Loss: 0.23235899209976196\n",
            "Epoch 5/20, Loss: 0.036593981087207794\n",
            "Epoch 6/20, Loss: 0.05294063314795494\n",
            "Epoch 7/20, Loss: 0.058490440249443054\n",
            "Epoch 8/20, Loss: 0.050724368542432785\n",
            "Epoch 9/20, Loss: 0.021397003903985023\n",
            "Epoch 10/20, Loss: 0.01335510890930891\n",
            "Epoch 11/20, Loss: 0.01952095329761505\n",
            "Epoch 12/20, Loss: 0.014045165851712227\n",
            "Epoch 13/20, Loss: 0.009324265643954277\n",
            "Epoch 14/20, Loss: 0.005260825622826815\n",
            "Epoch 15/20, Loss: 0.007539721205830574\n",
            "Epoch 16/20, Loss: 0.00598728284239769\n",
            "Epoch 17/20, Loss: 0.003876946633681655\n",
            "Epoch 18/20, Loss: 0.004602027591317892\n",
            "Epoch 19/20, Loss: 0.004438542760908604\n",
            "Epoch 20/20, Loss: 0.0021976956631988287\n",
            "Accuracy: 83.4640522875817%\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.90      0.89        30\n",
            "           1       0.96      0.90      0.93        30\n",
            "           2       0.71      0.73      0.72        30\n",
            "           3       0.50      0.47      0.48        30\n",
            "           4       0.83      0.83      0.83        30\n",
            "           5       0.75      0.90      0.82        30\n",
            "           6       0.97      0.93      0.95        30\n",
            "           7       0.93      0.87      0.90        30\n",
            "           8       0.88      1.00      0.94        30\n",
            "           9       0.83      0.63      0.72        30\n",
            "          10       0.94      1.00      0.97        30\n",
            "          11       0.77      0.90      0.83        30\n",
            "          12       0.94      1.00      0.97        30\n",
            "          13       0.50      0.47      0.48        30\n",
            "          14       0.89      0.80      0.84        30\n",
            "          15       0.96      0.90      0.93        30\n",
            "          16       0.89      0.80      0.84        30\n",
            "          17       0.80      0.93      0.86        30\n",
            "          18       0.90      0.90      0.90        30\n",
            "          19       0.96      0.90      0.93        30\n",
            "          20       1.00      1.00      1.00        30\n",
            "          21       0.94      0.97      0.95        30\n",
            "          22       0.77      0.90      0.83        30\n",
            "          23       0.80      0.80      0.80        30\n",
            "          24       0.93      0.83      0.88        30\n",
            "          25       0.90      0.93      0.92        30\n",
            "          26       0.96      0.80      0.87        30\n",
            "          27       0.88      0.73      0.80        30\n",
            "          28       0.97      1.00      0.98        30\n",
            "          29       0.76      0.87      0.81        30\n",
            "          30       0.82      0.90      0.86        30\n",
            "          31       0.92      0.80      0.86        30\n",
            "          32       1.00      1.00      1.00        30\n",
            "          33       0.87      0.87      0.87        30\n",
            "          34       0.50      0.60      0.55        30\n",
            "          35       0.80      0.93      0.86        30\n",
            "          36       0.86      0.83      0.85        30\n",
            "          37       0.67      0.73      0.70        30\n",
            "          38       0.88      0.77      0.82        30\n",
            "          39       0.96      0.80      0.87        30\n",
            "          40       0.97      0.97      0.97        30\n",
            "          41       0.71      0.97      0.82        30\n",
            "          42       0.80      0.80      0.80        30\n",
            "          43       0.46      0.40      0.43        30\n",
            "          44       0.93      0.83      0.88        30\n",
            "          45       0.69      0.60      0.64        30\n",
            "          46       0.88      0.77      0.82        30\n",
            "          47       0.93      0.93      0.93        30\n",
            "          48       0.88      0.93      0.90        30\n",
            "          49       0.53      0.53      0.53        30\n",
            "          50       1.00      1.00      1.00        30\n",
            "\n",
            "    accuracy                           0.83      1530\n",
            "   macro avg       0.84      0.83      0.83      1530\n",
            "weighted avg       0.84      0.83      0.83      1530\n",
            "\n",
            "5000 512\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3e4077c564254d659d9676a9e537a661": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8de8c5d982de48769db21381a3588103",
              "IPY_MODEL_27b253e716424dd8bbabd4c392cafed5",
              "IPY_MODEL_f257f150ac904743a135d4a6befd4a3e"
            ],
            "layout": "IPY_MODEL_0c74e6c437b64316831a059553c19ec1"
          }
        },
        "8de8c5d982de48769db21381a3588103": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1344389791c47f89e21c1a910e7ce9a",
            "placeholder": "",
            "style": "IPY_MODEL_def864be73874f3485545d8d6844575d",
            "value": "tokenizer_config.json:100%"
          }
        },
        "27b253e716424dd8bbabd4c392cafed5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0583237535e4e539c05a876d33d800b",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6855c5abd7d94a7aa1b198fe7d3fb71c",
            "value": 28
          }
        },
        "f257f150ac904743a135d4a6befd4a3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b22ca82fdde447c1a43a955ca3761f2f",
            "placeholder": "",
            "style": "IPY_MODEL_c4a9c36f77bf4acfaa4954d6f2cb1fe6",
            "value": "28.0/28.0[00:00&lt;00:00,2.35kB/s]"
          }
        },
        "0c74e6c437b64316831a059553c19ec1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1344389791c47f89e21c1a910e7ce9a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "def864be73874f3485545d8d6844575d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d0583237535e4e539c05a876d33d800b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6855c5abd7d94a7aa1b198fe7d3fb71c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b22ca82fdde447c1a43a955ca3761f2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4a9c36f77bf4acfaa4954d6f2cb1fe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ccc7097fc404c6e9bc3418d36fc2abf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f47e5571535944209d7aef28d9d8e2ed",
              "IPY_MODEL_abe1138bb6cf402cba789355fc307408",
              "IPY_MODEL_c66d2c7de4254f6e914ffae9e322f69b"
            ],
            "layout": "IPY_MODEL_e108b093dc5d4730b1c846ccb30b7f98"
          }
        },
        "f47e5571535944209d7aef28d9d8e2ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cd56b5fe652418585c86482c906e00e",
            "placeholder": "",
            "style": "IPY_MODEL_e9d955186a634b38a581a5b28ba180fc",
            "value": "vocab.txt:100%"
          }
        },
        "abe1138bb6cf402cba789355fc307408": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da0e55b305854ef1996b5731c4f36b59",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_12e4cb959fc8455a9debb0cbedf85f92",
            "value": 231508
          }
        },
        "c66d2c7de4254f6e914ffae9e322f69b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_894692a8417a411abd8d788ff45f1b22",
            "placeholder": "",
            "style": "IPY_MODEL_9e8f828dc382497dbd2daf70b077cc55",
            "value": "232k/232k[00:00&lt;00:00,1.39MB/s]"
          }
        },
        "e108b093dc5d4730b1c846ccb30b7f98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cd56b5fe652418585c86482c906e00e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9d955186a634b38a581a5b28ba180fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "da0e55b305854ef1996b5731c4f36b59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12e4cb959fc8455a9debb0cbedf85f92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "894692a8417a411abd8d788ff45f1b22": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e8f828dc382497dbd2daf70b077cc55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48333a60a19040c89a950b20690d54ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_96d49fac36974544a58cea85dd7e58bb",
              "IPY_MODEL_66dcf0f94dcb41d3976e7663ba6b76c4",
              "IPY_MODEL_1ab93c7bab6a41ee94a1e331dcfbdf7a"
            ],
            "layout": "IPY_MODEL_49c0a1487c784609b516d4a9a922d70a"
          }
        },
        "96d49fac36974544a58cea85dd7e58bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1d3cdea040942668de86452630128d1",
            "placeholder": "",
            "style": "IPY_MODEL_300cdcc0713b4790b8874a76dce10488",
            "value": "tokenizer.json:100%"
          }
        },
        "66dcf0f94dcb41d3976e7663ba6b76c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86b69020fbb44283b428689f1e2db474",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b67607997ae14a33805cb27d24590157",
            "value": 466062
          }
        },
        "1ab93c7bab6a41ee94a1e331dcfbdf7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8216787125440dca2a86b03ba980ad5",
            "placeholder": "",
            "style": "IPY_MODEL_2b63142093d841d49a68c6800e58a89a",
            "value": "466k/466k[00:00&lt;00:00,1.93MB/s]"
          }
        },
        "49c0a1487c784609b516d4a9a922d70a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1d3cdea040942668de86452630128d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "300cdcc0713b4790b8874a76dce10488": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "86b69020fbb44283b428689f1e2db474": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b67607997ae14a33805cb27d24590157": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8216787125440dca2a86b03ba980ad5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b63142093d841d49a68c6800e58a89a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "245cef7ff1a547e998c733ac3b307cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff8db09258ee4686b82f2c7f0e6784ca",
              "IPY_MODEL_f58b9805ae324df09cdb689cf902a239",
              "IPY_MODEL_e123f2103c2c4c96a8ae236281b7dbc8"
            ],
            "layout": "IPY_MODEL_6cec1424306e4cc9b4ac0f1f25c78eb6"
          }
        },
        "ff8db09258ee4686b82f2c7f0e6784ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e6e434ff5034e369c57bd41fac23795",
            "placeholder": "",
            "style": "IPY_MODEL_d9f8236a2d47464aac4ec8822425d42d",
            "value": "config.json:100%"
          }
        },
        "f58b9805ae324df09cdb689cf902a239": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41d944d0f3b24dda9d5be4715d2df279",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27b7d627b2444d55ad9a5feef52d7e1e",
            "value": 483
          }
        },
        "e123f2103c2c4c96a8ae236281b7dbc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d0aa1514dbb412e846bc66d6b9aafd4",
            "placeholder": "",
            "style": "IPY_MODEL_89d56bdb346940da8dc75ae98fdf38f5",
            "value": "483/483[00:00&lt;00:00,41.2kB/s]"
          }
        },
        "6cec1424306e4cc9b4ac0f1f25c78eb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e6e434ff5034e369c57bd41fac23795": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9f8236a2d47464aac4ec8822425d42d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41d944d0f3b24dda9d5be4715d2df279": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27b7d627b2444d55ad9a5feef52d7e1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d0aa1514dbb412e846bc66d6b9aafd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89d56bdb346940da8dc75ae98fdf38f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1caf32b40fc641e391f2a61f46561024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5c4c2020fed54648a1c1935b20faccd3",
              "IPY_MODEL_b617fb1a27af442aae3abb7abc78e55c",
              "IPY_MODEL_0fdc02b51e8948a7aa89c01442e702a8"
            ],
            "layout": "IPY_MODEL_2d8766118dec44a18c8f9bf4d1e3cf1b"
          }
        },
        "5c4c2020fed54648a1c1935b20faccd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2533df30a5a7487b98b097ce7a08839a",
            "placeholder": "",
            "style": "IPY_MODEL_e2895621c0024586ba5e65c2e93005b0",
            "value": "model.safetensors:100%"
          }
        },
        "b617fb1a27af442aae3abb7abc78e55c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fab914551cea4b84b3d8dc479f1fae31",
            "max": 267954768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4958bc259e52445d8234726525f37e01",
            "value": 267954768
          }
        },
        "0fdc02b51e8948a7aa89c01442e702a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2355060533284840baf92cfd34e2a68b",
            "placeholder": "",
            "style": "IPY_MODEL_7a454298be0e49b2bfc9b7d868e2c0c0",
            "value": "268M/268M[00:00&lt;00:00,395MB/s]"
          }
        },
        "2d8766118dec44a18c8f9bf4d1e3cf1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2533df30a5a7487b98b097ce7a08839a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2895621c0024586ba5e65c2e93005b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fab914551cea4b84b3d8dc479f1fae31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4958bc259e52445d8234726525f37e01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2355060533284840baf92cfd34e2a68b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a454298be0e49b2bfc9b7d868e2c0c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5187545c65845718ae7b85b14d23aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff186aa3696f438c82b1e472769aa0bf",
              "IPY_MODEL_1ddcce33603f4e0d9251ed7ef77f8bb4",
              "IPY_MODEL_a6ca013b8a9143fd97cc4ace9f3d44eb"
            ],
            "layout": "IPY_MODEL_7a048262f77e47bf8e0c223374949d00"
          }
        },
        "ff186aa3696f438c82b1e472769aa0bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e24b6d371e574d549fcc88bda6c28bfa",
            "placeholder": "",
            "style": "IPY_MODEL_8f29fa711da44fa492aae72f5e545ad9",
            "value": "tokenizer_config.json:100%"
          }
        },
        "1ddcce33603f4e0d9251ed7ef77f8bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbfb861b9f6a45fc98daf7ec0ec1e0ed",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3478a4cb73f422d9f7da32850fd0263",
            "value": 25
          }
        },
        "a6ca013b8a9143fd97cc4ace9f3d44eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1dcc77efdfcd4a99b9660852d29a6a93",
            "placeholder": "",
            "style": "IPY_MODEL_ccdd0f705c2c4dc0bc2e2f40da42af70",
            "value": "25.0/25.0[00:00&lt;00:00,2.14kB/s]"
          }
        },
        "7a048262f77e47bf8e0c223374949d00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e24b6d371e574d549fcc88bda6c28bfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f29fa711da44fa492aae72f5e545ad9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbfb861b9f6a45fc98daf7ec0ec1e0ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3478a4cb73f422d9f7da32850fd0263": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1dcc77efdfcd4a99b9660852d29a6a93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccdd0f705c2c4dc0bc2e2f40da42af70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74a6edd44feb4962ac0c36e4133f1b15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_01de6fd5c4f94431a9a9f71e71ee2899",
              "IPY_MODEL_fef6b7d83d154b808af242bceb3c05e3",
              "IPY_MODEL_0e4dcfeb65da4d588ef291620fd6beca"
            ],
            "layout": "IPY_MODEL_625a9ac712f3483e8e2e0b7ef87e7250"
          }
        },
        "01de6fd5c4f94431a9a9f71e71ee2899": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd8e268575ba4c9e84d0687f61cbaed6",
            "placeholder": "",
            "style": "IPY_MODEL_bf00d741d2e7497395fc48f220d31210",
            "value": "vocab.json:100%"
          }
        },
        "fef6b7d83d154b808af242bceb3c05e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_16e330cea71f4781bdcbe4a825bdbf72",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_346ef7bc08a640ae8b6d70a5573e3a05",
            "value": 898823
          }
        },
        "0e4dcfeb65da4d588ef291620fd6beca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa775951e8a44075b18d07f8d6fbb20e",
            "placeholder": "",
            "style": "IPY_MODEL_3f8c460bde484dc39525ddcd9a2cd6b3",
            "value": "899k/899k[00:00&lt;00:00,1.85MB/s]"
          }
        },
        "625a9ac712f3483e8e2e0b7ef87e7250": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd8e268575ba4c9e84d0687f61cbaed6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf00d741d2e7497395fc48f220d31210": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "16e330cea71f4781bdcbe4a825bdbf72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "346ef7bc08a640ae8b6d70a5573e3a05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aa775951e8a44075b18d07f8d6fbb20e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f8c460bde484dc39525ddcd9a2cd6b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e14f0d7ed08a454f8c8c46179443f849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_306697d7aab744a6840f0930ace6cdee",
              "IPY_MODEL_1b868d5ed1f7436a9101ba9714121575",
              "IPY_MODEL_6aeb10e2ca5d4686b932f3f77049abbf"
            ],
            "layout": "IPY_MODEL_fc03084d822e40629a6c5e64aed48722"
          }
        },
        "306697d7aab744a6840f0930ace6cdee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adcf218e347b413ea305730e7835b7e9",
            "placeholder": "",
            "style": "IPY_MODEL_d8ae5a57eba646bab6aeeeb7a7ea39bf",
            "value": "merges.txt:100%"
          }
        },
        "1b868d5ed1f7436a9101ba9714121575": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eaea7db6ca494ee194d8e48ca2232495",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a2f5f6013a7649339b2a7a33246056ec",
            "value": 456318
          }
        },
        "6aeb10e2ca5d4686b932f3f77049abbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41ae981540d24cdd84b18171dba640ec",
            "placeholder": "",
            "style": "IPY_MODEL_5c5b2ec0102b4300972dae852f8d67d5",
            "value": "456k/456k[00:00&lt;00:00,1.85MB/s]"
          }
        },
        "fc03084d822e40629a6c5e64aed48722": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adcf218e347b413ea305730e7835b7e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8ae5a57eba646bab6aeeeb7a7ea39bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eaea7db6ca494ee194d8e48ca2232495": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2f5f6013a7649339b2a7a33246056ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41ae981540d24cdd84b18171dba640ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c5b2ec0102b4300972dae852f8d67d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2a402058514497480c63fdea50a7b0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0b200710fa7f406fae8314b172c056c0",
              "IPY_MODEL_8306d5c54b1b49f0a7c86017847f604e",
              "IPY_MODEL_ec2eb65b361b4a58a03eaeed65161dc7"
            ],
            "layout": "IPY_MODEL_c200c2b857bf451cb9b9eac3b2ee5395"
          }
        },
        "0b200710fa7f406fae8314b172c056c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c28c8e5586dd4e3fb8815624eae66c76",
            "placeholder": "",
            "style": "IPY_MODEL_0248373c416c405dbdc8fb13b15bf937",
            "value": "tokenizer.json:100%"
          }
        },
        "8306d5c54b1b49f0a7c86017847f604e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5fb4e47d63e4b6191009f0faac5f170",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90f9831538f74acc80db28ad2f40dba9",
            "value": 1355863
          }
        },
        "ec2eb65b361b4a58a03eaeed65161dc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f3cf2c788544bbc9823ed90131f7fab",
            "placeholder": "",
            "style": "IPY_MODEL_2e64a0f51bc1496eae90f72e136389c6",
            "value": "1.36M/1.36M[00:00&lt;00:00,3.31MB/s]"
          }
        },
        "c200c2b857bf451cb9b9eac3b2ee5395": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c28c8e5586dd4e3fb8815624eae66c76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0248373c416c405dbdc8fb13b15bf937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5fb4e47d63e4b6191009f0faac5f170": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90f9831538f74acc80db28ad2f40dba9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f3cf2c788544bbc9823ed90131f7fab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e64a0f51bc1496eae90f72e136389c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff1587237fdc4723b2da8f262048b464": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cc3fbc16020541deac253c5e70955aef",
              "IPY_MODEL_07f652d7233a4e84b72c223c92855858",
              "IPY_MODEL_7e1f2c65cef14374a12f016288a010a0"
            ],
            "layout": "IPY_MODEL_81721c5260584bb7969ba40861b41850"
          }
        },
        "cc3fbc16020541deac253c5e70955aef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bd8088da1fb47ffa6ce567d0f639e09",
            "placeholder": "",
            "style": "IPY_MODEL_5e573dd3f8f546439df31c1c8cd4171f",
            "value": "config.json:100%"
          }
        },
        "07f652d7233a4e84b72c223c92855858": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9dfbc1407f845719f0e6d878bdb6247",
            "max": 481,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1c29bb312f9f495bbb61c8037d361c1f",
            "value": 481
          }
        },
        "7e1f2c65cef14374a12f016288a010a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_482bed4fdfd44e12b6db51efce190704",
            "placeholder": "",
            "style": "IPY_MODEL_d0d6e93ace444e27b584058fdc93a784",
            "value": "481/481[00:00&lt;00:00,39.9kB/s]"
          }
        },
        "81721c5260584bb7969ba40861b41850": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bd8088da1fb47ffa6ce567d0f639e09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e573dd3f8f546439df31c1c8cd4171f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9dfbc1407f845719f0e6d878bdb6247": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c29bb312f9f495bbb61c8037d361c1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "482bed4fdfd44e12b6db51efce190704": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0d6e93ace444e27b584058fdc93a784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2cabb4211cd84154977d5dfe140546ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_40edf46af7f94fb396b527326a78944d",
              "IPY_MODEL_506770d16a1947a79c9b623e3752b1f1",
              "IPY_MODEL_59c202eb1f9d4015b29799e3c1710f2c"
            ],
            "layout": "IPY_MODEL_05f7375ae9ee4e63a7d0898d88c5ae76"
          }
        },
        "40edf46af7f94fb396b527326a78944d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b74b184050740ecabba0f5d10658e0e",
            "placeholder": "",
            "style": "IPY_MODEL_6e7d94b2e6fc4c2daa97c60c4a15ea13",
            "value": "model.safetensors:100%"
          }
        },
        "506770d16a1947a79c9b623e3752b1f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec5d2f7e7a5540a180e6b6b7c799a9aa",
            "max": 498818054,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1d424d024180449689d32bb4071fda0e",
            "value": 498818054
          }
        },
        "59c202eb1f9d4015b29799e3c1710f2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a3c4f4990b54791ad49b5ea5884411c",
            "placeholder": "",
            "style": "IPY_MODEL_4bdcac30e04047fa84855ea0d5669cb0",
            "value": "499M/499M[00:01&lt;00:00,373MB/s]"
          }
        },
        "05f7375ae9ee4e63a7d0898d88c5ae76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b74b184050740ecabba0f5d10658e0e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e7d94b2e6fc4c2daa97c60c4a15ea13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ec5d2f7e7a5540a180e6b6b7c799a9aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d424d024180449689d32bb4071fda0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3a3c4f4990b54791ad49b5ea5884411c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bdcac30e04047fa84855ea0d5669cb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}